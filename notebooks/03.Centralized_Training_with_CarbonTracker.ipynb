{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1965fde6",
   "metadata": {},
   "source": [
    "### In this notebook we perform Centralized learning, almost like 02.Individual_Training. \n",
    "\n",
    "In centralized learning there is an entity that has access to data from all base stations.\n",
    "Here, there is no option to filter out any base station.\n",
    "In this setting we also measure the energy consumption using the Carbontracker tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c11879af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f6cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d264d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48559200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c5e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e60d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/ElBorn_test.csv'], # test dataset\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    \n",
    "    filter_bs=None, # whether to use a single bs for training. It will be changed dynamically\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    epochs=150, # the number of maximum epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    early_stopping=True, # whether to use early stopping\n",
    "    patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "    \n",
    "    plot_history=True, # plot loss history\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0660fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], test_size=0.2, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], num_lags=10, filter_bs=None, identifier='District', nan_constant=0, x_scaler='minmax', y_scaler='minmax', outlier_detection=None, criterion='mse', epochs=150, lr=0.001, optimizer='adam', batch_size=128, early_stopping=True, patience=50, max_grad_norm=0.0, reg1=0.0, reg2=0.0, plot_history=True, cuda=True, seed=0, assign_stats=None, use_time_features=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e574ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b20d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up']\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d661a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b0bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd59b3",
   "metadata": {},
   "source": [
    "### By setting filter_bs to None, the preprocessing pipeline returns data from all three base stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf0480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(filter_bs=None):\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path, filter_data=filter_bs)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scaler = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                             scaler=args.x_scaler, identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scaler = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                             scaler=args.y_scaler, identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85eb3771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 22:56:19,766 | data_utils.py:383 | Observations info in ElBorn\n",
      "INFO logger 2022-10-24 22:56:19,766 | data_utils.py:384 | \tTotal number of samples:  4192\n",
      "INFO logger 2022-10-24 22:56:19,766 | data_utils.py:385 | \tNumber of samples for training: 3354\n",
      "INFO logger 2022-10-24 22:56:19,766 | data_utils.py:386 | \tNumber of samples for validation:  838\n",
      "INFO logger 2022-10-24 22:56:19,774 | data_utils.py:383 | Observations info in LesCorts\n",
      "INFO logger 2022-10-24 22:56:19,774 | data_utils.py:384 | \tTotal number of samples:  6892\n",
      "INFO logger 2022-10-24 22:56:19,774 | data_utils.py:385 | \tNumber of samples for training: 5514\n",
      "INFO logger 2022-10-24 22:56:19,774 | data_utils.py:386 | \tNumber of samples for validation:  1378\n",
      "INFO logger 2022-10-24 22:56:19,782 | data_utils.py:383 | Observations info in PobleSec\n",
      "INFO logger 2022-10-24 22:56:19,782 | data_utils.py:384 | \tTotal number of samples:  15927\n",
      "INFO logger 2022-10-24 22:56:19,782 | data_utils.py:385 | \tNumber of samples for training: 12742\n",
      "INFO logger 2022-10-24 22:56:19,782 | data_utils.py:386 | \tNumber of samples for validation:  3185\n",
      "INFO logger 2022-10-24 22:56:19,790 | data_utils.py:389 | Observations info using all data\n",
      "INFO logger 2022-10-24 22:56:19,790 | data_utils.py:390 | \tTotal number of samples:  27011\n",
      "INFO logger 2022-10-24 22:56:19,790 | data_utils.py:391 | \tNumber of samples for training: 21610\n",
      "INFO logger 2022-10-24 22:56:19,790 | data_utils.py:392 | \tNumber of samples for validation:  5401\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45e64887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>3.143298e-08</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>4.497698e-08</td>\n",
       "      <td>0.040119</td>\n",
       "      <td>0.207425</td>\n",
       "      <td>0.483274</td>\n",
       "      <td>0.864261</td>\n",
       "      <td>0.937978</td>\n",
       "      <td>0.233939</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>...</td>\n",
       "      <td>4.711435e-08</td>\n",
       "      <td>0.048765</td>\n",
       "      <td>0.232918</td>\n",
       "      <td>0.478785</td>\n",
       "      <td>0.862283</td>\n",
       "      <td>0.926591</td>\n",
       "      <td>0.281372</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>4.439640e-08</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>4.615535e-08</td>\n",
       "      <td>0.048621</td>\n",
       "      <td>0.259314</td>\n",
       "      <td>0.530084</td>\n",
       "      <td>0.864602</td>\n",
       "      <td>0.924797</td>\n",
       "      <td>0.279543</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>...</td>\n",
       "      <td>4.562575e-08</td>\n",
       "      <td>0.048478</td>\n",
       "      <td>0.242482</td>\n",
       "      <td>0.499756</td>\n",
       "      <td>0.855495</td>\n",
       "      <td>0.931433</td>\n",
       "      <td>0.279886</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.091507</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>2.993595e-08</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>4.646104e-08</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.261772</td>\n",
       "      <td>0.512427</td>\n",
       "      <td>0.864957</td>\n",
       "      <td>0.930847</td>\n",
       "      <td>0.255049</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>...</td>\n",
       "      <td>4.634120e-08</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.241381</td>\n",
       "      <td>0.450879</td>\n",
       "      <td>0.867035</td>\n",
       "      <td>0.929728</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>5.382563e-08</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>4.532153e-08</td>\n",
       "      <td>0.055921</td>\n",
       "      <td>0.320280</td>\n",
       "      <td>0.506925</td>\n",
       "      <td>0.854759</td>\n",
       "      <td>0.925932</td>\n",
       "      <td>0.321098</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>...</td>\n",
       "      <td>4.689734e-08</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>0.495057</td>\n",
       "      <td>0.876713</td>\n",
       "      <td>0.927495</td>\n",
       "      <td>0.322424</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.107071</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>5.922178e-08</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>4.655542e-08</td>\n",
       "      <td>0.060925</td>\n",
       "      <td>0.286799</td>\n",
       "      <td>0.497228</td>\n",
       "      <td>0.854279</td>\n",
       "      <td>0.929208</td>\n",
       "      <td>0.348702</td>\n",
       "      <td>0.003109</td>\n",
       "      <td>...</td>\n",
       "      <td>4.516392e-08</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.267656</td>\n",
       "      <td>0.452835</td>\n",
       "      <td>0.861872</td>\n",
       "      <td>0.928728</td>\n",
       "      <td>0.232544</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00      3.143298e-08      0.001213        4.497698e-08   \n",
       "2018-03-28 16:18:00      4.439640e-08      0.001912        4.615535e-08   \n",
       "2018-03-28 16:20:00      2.993595e-08      0.001362        4.646104e-08   \n",
       "2018-03-28 16:22:00      5.382563e-08      0.002076        4.532153e-08   \n",
       "2018-03-28 16:24:00      5.922178e-08      0.002290        4.655542e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2018-03-28 16:16:00        0.040119           0.207425       0.483274   \n",
       "2018-03-28 16:18:00        0.048621           0.259314       0.530084   \n",
       "2018-03-28 16:20:00        0.044267           0.261772       0.512427   \n",
       "2018-03-28 16:22:00        0.055921           0.320280       0.506925   \n",
       "2018-03-28 16:24:00        0.060925           0.286799       0.497228   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2018-03-28 16:16:00             0.864261         0.937978           0.233939   \n",
       "2018-03-28 16:18:00             0.864602         0.924797           0.279543   \n",
       "2018-03-28 16:20:00             0.864957         0.930847           0.255049   \n",
       "2018-03-28 16:22:00             0.854759         0.925932           0.321098   \n",
       "2018-03-28 16:24:00             0.854279         0.929208           0.348702   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2018-03-28 16:16:00   0.001755  ...       4.711435e-08       0.048765   \n",
       "2018-03-28 16:18:00   0.002709  ...       4.562575e-08       0.048478   \n",
       "2018-03-28 16:20:00   0.001829  ...       4.634120e-08       0.047619   \n",
       "2018-03-28 16:22:00   0.002827  ...       4.689734e-08       0.056978   \n",
       "2018-03-28 16:24:00   0.003109  ...       4.516392e-08       0.039556   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00          0.232918      0.478785            0.862283   \n",
       "2018-03-28 16:18:00          0.242482      0.499756            0.855495   \n",
       "2018-03-28 16:20:00          0.241381      0.450879            0.867035   \n",
       "2018-03-28 16:22:00          0.315197      0.495057            0.876713   \n",
       "2018-03-28 16:24:00          0.267656      0.452835            0.861872   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2018-03-28 16:16:00        0.926591          0.281372  0.002074    0.091860   \n",
       "2018-03-28 16:18:00        0.931433          0.279886  0.002412    0.091507   \n",
       "2018-03-28 16:20:00        0.929728          0.274900  0.001903    0.089721   \n",
       "2018-03-28 16:22:00        0.927495          0.322424  0.003139    0.107071   \n",
       "2018-03-28 16:24:00        0.928728          0.232544  0.001653    0.075132   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2018-03-28 16:16:00    ElBorn  \n",
       "2018-03-28 16:18:00    ElBorn  \n",
       "2018-03-28 16:20:00    ElBorn  \n",
       "2018-03-28 16:22:00    ElBorn  \n",
       "2018-03-28 16:24:00    ElBorn  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591b150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:16:00</th>\n",
       "      <td>0.279886</td>\n",
       "      <td>0.048478</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.091507</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:18:00</th>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.089721</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:20:00</th>\n",
       "      <td>0.322424</td>\n",
       "      <td>0.056978</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.107071</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:22:00</th>\n",
       "      <td>0.232544</td>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-28 16:24:00</th>\n",
       "      <td>0.313391</td>\n",
       "      <td>0.054345</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.102806</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>ElBorn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2018-03-28 16:16:00    0.279886  0.048478  0.001722  0.091507  0.002412   \n",
       "2018-03-28 16:18:00    0.274900  0.047619  0.001502  0.089721  0.001903   \n",
       "2018-03-28 16:20:00    0.322424  0.056978  0.002261  0.107071  0.003139   \n",
       "2018-03-28 16:22:00    0.232544  0.039556  0.001340  0.075132  0.001653   \n",
       "2018-03-28 16:24:00    0.313391  0.054345  0.001927  0.102806  0.002580   \n",
       "\n",
       "                    District  \n",
       "time                          \n",
       "2018-03-28 16:16:00   ElBorn  \n",
       "2018-03-28 16:18:00   ElBorn  \n",
       "2018-03-28 16:20:00   ElBorn  \n",
       "2018-03-28 16:22:00   ElBorn  \n",
       "2018-03-28 16:24:00   ElBorn  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0fa465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MinMaxScaler(), MinMaxScaler())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55102f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698aa32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3616bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6592db61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_X_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d521ab70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area_X_val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90c8a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.14329753e-08],\n",
       "         [1.21277163e-03],\n",
       "         [4.49769750e-08],\n",
       "         [4.01188992e-02],\n",
       "         [2.07425252e-01],\n",
       "         [4.83274072e-01],\n",
       "         [8.64260554e-01],\n",
       "         [9.37977910e-01],\n",
       "         [2.33939394e-01],\n",
       "         [1.75528484e-03],\n",
       "         [7.64968917e-02]],\n",
       "\n",
       "        [[4.43963977e-08],\n",
       "         [1.91187230e-03],\n",
       "         [4.61553498e-08],\n",
       "         [4.86211926e-02],\n",
       "         [2.59314001e-01],\n",
       "         [5.30084312e-01],\n",
       "         [8.64602447e-01],\n",
       "         [9.24797297e-01],\n",
       "         [2.79542595e-01],\n",
       "         [2.70937034e-03],\n",
       "         [9.14471596e-02]],\n",
       "\n",
       "        [[2.99359542e-08],\n",
       "         [1.36231177e-03],\n",
       "         [4.64610359e-08],\n",
       "         [4.42666970e-02],\n",
       "         [2.61771828e-01],\n",
       "         [5.12427032e-01],\n",
       "         [8.64957333e-01],\n",
       "         [9.30847228e-01],\n",
       "         [2.55048603e-01],\n",
       "         [1.82946306e-03],\n",
       "         [8.37529153e-02]],\n",
       "\n",
       "        [[5.38256302e-08],\n",
       "         [2.07580579e-03],\n",
       "         [4.53215314e-08],\n",
       "         [5.59207276e-02],\n",
       "         [3.20279956e-01],\n",
       "         [5.06924510e-01],\n",
       "         [8.54759097e-01],\n",
       "         [9.25932348e-01],\n",
       "         [3.21097761e-01],\n",
       "         [2.82748532e-03],\n",
       "         [1.05646886e-01]],\n",
       "\n",
       "        [[5.92217795e-08],\n",
       "         [2.28964840e-03],\n",
       "         [4.65554244e-08],\n",
       "         [6.09245785e-02],\n",
       "         [2.86799014e-01],\n",
       "         [4.97228354e-01],\n",
       "         [8.54279280e-01],\n",
       "         [9.29207981e-01],\n",
       "         [3.48702133e-01],\n",
       "         [3.10886209e-03],\n",
       "         [1.15539603e-01]],\n",
       "\n",
       "        [[4.27273470e-08],\n",
       "         [1.97860459e-03],\n",
       "         [4.64907401e-08],\n",
       "         [5.53930141e-02],\n",
       "         [3.06054235e-01],\n",
       "         [5.20298362e-01],\n",
       "         [8.64346623e-01],\n",
       "         [9.37814415e-01],\n",
       "         [3.20914805e-01],\n",
       "         [2.70142243e-03],\n",
       "         [1.05143227e-01]],\n",
       "\n",
       "        [[3.29200631e-08],\n",
       "         [1.46437297e-03],\n",
       "         [4.62307348e-08],\n",
       "         [4.84285802e-02],\n",
       "         [2.37825811e-01],\n",
       "         [4.55921143e-01],\n",
       "         [8.78899217e-01],\n",
       "         [9.18324053e-01],\n",
       "         [2.80411661e-01],\n",
       "         [1.78931502e-03],\n",
       "         [9.16718319e-02]],\n",
       "\n",
       "        [[4.18195008e-08],\n",
       "         [1.60662318e-03],\n",
       "         [4.63657486e-08],\n",
       "         [5.45288548e-02],\n",
       "         [2.72135526e-01],\n",
       "         [4.97247547e-01],\n",
       "         [8.55873585e-01],\n",
       "         [9.23164129e-01],\n",
       "         [3.14328194e-01],\n",
       "         [2.19081831e-03],\n",
       "         [1.03006192e-01]],\n",
       "\n",
       "        [[3.44121815e-08],\n",
       "         [1.69989897e-03],\n",
       "         [4.60645957e-08],\n",
       "         [4.88040037e-02],\n",
       "         [2.65302420e-01],\n",
       "         [4.96077031e-01],\n",
       "         [8.66527557e-01],\n",
       "         [9.23408091e-01],\n",
       "         [2.80640364e-01],\n",
       "         [2.17916491e-03],\n",
       "         [9.25039276e-02]],\n",
       "\n",
       "        [[3.66808699e-08],\n",
       "         [1.56419107e-03],\n",
       "         [4.71143515e-08],\n",
       "         [4.87649143e-02],\n",
       "         [2.32917622e-01],\n",
       "         [4.78784889e-01],\n",
       "         [8.62282574e-01],\n",
       "         [9.26590562e-01],\n",
       "         [2.81372219e-01],\n",
       "         [2.07389053e-03],\n",
       "         [9.18597132e-02]]],\n",
       "\n",
       "\n",
       "       [[[4.43963977e-08],\n",
       "         [1.91187230e-03],\n",
       "         [4.61553498e-08],\n",
       "         [4.86211926e-02],\n",
       "         [2.59314001e-01],\n",
       "         [5.30084312e-01],\n",
       "         [8.64602447e-01],\n",
       "         [9.24797297e-01],\n",
       "         [2.79542595e-01],\n",
       "         [2.70937034e-03],\n",
       "         [9.14471596e-02]],\n",
       "\n",
       "        [[2.99359542e-08],\n",
       "         [1.36231177e-03],\n",
       "         [4.64610359e-08],\n",
       "         [4.42666970e-02],\n",
       "         [2.61771828e-01],\n",
       "         [5.12427032e-01],\n",
       "         [8.64957333e-01],\n",
       "         [9.30847228e-01],\n",
       "         [2.55048603e-01],\n",
       "         [1.82946306e-03],\n",
       "         [8.37529153e-02]],\n",
       "\n",
       "        [[5.38256302e-08],\n",
       "         [2.07580579e-03],\n",
       "         [4.53215314e-08],\n",
       "         [5.59207276e-02],\n",
       "         [3.20279956e-01],\n",
       "         [5.06924510e-01],\n",
       "         [8.54759097e-01],\n",
       "         [9.25932348e-01],\n",
       "         [3.21097761e-01],\n",
       "         [2.82748532e-03],\n",
       "         [1.05646886e-01]],\n",
       "\n",
       "        [[5.92217795e-08],\n",
       "         [2.28964840e-03],\n",
       "         [4.65554244e-08],\n",
       "         [6.09245785e-02],\n",
       "         [2.86799014e-01],\n",
       "         [4.97228354e-01],\n",
       "         [8.54279280e-01],\n",
       "         [9.29207981e-01],\n",
       "         [3.48702133e-01],\n",
       "         [3.10886209e-03],\n",
       "         [1.15539603e-01]],\n",
       "\n",
       "        [[4.27273470e-08],\n",
       "         [1.97860459e-03],\n",
       "         [4.64907401e-08],\n",
       "         [5.53930141e-02],\n",
       "         [3.06054235e-01],\n",
       "         [5.20298362e-01],\n",
       "         [8.64346623e-01],\n",
       "         [9.37814415e-01],\n",
       "         [3.20914805e-01],\n",
       "         [2.70142243e-03],\n",
       "         [1.05143227e-01]],\n",
       "\n",
       "        [[3.29200631e-08],\n",
       "         [1.46437297e-03],\n",
       "         [4.62307348e-08],\n",
       "         [4.84285802e-02],\n",
       "         [2.37825811e-01],\n",
       "         [4.55921143e-01],\n",
       "         [8.78899217e-01],\n",
       "         [9.18324053e-01],\n",
       "         [2.80411661e-01],\n",
       "         [1.78931502e-03],\n",
       "         [9.16718319e-02]],\n",
       "\n",
       "        [[4.18195008e-08],\n",
       "         [1.60662318e-03],\n",
       "         [4.63657486e-08],\n",
       "         [5.45288548e-02],\n",
       "         [2.72135526e-01],\n",
       "         [4.97247547e-01],\n",
       "         [8.55873585e-01],\n",
       "         [9.23164129e-01],\n",
       "         [3.14328194e-01],\n",
       "         [2.19081831e-03],\n",
       "         [1.03006192e-01]],\n",
       "\n",
       "        [[3.44121815e-08],\n",
       "         [1.69989897e-03],\n",
       "         [4.60645957e-08],\n",
       "         [4.88040037e-02],\n",
       "         [2.65302420e-01],\n",
       "         [4.96077031e-01],\n",
       "         [8.66527557e-01],\n",
       "         [9.23408091e-01],\n",
       "         [2.80640364e-01],\n",
       "         [2.17916491e-03],\n",
       "         [9.25039276e-02]],\n",
       "\n",
       "        [[3.66808699e-08],\n",
       "         [1.56419107e-03],\n",
       "         [4.71143515e-08],\n",
       "         [4.87649143e-02],\n",
       "         [2.32917622e-01],\n",
       "         [4.78784889e-01],\n",
       "         [8.62282574e-01],\n",
       "         [9.26590562e-01],\n",
       "         [2.81372219e-01],\n",
       "         [2.07389053e-03],\n",
       "         [9.18597132e-02]],\n",
       "\n",
       "        [[3.66673198e-08],\n",
       "         [1.72214315e-03],\n",
       "         [4.56257538e-08],\n",
       "         [4.84776907e-02],\n",
       "         [2.42481545e-01],\n",
       "         [4.99756068e-01],\n",
       "         [8.55494559e-01],\n",
       "         [9.31432724e-01],\n",
       "         [2.79885650e-01],\n",
       "         [2.41222628e-03],\n",
       "         [9.15068761e-02]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a11d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27988565, 0.04847769, 0.00172214, 0.09150688, 0.00241223],\n",
       "       [0.27489996, 0.04761904, 0.00150213, 0.08972067, 0.00190256]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "076cc206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21580, 5371)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7777e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0274244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e90c8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "495af4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"gru\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1933251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (gru): GRU(11, 128, batch_first=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4b5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X\n",
    "        log_per=1):\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0])\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per,\n",
    "                  use_carbontracker=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7edadc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: The following components were found: GPU with device(s) NVIDIA GeForce GTX 1650 Ti.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 22:56:35,981 | train_utils.py:97 | Epoch 1 [Train]: loss 0.003362360383613316, mse: 0.0014821417862549424, rmse: 0.038498594601036315, mae 0.01988358423113823, r2: 0.6481391199712613, nrmse: 1.211988530862326\n",
      "INFO logger 2022-10-24 22:56:35,981 | train_utils.py:99 | Epoch 1 [Test]: loss 5.0885799606463344e-06, mse: 0.0006503971526399255, rmse: 0.025502885182659734, mae 0.015069444663822651, r2: 0.5788994666257106, nrmse: 1.4976602246208361\n",
      "INFO logger 2022-10-24 22:56:35,989 | helpers.py:148 | Validation loss decreased (inf --> 0.000005). Caching model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: \n",
      "Actual consumption for 1 epoch(s):\n",
      "\tTime:\t0:00:11\n",
      "\tEnergy:\t0.000027 kWh\n",
      "\tCO2eq:\t0.007892 g\n",
      "\tThis is equivalent to:\n",
      "\t0.000066 km travelled by car\n",
      "CarbonTracker: \n",
      "Predicted consumption for 150 epoch(s):\n",
      "\tTime:\t0:28:29\n",
      "\tEnergy:\t0.004024 kWh\n",
      "\tCO2eq:\t1.183827 g\n",
      "\tThis is equivalent to:\n",
      "\t0.009832 km travelled by car\n",
      "CarbonTracker: Finished monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 22:56:39,532 | train_utils.py:97 | Epoch 2 [Train]: loss 0.0013997200853656288, mse: 0.0011750881094485521, rmse: 0.034279558186309116, mae 0.016529232263565063, r2: 0.7188824393802479, nrmse: 1.1056780835392557\n",
      "INFO logger 2022-10-24 22:56:39,540 | train_utils.py:99 | Epoch 2 [Test]: loss 4.326711965229253e-06, mse: 0.000552975689060986, rmse: 0.02351543512378595, mae 0.012636438012123108, r2: 0.6438951342489813, nrmse: 1.370843508901875\n",
      "INFO logger 2022-10-24 22:56:39,540 | helpers.py:148 | Validation loss decreased (0.000005 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:42,078 | train_utils.py:97 | Epoch 3 [Train]: loss 0.0011278649705710833, mse: 0.0010529655264690518, rmse: 0.032449430294984406, mae 0.015142296440899372, r2: 0.7451748609607612, nrmse: 1.0766963792972208\n",
      "INFO logger 2022-10-24 22:56:42,078 | train_utils.py:99 | Epoch 3 [Test]: loss 4.10277706639329e-06, mse: 0.000524334900546819, rmse: 0.022898360215238535, mae 0.011855759657919407, r2: 0.6623700520658609, nrmse: 1.3368729297421988\n",
      "INFO logger 2022-10-24 22:56:42,078 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:43,827 | train_utils.py:97 | Epoch 4 [Train]: loss 0.0010282822671765187, mse: 0.0009975428692996502, rmse: 0.03158390205942974, mae 0.014325657859444618, r2: 0.7551492956664971, nrmse: 1.0714753169086566\n",
      "INFO logger 2022-10-24 22:56:43,827 | train_utils.py:99 | Epoch 4 [Test]: loss 3.9550776492295885e-06, mse: 0.0005054482608102262, rmse: 0.022482176514079463, mae 0.01124055590480566, r2: 0.6723607380168284, nrmse: 1.3303296947973928\n",
      "INFO logger 2022-10-24 22:56:43,827 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:45,788 | train_utils.py:97 | Epoch 5 [Train]: loss 0.0009933034124778662, mse: 0.0009675755281932652, rmse: 0.031105876103933564, mae 0.013850405812263489, r2: 0.7600249136452388, nrmse: 1.0680059133083177\n",
      "INFO logger 2022-10-24 22:56:45,788 | train_utils.py:99 | Epoch 5 [Test]: loss 3.861398677586305e-06, mse: 0.0004934686003252864, rmse: 0.022214153153457962, mae 0.01080759521573782, r2: 0.67878675781843, nrmse: 1.3179806679332398\n",
      "INFO logger 2022-10-24 22:56:45,788 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:47,662 | train_utils.py:97 | Epoch 6 [Train]: loss 0.000977495595669517, mse: 0.00095021614106372, rmse: 0.030825576086485715, mae 0.013628053478896618, r2: 0.7629251915215158, nrmse: 1.0653504165515795\n",
      "INFO logger 2022-10-24 22:56:47,662 | train_utils.py:99 | Epoch 6 [Test]: loss 3.8051910718918206e-06, mse: 0.00048627969226799905, rmse: 0.022051750322094596, mae 0.01059919036924839, r2: 0.6830505086838606, nrmse: 1.305801077050526\n",
      "INFO logger 2022-10-24 22:56:47,670 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:49,389 | train_utils.py:97 | Epoch 7 [Train]: loss 0.0009686360266986137, mse: 0.0009382957359775901, rmse: 0.03063161334271491, mae 0.013488774187862873, r2: 0.7650368902036118, nrmse: 1.0630535268168357\n",
      "INFO logger 2022-10-24 22:56:49,397 | train_utils.py:99 | Epoch 7 [Test]: loss 3.765510044622805e-06, mse: 0.0004812035185750574, rmse: 0.021936351532902125, mae 0.010471047833561897, r2: 0.6863079327181922, nrmse: 1.2953482140913124\n",
      "INFO logger 2022-10-24 22:56:49,397 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:51,130 | train_utils.py:97 | Epoch 8 [Train]: loss 0.0009616804726372925, mse: 0.000929108529817313, rmse: 0.03048128163016301, mae 0.013374308124184608, r2: 0.7668432358881322, nrmse: 1.0602975126914247\n",
      "INFO logger 2022-10-24 22:56:51,130 | train_utils.py:99 | Epoch 8 [Test]: loss 3.735295767024811e-06, mse: 0.00047733765677548945, rmse: 0.021848058421184466, mae 0.010369626805186272, r2: 0.6889440822826849, nrmse: 1.2865144412554141\n",
      "INFO logger 2022-10-24 22:56:51,130 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:53,494 | train_utils.py:97 | Epoch 9 [Train]: loss 0.0009544429260088384, mse: 0.0009217398473992944, rmse: 0.03036016876434145, mae 0.013286556117236614, r2: 0.7684560206506399, nrmse: 1.0571383903807086\n",
      "INFO logger 2022-10-24 22:56:53,494 | train_utils.py:99 | Epoch 9 [Test]: loss 3.712256412017958e-06, mse: 0.0004743898753076792, rmse: 0.021780492999647166, mae 0.010301153175532818, r2: 0.6910267920559964, nrmse: 1.2795685829128756\n",
      "INFO logger 2022-10-24 22:56:53,494 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:55,157 | train_utils.py:97 | Epoch 10 [Train]: loss 0.0009472834448517471, mse: 0.0009157584281638265, rmse: 0.030261500758617815, mae 0.013219376094639301, r2: 0.7698565758560281, nrmse: 1.0541055565002055\n",
      "INFO logger 2022-10-24 22:56:55,165 | train_utils.py:99 | Epoch 10 [Test]: loss 3.694066462440838e-06, mse: 0.00047206258750520647, rmse: 0.021727001346370983, mae 0.010253125801682472, r2: 0.6927291499328779, nrmse: 1.274144738004205\n",
      "INFO logger 2022-10-24 22:56:55,165 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:56,853 | train_utils.py:97 | Epoch 11 [Train]: loss 0.0009411606692804434, mse: 0.0009108626982197165, rmse: 0.03018050195440289, mae 0.013159153051674366, r2: 0.7710472373253596, nrmse: 1.0514919737387407\n",
      "INFO logger 2022-10-24 22:56:56,853 | train_utils.py:99 | Epoch 11 [Test]: loss 3.6787578896155107e-06, mse: 0.0004701036377809942, rmse: 0.021681873484110964, mae 0.010206915438175201, r2: 0.6942249436159479, nrmse: 1.269725141385021\n",
      "INFO logger 2022-10-24 22:56:56,853 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:56:58,721 | train_utils.py:97 | Epoch 12 [Train]: loss 0.0009362405506493082, mse: 0.0009068296058103442, rmse: 0.030113611636772238, mae 0.013099020346999168, r2: 0.7720576661288919, nrmse: 1.0493148364574716\n",
      "INFO logger 2022-10-24 22:56:58,729 | train_utils.py:99 | Epoch 12 [Test]: loss 3.665427444567736e-06, mse: 0.0004683980660047382, rmse: 0.021642506001032742, mae 0.010154795832931995, r2: 0.695568685716472, nrmse: 1.2661174558880084\n",
      "INFO logger 2022-10-24 22:56:58,729 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:00,372 | train_utils.py:97 | Epoch 13 [Train]: loss 0.0009323291330243194, mse: 0.000903528300113976, rmse: 0.03005874748079128, mae 0.01304441224783659, r2: 0.7729001264607094, nrmse: 1.0475659275204938\n",
      "INFO logger 2022-10-24 22:57:00,380 | train_utils.py:99 | Epoch 13 [Test]: loss 3.6541280002496043e-06, mse: 0.0004669528570957482, rmse: 0.021609092000723865, mae 0.01010560616850853, r2: 0.6967048507858371, nrmse: 1.263400349113863\n",
      "INFO logger 2022-10-24 22:57:00,380 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:02,053 | train_utils.py:97 | Epoch 14 [Train]: loss 0.0009292156608880651, mse: 0.0009008832275867462, rmse: 0.030014716850017862, mae 0.013007311150431633, r2: 0.7735783434071436, nrmse: 1.04621391217964\n",
      "INFO logger 2022-10-24 22:57:02,053 | train_utils.py:99 | Epoch 14 [Test]: loss 3.645233797728802e-06, mse: 0.0004658151010517031, rmse: 0.021582750080833144, mae 0.010076032020151615, r2: 0.6975536534785718, nrmse: 1.2615920447698428\n",
      "INFO logger 2022-10-24 22:57:02,053 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:03,854 | train_utils.py:97 | Epoch 15 [Train]: loss 0.0009266188216525523, mse: 0.0008987686596810818, rmse: 0.029979470637105682, mae 0.012994793243706226, r2: 0.7741181518208933, nrmse: 1.0451325292416536\n",
      "INFO logger 2022-10-24 22:57:03,854 | train_utils.py:99 | Epoch 15 [Test]: loss 3.638745477766444e-06, mse: 0.0004649842157959938, rmse: 0.02156349266227514, mae 0.01007642038166523, r2: 0.6981029341569603, nrmse: 1.2604284406659572\n",
      "INFO logger 2022-10-24 22:57:03,854 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:05,518 | train_utils.py:97 | Epoch 16 [Train]: loss 0.0009241502556123925, mse: 0.0008969976333901286, rmse: 0.02994991875431599, mae 0.012998471036553383, r2: 0.7745723545402466, nrmse: 1.0441478525030274\n",
      "INFO logger 2022-10-24 22:57:05,518 | train_utils.py:99 | Epoch 16 [Test]: loss 3.6340469587100086e-06, mse: 0.0004643828724510968, rmse: 0.021549544599621977, mae 0.010097777470946312, r2: 0.6984343112991959, nrmse: 1.2595304495672084\n",
      "INFO logger 2022-10-24 22:57:05,518 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:07,199 | train_utils.py:97 | Epoch 17 [Train]: loss 0.0009215405515581782, mse: 0.0008953585056588054, rmse: 0.029922541764676434, mae 0.013003368861973286, r2: 0.7749969062816715, nrmse: 1.0431341432177685\n",
      "INFO logger 2022-10-24 22:57:07,207 | train_utils.py:99 | Epoch 17 [Test]: loss 3.6301199526128865e-06, mse: 0.0004638790269382298, rmse: 0.0215378510287872, mae 0.010121330618858337, r2: 0.6986765956520987, nrmse: 1.2586658423563741\n",
      "INFO logger 2022-10-24 22:57:07,207 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:09,043 | train_utils.py:97 | Epoch 18 [Train]: loss 0.0009188666855373052, mse: 0.0008937109960243106, rmse: 0.029894999515375654, mae 0.01300220936536789, r2: 0.7754201003526316, nrmse: 1.042073630482468\n",
      "INFO logger 2022-10-24 22:57:09,043 | train_utils.py:99 | Epoch 18 [Test]: loss 3.6261050531916267e-06, mse: 0.0004633647040463984, rmse: 0.02152590774035786, mae 0.010137856006622314, r2: 0.6989237563894417, nrmse: 1.2578083077049815\n",
      "INFO logger 2022-10-24 22:57:09,051 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:10,765 | train_utils.py:97 | Epoch 19 [Train]: loss 0.0009163368804850092, mse: 0.0008920219843275845, rmse: 0.029866737088734427, mae 0.012996208854019642, r2: 0.7758373609761662, nrmse: 1.0410366350631137\n",
      "INFO logger 2022-10-24 22:57:10,765 | train_utils.py:99 | Epoch 19 [Test]: loss 3.621725091978197e-06, mse: 0.00046280352398753166, rmse: 0.02151286879957045, mae 0.010148132219910622, r2: 0.699195380587698, nrmse: 1.257068462465964\n",
      "INFO logger 2022-10-24 22:57:10,765 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:12,454 | train_utils.py:97 | Epoch 20 [Train]: loss 0.0009140251698512301, mse: 0.0008903167326934636, rmse: 0.02983817576014766, mae 0.012987370602786541, r2: 0.7762334755367037, nrmse: 1.0401022804604043\n",
      "INFO logger 2022-10-24 22:57:12,454 | train_utils.py:99 | Epoch 20 [Test]: loss 3.6170991834836907e-06, mse: 0.00046221091179177165, rmse: 0.02149909095268383, mae 0.010155302472412586, r2: 0.6994696256061415, nrmse: 1.2565618554508808\n",
      "INFO logger 2022-10-24 22:57:12,454 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:14,257 | train_utils.py:97 | Epoch 21 [Train]: loss 0.0009118733220914009, mse: 0.0008886106079444289, rmse: 0.02980957242136205, mae 0.012975329533219337, r2: 0.7766019486105636, nrmse: 1.039302950137451\n",
      "INFO logger 2022-10-24 22:57:14,257 | train_utils.py:99 | Epoch 21 [Test]: loss 3.6123968848849458e-06, mse: 0.0004616084916051477, rmse: 0.021485076020464712, mae 0.010159311816096306, r2: 0.6997260840975349, nrmse: 1.2563082079548424\n",
      "INFO logger 2022-10-24 22:57:14,257 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:16,047 | train_utils.py:97 | Epoch 22 [Train]: loss 0.0009097857267095379, mse: 0.0008868840523064137, rmse: 0.029780598588786185, mae 0.012959632091224194, r2: 0.7769497934394848, nrmse: 1.0386165159364098\n",
      "INFO logger 2022-10-24 22:57:16,047 | train_utils.py:99 | Epoch 22 [Test]: loss 3.6077105809299452e-06, mse: 0.00046100799227133393, rmse: 0.02147109667137042, mae 0.010157834738492966, r2: 0.6999597349506962, nrmse: 1.2562309630911301\n",
      "INFO logger 2022-10-24 22:57:16,047 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:17,856 | train_utils.py:97 | Epoch 23 [Train]: loss 0.0009076730759433621, mse: 0.0008851128513924778, rmse: 0.029750846229855004, mae 0.012934176251292229, r2: 0.7772896517828138, nrmse: 1.0379969255666512\n",
      "INFO logger 2022-10-24 22:57:17,856 | train_utils.py:99 | Epoch 23 [Test]: loss 3.6031546596698832e-06, mse: 0.00046042431495152414, rmse: 0.021457500202761833, mae 0.010148853994905949, r2: 0.7001712002626936, nrmse: 1.256223330543403\n",
      "INFO logger 2022-10-24 22:57:17,856 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:19,706 | train_utils.py:97 | Epoch 24 [Train]: loss 0.0009054681479739033, mse: 0.0008833102183416486, rmse: 0.029720535297023985, mae 0.012897146865725517, r2: 0.777628850807418, nrmse: 1.0374040614716076\n",
      "INFO logger 2022-10-24 22:57:19,706 | train_utils.py:99 | Epoch 24 [Test]: loss 3.5990055363736783e-06, mse: 0.00045989250065758824, rmse: 0.0214451043517533, mae 0.010130914859473705, r2: 0.7003527729637541, nrmse: 1.2562167171091012\n",
      "INFO logger 2022-10-24 22:57:19,706 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:22,040 | train_utils.py:97 | Epoch 25 [Train]: loss 0.0009031547899922829, mse: 0.0008815749315544963, rmse: 0.029691327547863133, mae 0.012850165367126465, r2: 0.7779603208467163, nrmse: 1.0368284512289068\n",
      "INFO logger 2022-10-24 22:57:22,040 | train_utils.py:99 | Epoch 25 [Test]: loss 3.5957337403520397e-06, mse: 0.0004594727361109108, rmse: 0.02143531516238823, mae 0.010105735622346401, r2: 0.7004775609092071, nrmse: 1.2562642717565358\n",
      "INFO logger 2022-10-24 22:57:22,040 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:23,785 | train_utils.py:97 | Epoch 26 [Train]: loss 0.0009007940610759901, mse: 0.0008800782379694283, rmse: 0.029666112619779295, mae 0.012802541255950928, r2: 0.778253999833768, nrmse: 1.0363330280898937\n",
      "INFO logger 2022-10-24 22:57:23,785 | train_utils.py:99 | Epoch 26 [Test]: loss 3.5939461374057576e-06, mse: 0.00045924284495413303, rmse: 0.021429952052072657, mae 0.010081248357892036, r2: 0.7004874034431868, nrmse: 1.2566649957769223\n",
      "INFO logger 2022-10-24 22:57:23,789 | helpers.py:148 | Validation loss decreased (0.000004 --> 0.000004). Caching model ...\n",
      "INFO logger 2022-10-24 22:57:25,743 | train_utils.py:97 | Epoch 27 [Train]: loss 0.0008984979966571592, mse: 0.0008790284628048539, rmse: 0.02964841417015173, mae 0.012774067930877209, r2: 0.7784531440696579, nrmse: 1.036091904679607\n",
      "INFO logger 2022-10-24 22:57:25,743 | train_utils.py:99 | Epoch 27 [Test]: loss 3.5943174930164107e-06, mse: 0.00045928958570584655, rmse: 0.021431042571602683, mae 0.010078544728457928, r2: 0.7002887676614031, nrmse: 1.2579987026149055\n",
      "INFO logger 2022-10-24 22:57:25,751 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 22:57:27,342 | train_utils.py:97 | Epoch 28 [Train]: loss 0.0008963168694967244, mse: 0.0008785403333604336, rmse: 0.029640181061532562, mae 0.012782151810824871, r2: 0.778507596160849, nrmse: 1.0363063994723989\n",
      "INFO logger 2022-10-24 22:57:27,342 | train_utils.py:99 | Epoch 28 [Test]: loss 3.5971208046087443e-06, mse: 0.00045964811579324305, rmse: 0.021439405677239353, mae 0.010114363394677639, r2: 0.6998229534937621, nrmse: 1.260622674134213\n",
      "INFO logger 2022-10-24 22:57:27,342 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 22:57:29,078 | train_utils.py:97 | Epoch 29 [Train]: loss 0.0008941053199753066, mse: 0.0008783111115917563, rmse: 0.029636314068921532, mae 0.012804655358195305, r2: 0.7784702579340096, nrmse: 1.0369176894122467\n",
      "INFO logger 2022-10-24 22:57:29,078 | train_utils.py:99 | Epoch 29 [Test]: loss 3.600777771890585e-06, mse: 0.0004601151158567518, rmse: 0.02145029407389912, mae 0.010159815661609173, r2: 0.6992559033736363, nrmse: 1.2635528319186493\n",
      "INFO logger 2022-10-24 22:57:29,078 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 22:57:30,845 | train_utils.py:97 | Epoch 30 [Train]: loss 0.0008918554138871562, mse: 0.0008777275797910988, rmse: 0.029626467555061282, mae 0.012798026204109192, r2: 0.7784936163372412, nrmse: 1.037612419020648\n",
      "INFO logger 2022-10-24 22:57:30,845 | train_utils.py:99 | Epoch 30 [Test]: loss 3.602714055799814e-06, mse: 0.00046036174171604216, rmse: 0.02145604207947128, mae 0.010171876288950443, r2: 0.698870793953343, nrmse: 1.2652565811012213\n",
      "INFO logger 2022-10-24 22:57:30,845 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2022-10-24 22:57:32,447 | train_utils.py:97 | Epoch 31 [Train]: loss 0.0008900674515186476, mse: 0.000876392878126353, rmse: 0.029603933490777085, mae 0.012741203419864178, r2: 0.7787239988780517, nrmse: 1.038007475523195\n",
      "INFO logger 2022-10-24 22:57:32,447 | train_utils.py:99 | Epoch 31 [Test]: loss 3.602647349587403e-06, mse: 0.00046035219565965235, rmse: 0.021455819622182983, mae 0.01013563759624958, r2: 0.6987510075493797, nrmse: 1.2653147727835936\n",
      "INFO logger 2022-10-24 22:57:32,447 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2022-10-24 22:57:34,251 | train_utils.py:97 | Epoch 32 [Train]: loss 0.000889101936517627, mse: 0.0008746276726014912, rmse: 0.029574104764159663, mae 0.012646196410059929, r2: 0.7790945413717889, nrmse: 1.0382648252320852\n",
      "INFO logger 2022-10-24 22:57:34,251 | train_utils.py:99 | Epoch 32 [Test]: loss 3.6033853309621244e-06, mse: 0.00046044570626690984, rmse: 0.02145799865474201, mae 0.010069992393255234, r2: 0.6986781767445143, nrmse: 1.2643358617798022\n",
      "INFO logger 2022-10-24 22:57:34,251 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2022-10-24 22:57:35,956 | train_utils.py:97 | Epoch 33 [Train]: loss 0.0008893835509986636, mse: 0.0008737781317904592, rmse: 0.029559738357949977, mae 0.012594993226230145, r2: 0.779266284741347, nrmse: 1.0389785196674777\n",
      "INFO logger 2022-10-24 22:57:35,956 | train_utils.py:99 | Epoch 33 [Test]: loss 3.6103133193916965e-06, mse: 0.00046133087016642094, rmse: 0.021478614251539158, mae 0.010052548721432686, r2: 0.6981379999202577, nrmse: 1.2639027840385146\n",
      "INFO logger 2022-10-24 22:57:35,956 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2022-10-24 22:57:37,859 | train_utils.py:97 | Epoch 34 [Train]: loss 0.0008910624157966627, mse: 0.0008740437915548682, rmse: 0.029564231624631617, mae 0.012651389464735985, r2: 0.7792295484736961, nrmse: 1.0392482756432722\n",
      "INFO logger 2022-10-24 22:57:37,859 | train_utils.py:99 | Epoch 34 [Test]: loss 3.6186395305331717e-06, mse: 0.0004623968561645597, rmse: 0.021503414988428227, mae 0.01013182383030653, r2: 0.6973795634411768, nrmse: 1.2657585315382793\n",
      "INFO logger 2022-10-24 22:57:37,859 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2022-10-24 22:57:39,620 | train_utils.py:97 | Epoch 35 [Train]: loss 0.0008945937723307083, mse: 0.0008740000193938613, rmse: 0.029563491326192536, mae 0.012604300864040852, r2: 0.779350525802162, nrmse: 1.0387163714506569\n",
      "INFO logger 2022-10-24 22:57:39,620 | train_utils.py:99 | Epoch 35 [Test]: loss 3.613494756581311e-06, mse: 0.00046174097224138677, rmse: 0.02148815888440391, mae 0.010064831003546715, r2: 0.6980602490227816, nrmse: 1.2666729178211207\n",
      "INFO logger 2022-10-24 22:57:39,620 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2022-10-24 22:57:41,439 | train_utils.py:97 | Epoch 36 [Train]: loss 0.0009103033009911119, mse: 0.0008779347990639508, rmse: 0.02962996454712612, mae 0.01254144124686718, r2: 0.7791055715053546, nrmse: 1.0379441795148234\n",
      "INFO logger 2022-10-24 22:57:41,439 | train_utils.py:99 | Epoch 36 [Test]: loss 3.626014810184286e-06, mse: 0.00046334447688423097, rmse: 0.02152543790226417, mae 0.009968305006623268, r2: 0.6984263653032661, nrmse: 1.2650309329702867\n",
      "INFO logger 2022-10-24 22:57:41,439 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2022-10-24 22:57:43,062 | train_utils.py:97 | Epoch 37 [Train]: loss 0.0009027705738155967, mse: 0.0008764808881096542, rmse: 0.029605419911050986, mae 0.012526271864771843, r2: 0.7795152095266649, nrmse: 1.0367230455839027\n",
      "INFO logger 2022-10-24 22:57:43,062 | train_utils.py:99 | Epoch 37 [Test]: loss 3.6275797560936323e-06, mse: 0.00046354284859262407, rmse: 0.021530045252916307, mae 0.009972414933145046, r2: 0.6984696532513958, nrmse: 1.2622321789679023\n",
      "INFO logger 2022-10-24 22:57:43,062 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2022-10-24 22:57:44,611 | train_utils.py:97 | Epoch 38 [Train]: loss 0.0008912507651317603, mse: 0.0008758833864703774, rmse: 0.02959532710531136, mae 0.012457756325602531, r2: 0.7794055355307927, nrmse: 1.0375967215940514\n",
      "INFO logger 2022-10-24 22:57:44,611 | train_utils.py:99 | Epoch 38 [Test]: loss 3.6241310957800237e-06, mse: 0.0004631010233424604, rmse: 0.021519782139753654, mae 0.009924636222422123, r2: 0.6987239788379075, nrmse: 1.2623082845143139\n",
      "INFO logger 2022-10-24 22:57:44,611 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2022-10-24 22:57:46,061 | train_utils.py:97 | Epoch 39 [Train]: loss 0.0008841565616626078, mse: 0.0008758669719099998, rmse: 0.02959504978725327, mae 0.012408480979502201, r2: 0.7796028634637393, nrmse: 1.036900114122107\n",
      "INFO logger 2022-10-24 22:57:46,061 | train_utils.py:99 | Epoch 39 [Test]: loss 3.6233630629799192e-06, mse: 0.0004630017501767725, rmse: 0.021517475460117817, mae 0.009914988651871681, r2: 0.6986331755444896, nrmse: 1.2624541202190118\n",
      "INFO logger 2022-10-24 22:57:46,061 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2022-10-24 22:57:47,495 | train_utils.py:97 | Epoch 40 [Train]: loss 0.0008811684428616184, mse: 0.0008746549719944596, rmse: 0.029574566302728086, mae 0.012421293184161186, r2: 0.7797519357796368, nrmse: 1.0369505951984976\n",
      "INFO logger 2022-10-24 22:57:47,495 | train_utils.py:99 | Epoch 40 [Test]: loss 3.6212096403009936e-06, mse: 0.000462724914541468, rmse: 0.021511041688897076, mae 0.009954681620001793, r2: 0.6984274695803225, nrmse: 1.263514389336898\n",
      "INFO logger 2022-10-24 22:57:47,495 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2022-10-24 22:57:48,885 | train_utils.py:97 | Epoch 41 [Train]: loss 0.0008782840678313787, mse: 0.0008733173017390072, rmse: 0.029551942435972076, mae 0.012452107854187489, r2: 0.7799189065375239, nrmse: 1.0370165982436295\n",
      "INFO logger 2022-10-24 22:57:48,885 | train_utils.py:99 | Epoch 41 [Test]: loss 3.6193784567934698e-06, mse: 0.000462489842902869, rmse: 0.02150557701859843, mae 0.010015339590609074, r2: 0.6981496060528536, nrmse: 1.2649831247276493\n",
      "INFO logger 2022-10-24 22:57:48,885 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2022-10-24 22:57:50,332 | train_utils.py:97 | Epoch 42 [Train]: loss 0.0008753185607800323, mse: 0.0008719022152945399, rmse: 0.029527990370063113, mae 0.01248210296034813, r2: 0.7801133387417639, nrmse: 1.036930195663226\n",
      "INFO logger 2022-10-24 22:57:50,332 | train_utils.py:99 | Epoch 42 [Test]: loss 3.6167678598639993e-06, mse: 0.0004621547122951597, rmse: 0.021497783892651813, mae 0.010072881355881691, r2: 0.6979561085339345, nrmse: 1.2666429265542885\n",
      "INFO logger 2022-10-24 22:57:50,332 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2022-10-24 22:57:51,752 | train_utils.py:97 | Epoch 43 [Train]: loss 0.0008729457481395379, mse: 0.0008705343934707344, rmse: 0.029504819834575067, mae 0.012500797398388386, r2: 0.780294758980549, nrmse: 1.0368876423520077\n",
      "INFO logger 2022-10-24 22:57:51,752 | train_utils.py:99 | Epoch 43 [Test]: loss 3.614209783671207e-06, mse: 0.00046182586811482906, rmse: 0.021490134204207034, mae 0.010115392506122589, r2: 0.6978098698685933, nrmse: 1.2684806921796015\n",
      "INFO logger 2022-10-24 22:57:51,752 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2022-10-24 22:57:53,175 | train_utils.py:97 | Epoch 44 [Train]: loss 0.0008714730108897785, mse: 0.00086926796939224, rmse: 0.02948335071514498, mae 0.012503601610660553, r2: 0.7804575073591888, nrmse: 1.0369057917319464\n",
      "INFO logger 2022-10-24 22:57:53,175 | train_utils.py:99 | Epoch 44 [Test]: loss 3.6120719553296632e-06, mse: 0.0004615508078131825, rmse: 0.021483733563167795, mae 0.010137618519365788, r2: 0.6976891163792344, nrmse: 1.2703990511756127\n",
      "INFO logger 2022-10-24 22:57:53,175 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2022-10-24 22:57:54,591 | train_utils.py:97 | Epoch 45 [Train]: loss 0.0008709841235026109, mse: 0.000868195085786283, rmse: 0.02946515036082937, mae 0.012492179870605469, r2: 0.7805853681540186, nrmse: 1.0370314853568665\n",
      "INFO logger 2022-10-24 22:57:54,591 | train_utils.py:99 | Epoch 45 [Test]: loss 3.610853605642157e-06, mse: 0.00046139367623254657, rmse: 0.021480076262260957, mae 0.010143052786588669, r2: 0.6975520816936216, nrmse: 1.2723572494105226\n",
      "INFO logger 2022-10-24 22:57:54,591 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2022-10-24 22:57:56,085 | train_utils.py:97 | Epoch 46 [Train]: loss 0.0008712355909210601, mse: 0.0008674358832649887, rmse: 0.029452264484500824, mae 0.01247400976717472, r2: 0.7806534302035252, nrmse: 1.0372679740949657\n",
      "INFO logger 2022-10-24 22:57:56,085 | train_utils.py:99 | Epoch 46 [Test]: loss 3.611214329766056e-06, mse: 0.00046143820509314537, rmse: 0.021481112752675207, mae 0.010137303732335567, r2: 0.697347479773567, nrmse: 1.2743134299049559\n",
      "INFO logger 2022-10-24 22:57:56,085 | helpers.py:136 | EarlyStopping counter: 20 out of 50\n",
      "INFO logger 2022-10-24 22:57:57,500 | train_utils.py:97 | Epoch 47 [Train]: loss 0.0008717680584541839, mse: 0.0008670458337292075, rmse: 0.029445642015911412, mae 0.012464943341910839, r2: 0.7806348484190171, nrmse: 1.0376456864044288\n",
      "INFO logger 2022-10-24 22:57:57,500 | train_utils.py:99 | Epoch 47 [Test]: loss 3.6135559255610375e-06, mse: 0.00046173640294000506, rmse: 0.02148805256276159, mae 0.010134328156709671, r2: 0.697009948548986, nrmse: 1.2762850092537208\n",
      "INFO logger 2022-10-24 22:57:57,500 | helpers.py:136 | EarlyStopping counter: 21 out of 50\n",
      "INFO logger 2022-10-24 22:57:58,957 | train_utils.py:97 | Epoch 48 [Train]: loss 0.0008719663215741567, mse: 0.0008668750524520874, rmse: 0.029442741931621916, mae 0.01247555110603571, r2: 0.7805364910298496, nrmse: 1.0381862226489156\n",
      "INFO logger 2022-10-24 22:57:58,957 | train_utils.py:99 | Epoch 48 [Test]: loss 3.617146947957628e-06, mse: 0.0004621942644007504, rmse: 0.021498703784199418, mae 0.010145243257284164, r2: 0.696526838657396, nrmse: 1.2784114441771275\n",
      "INFO logger 2022-10-24 22:57:58,957 | helpers.py:136 | EarlyStopping counter: 22 out of 50\n",
      "INFO logger 2022-10-24 22:58:00,358 | train_utils.py:97 | Epoch 49 [Train]: loss 0.0008714886043499979, mse: 0.0008667680667713284, rmse: 0.0294409250325347, mae 0.012499270960688591, r2: 0.7804079541604001, nrmse: 1.0388227377397865\n",
      "INFO logger 2022-10-24 22:58:00,358 | train_utils.py:99 | Epoch 49 [Test]: loss 3.620846595619397e-06, mse: 0.0004626666777767241, rmse: 0.021509687998125963, mae 0.010168498381972313, r2: 0.6959685811459496, nrmse: 1.2806913200153012\n",
      "INFO logger 2022-10-24 22:58:00,358 | helpers.py:136 | EarlyStopping counter: 23 out of 50\n",
      "INFO logger 2022-10-24 22:58:01,899 | train_utils.py:97 | Epoch 50 [Train]: loss 0.0008704041506562996, mse: 0.0008665889617986977, rmse: 0.029437883106614472, mae 0.012528101913630962, r2: 0.7803016025731606, nrmse: 1.0394341677430996\n",
      "INFO logger 2022-10-24 22:58:01,899 | train_utils.py:99 | Epoch 50 [Test]: loss 3.6239179844580997e-06, mse: 0.00046305861906148493, rmse: 0.02151879687764827, mae 0.010199161246418953, r2: 0.6954254909629083, nrmse: 1.2828868884465685\n",
      "INFO logger 2022-10-24 22:58:01,899 | helpers.py:136 | EarlyStopping counter: 24 out of 50\n",
      "INFO logger 2022-10-24 22:58:03,549 | train_utils.py:97 | Epoch 51 [Train]: loss 0.0008688842091771339, mse: 0.0008662659674882889, rmse: 0.029432396563791555, mae 0.012556982226669788, r2: 0.7802394803227852, nrmse: 1.0399297766835938\n",
      "INFO logger 2022-10-24 22:58:03,549 | train_utils.py:99 | Epoch 51 [Test]: loss 3.626127516406886e-06, mse: 0.00046333990758284926, rmse: 0.021525331764756826, mae 0.01023211982101202, r2: 0.694961045085237, nrmse: 1.2847041504478072\n",
      "INFO logger 2022-10-24 22:58:03,549 | helpers.py:136 | EarlyStopping counter: 25 out of 50\n",
      "INFO logger 2022-10-24 22:58:05,378 | train_utils.py:97 | Epoch 52 [Train]: loss 0.0008671875150984031, mse: 0.000865787616930902, rmse: 0.02942426918261356, mae 0.012578576803207397, r2: 0.780245481109851, nrmse: 1.0401859027030036\n",
      "INFO logger 2022-10-24 22:58:05,378 | train_utils.py:99 | Epoch 52 [Test]: loss 3.6275737338874187e-06, mse: 0.00046352349454537034, rmse: 0.021529595782210365, mae 0.010260296054184437, r2: 0.6946145816858447, nrmse: 1.2858809279007652\n",
      "INFO logger 2022-10-24 22:58:05,378 | helpers.py:136 | EarlyStopping counter: 26 out of 50\n",
      "INFO logger 2022-10-24 22:58:07,096 | train_utils.py:97 | Epoch 53 [Train]: loss 0.0008655511854806735, mse: 0.0008651586249470711, rmse: 0.02941357892108798, mae 0.012586748227477074, r2: 0.7803315629264065, nrmse: 1.0401261791305274\n",
      "INFO logger 2022-10-24 22:58:07,096 | train_utils.py:99 | Epoch 53 [Test]: loss 3.6284122368634846e-06, mse: 0.0004636295489035547, rmse: 0.02153205863134212, mae 0.010276871733367443, r2: 0.6944092868614135, nrmse: 1.2862529152111255\n",
      "INFO logger 2022-10-24 22:58:07,112 | helpers.py:136 | EarlyStopping counter: 27 out of 50\n",
      "INFO logger 2022-10-24 22:58:09,131 | train_utils.py:97 | Epoch 54 [Train]: loss 0.0008641483538294994, mse: 0.0008644068730063736, rmse: 0.029400797149165422, mae 0.012582855299115181, r2: 0.780489506216331, nrmse: 1.039763468296513\n",
      "INFO logger 2022-10-24 22:58:09,131 | train_utils.py:99 | Epoch 54 [Test]: loss 3.6288113998809425e-06, mse: 0.00046367867616936564, rmse: 0.021533199394640955, mae 0.010278800502419472, r2: 0.6943425366557971, nrmse: 1.2858556948691757\n",
      "INFO logger 2022-10-24 22:58:09,139 | helpers.py:136 | EarlyStopping counter: 28 out of 50\n",
      "INFO logger 2022-10-24 22:58:11,120 | train_utils.py:97 | Epoch 55 [Train]: loss 0.0008631189786095592, mse: 0.0008635820122435689, rmse: 0.02938676593712838, mae 0.012568500824272633, r2: 0.780687020575097, nrmse: 1.0392516033420807\n",
      "INFO logger 2022-10-24 22:58:11,128 | train_utils.py:99 | Epoch 55 [Test]: loss 3.6290708072094693e-06, mse: 0.0004637103120330721, rmse: 0.021533933965559384, mae 0.010268619284033775, r2: 0.6943543978580115, nrmse: 1.2851094951789044\n",
      "INFO logger 2022-10-24 22:58:11,128 | helpers.py:136 | EarlyStopping counter: 29 out of 50\n",
      "INFO logger 2022-10-24 22:58:12,978 | train_utils.py:97 | Epoch 56 [Train]: loss 0.0008625275851010464, mse: 0.0008627389324828982, rmse: 0.029372417886222753, mae 0.012546762824058533, r2: 0.7808882134240795, nrmse: 1.0387704014740295\n",
      "INFO logger 2022-10-24 22:58:12,978 | train_utils.py:99 | Epoch 56 [Test]: loss 3.629592841928197e-06, mse: 0.00046377634862437844, rmse: 0.021535467225588082, mae 0.01025012694299221, r2: 0.694354996319347, nrmse: 1.2845776097775632\n",
      "INFO logger 2022-10-24 22:58:12,978 | helpers.py:136 | EarlyStopping counter: 30 out of 50\n",
      "INFO logger 2022-10-24 22:58:14,951 | train_utils.py:97 | Epoch 57 [Train]: loss 0.0008621583359135965, mse: 0.0008619069121778011, rmse: 0.02935825117710183, mae 0.012520672753453255, r2: 0.7810640996046908, nrmse: 1.038430327229418\n",
      "INFO logger 2022-10-24 22:58:14,951 | train_utils.py:99 | Epoch 57 [Test]: loss 3.630575699036337e-06, mse: 0.0004639005346689373, rmse: 0.021538350323758254, mae 0.010225390084087849, r2: 0.6942852869093805, nrmse: 1.2845370465675816\n",
      "INFO logger 2022-10-24 22:58:14,951 | helpers.py:136 | EarlyStopping counter: 31 out of 50\n",
      "INFO logger 2022-10-24 22:58:16,676 | train_utils.py:97 | Epoch 58 [Train]: loss 0.0008617811672680686, mse: 0.0008610343793407083, rmse: 0.02934338731879311, mae 0.01249676663428545, r2: 0.7811916681730215, nrmse: 1.038327311224714\n",
      "INFO logger 2022-10-24 22:58:16,676 | train_utils.py:99 | Epoch 58 [Test]: loss 3.63205748417748e-06, mse: 0.0004640888364519924, rmse: 0.021542721194222247, mae 0.010201102122664452, r2: 0.694091643560373, nrmse: 1.285200417262281\n",
      "INFO logger 2022-10-24 22:58:16,676 | helpers.py:136 | EarlyStopping counter: 32 out of 50\n",
      "INFO logger 2022-10-24 22:58:18,357 | train_utils.py:97 | Epoch 59 [Train]: loss 0.0008613576216576082, mse: 0.0008600848959758878, rmse: 0.02932720402588504, mae 0.012476911768317223, r2: 0.7813021169641547, nrmse: 1.0382636166929085\n",
      "INFO logger 2022-10-24 22:58:18,357 | train_utils.py:99 | Epoch 59 [Test]: loss 3.633989470381341e-06, mse: 0.0004643347638193518, rmse: 0.02154842833756912, mae 0.010178749449551105, r2: 0.6937987455859254, nrmse: 1.286278378053278\n",
      "INFO logger 2022-10-24 22:58:18,357 | helpers.py:136 | EarlyStopping counter: 33 out of 50\n",
      "INFO logger 2022-10-24 22:58:20,172 | train_utils.py:97 | Epoch 60 [Train]: loss 0.0008608865445111494, mse: 0.0008590942015871406, rmse: 0.02931030879378688, mae 0.012458930723369122, r2: 0.7814345149996017, nrmse: 1.0379888687954866\n",
      "INFO logger 2022-10-24 22:58:20,172 | train_utils.py:99 | Epoch 60 [Test]: loss 3.6363448723431487e-06, mse: 0.0004646349116228521, rmse: 0.021555391706551102, mae 0.010160787031054497, r2: 0.693473156822228, nrmse: 1.2872425333935218\n",
      "INFO logger 2022-10-24 22:58:20,180 | helpers.py:136 | EarlyStopping counter: 34 out of 50\n",
      "INFO logger 2022-10-24 22:58:22,166 | train_utils.py:97 | Epoch 61 [Train]: loss 0.0008601425918996345, mse: 0.0008580979192629457, rmse: 0.029293308438326757, mae 0.01244304422289133, r2: 0.7816110067572348, nrmse: 1.0374255832040877\n",
      "INFO logger 2022-10-24 22:58:22,174 | train_utils.py:99 | Epoch 61 [Test]: loss 3.6387483232605732e-06, mse: 0.00046494166599586606, rmse: 0.021562506023091704, mae 0.010148128494620323, r2: 0.6931780284005062, nrmse: 1.2878229862319348\n",
      "INFO logger 2022-10-24 22:58:22,174 | helpers.py:136 | EarlyStopping counter: 35 out of 50\n",
      "INFO logger 2022-10-24 22:58:24,013 | train_utils.py:97 | Epoch 62 [Train]: loss 0.0008588579596554772, mse: 0.0008571076323278248, rmse: 0.02927640060403302, mae 0.012426535598933697, r2: 0.781829982816191, nrmse: 1.036688376312402\n",
      "INFO logger 2022-10-24 22:58:24,013 | train_utils.py:99 | Epoch 62 [Test]: loss 3.6407517455234305e-06, mse: 0.00046519696479663253, rmse: 0.021568425181190964, mae 0.010136683471500874, r2: 0.6929577539362632, nrmse: 1.2879482305065701\n",
      "INFO logger 2022-10-24 22:58:24,021 | helpers.py:136 | EarlyStopping counter: 36 out of 50\n",
      "INFO logger 2022-10-24 22:58:25,885 | train_utils.py:97 | Epoch 63 [Train]: loss 0.0008570621205626123, mse: 0.0008561350405216217, rmse: 0.029259785380648672, mae 0.012409995310008526, r2: 0.7820642605433801, nrmse: 1.03593175500856\n",
      "INFO logger 2022-10-24 22:58:25,885 | train_utils.py:99 | Epoch 63 [Test]: loss 3.6422933632577454e-06, mse: 0.0004653936193790287, rmse: 0.021572983553023645, mae 0.01012505404651165, r2: 0.6928074916903608, nrmse: 1.2877056059477807\n",
      "INFO logger 2022-10-24 22:58:25,885 | helpers.py:136 | EarlyStopping counter: 37 out of 50\n",
      "INFO logger 2022-10-24 22:58:28,012 | train_utils.py:97 | Epoch 64 [Train]: loss 0.0008550245386870904, mse: 0.0008551342179998755, rmse: 0.029242678023735712, mae 0.01239642035216093, r2: 0.7822866624790684, nrmse: 1.0353116938505342\n",
      "INFO logger 2022-10-24 22:58:28,012 | train_utils.py:99 | Epoch 64 [Test]: loss 3.643502161811541e-06, mse: 0.0004655477241612971, rmse: 0.021576554965084142, mae 0.010115392506122589, r2: 0.6926860602845519, nrmse: 1.287318210603761\n",
      "INFO logger 2022-10-24 22:58:28,012 | helpers.py:136 | EarlyStopping counter: 38 out of 50\n",
      "INFO logger 2022-10-24 22:58:29,872 | train_utils.py:97 | Epoch 65 [Train]: loss 0.0008532849306114382, mse: 0.0008540102280676365, rmse: 0.029223453390515578, mae 0.012384658679366112, r2: 0.7825058629660651, nrmse: 1.0348372688982486\n",
      "INFO logger 2022-10-24 22:58:29,872 | train_utils.py:99 | Epoch 65 [Test]: loss 3.644254798734102e-06, mse: 0.00046564353397116065, rmse: 0.02157877508041549, mae 0.010106401517987251, r2: 0.6925986662970556, nrmse: 1.2867969250671776\n",
      "INFO logger 2022-10-24 22:58:29,872 | helpers.py:136 | EarlyStopping counter: 39 out of 50\n",
      "INFO logger 2022-10-24 22:58:32,568 | train_utils.py:97 | Epoch 66 [Train]: loss 0.0008519842806382101, mse: 0.0008527914178557694, rmse: 0.029202592656402435, mae 0.012370273470878601, r2: 0.7827528728529136, nrmse: 1.0343155246507212\n",
      "INFO logger 2022-10-24 22:58:32,568 | train_utils.py:99 | Epoch 66 [Test]: loss 3.6447811264168365e-06, mse: 0.00046570980339311063, rmse: 0.0215803105490424, mae 0.010094058699905872, r2: 0.692556093209898, nrmse: 1.2860741450740742\n",
      "INFO logger 2022-10-24 22:58:32,568 | helpers.py:136 | EarlyStopping counter: 40 out of 50\n",
      "INFO logger 2022-10-24 22:58:34,279 | train_utils.py:97 | Epoch 67 [Train]: loss 0.0008504612013585977, mse: 0.0008514319779351354, rmse: 0.0291793073587283, mae 0.012348277494311333, r2: 0.783053818726115, nrmse: 1.0336621168356583\n",
      "INFO logger 2022-10-24 22:58:34,279 | train_utils.py:99 | Epoch 67 [Test]: loss 3.6454407229288783e-06, mse: 0.0004657933604903519, rmse: 0.021582246418998, mae 0.010075541213154793, r2: 0.6925364538062914, nrmse: 1.2850972515258243\n",
      "INFO logger 2022-10-24 22:58:34,279 | helpers.py:136 | EarlyStopping counter: 41 out of 50\n",
      "INFO logger 2022-10-24 22:58:35,983 | train_utils.py:97 | Epoch 68 [Train]: loss 0.0008490565589646884, mse: 0.0008499493706040084, rmse: 0.02915389117431854, mae 0.01232498325407505, r2: 0.7833935488273018, nrmse: 1.0329612710649518\n",
      "INFO logger 2022-10-24 22:58:35,983 | train_utils.py:99 | Epoch 68 [Test]: loss 3.6464338179907826e-06, mse: 0.0004659193509723991, rmse: 0.02158516506706398, mae 0.010055696591734886, r2: 0.6924954150075668, nrmse: 1.2842088085873824\n",
      "INFO logger 2022-10-24 22:58:35,983 | helpers.py:136 | EarlyStopping counter: 42 out of 50\n",
      "INFO logger 2022-10-24 22:58:37,853 | train_utils.py:97 | Epoch 69 [Train]: loss 0.0008475233990200316, mse: 0.0008484545978717506, rmse: 0.029128243988811798, mae 0.012302974238991737, r2: 0.7837215134702044, nrmse: 1.0323284902167755\n",
      "INFO logger 2022-10-24 22:58:37,853 | train_utils.py:99 | Epoch 69 [Test]: loss 3.6478000331270885e-06, mse: 0.00046609327546320856, rmse: 0.021589193488021005, mae 0.010037114843726158, r2: 0.6924058127220226, nrmse: 1.2835727018325085\n",
      "INFO logger 2022-10-24 22:58:37,853 | helpers.py:136 | EarlyStopping counter: 43 out of 50\n",
      "INFO logger 2022-10-24 22:58:39,474 | train_utils.py:97 | Epoch 70 [Train]: loss 0.0008458728716040828, mse: 0.000846880313474685, rmse: 0.029101208110226026, mae 0.01227913610637188, r2: 0.7840608659186026, nrmse: 1.0317115022535237\n",
      "INFO logger 2022-10-24 22:58:39,474 | train_utils.py:99 | Epoch 70 [Test]: loss 3.6492934596649467e-06, mse: 0.0004662837309297174, rmse: 0.02159360393564996, mae 0.01001829095184803, r2: 0.6922907313101166, nrmse: 1.2830798269902741\n",
      "INFO logger 2022-10-24 22:58:39,474 | helpers.py:136 | EarlyStopping counter: 44 out of 50\n",
      "INFO logger 2022-10-24 22:58:41,126 | train_utils.py:97 | Epoch 71 [Train]: loss 0.0008442211434506259, mse: 0.0008452296024188399, rmse: 0.029072832720924185, mae 0.012254766188561916, r2: 0.7843987580565498, nrmse: 1.0311925884681585\n",
      "INFO logger 2022-10-24 22:58:41,126 | train_utils.py:99 | Epoch 71 [Test]: loss 3.650756140506657e-06, mse: 0.00046646996634081006, rmse: 0.021597915786964492, mae 0.009998534806072712, r2: 0.6921765093311191, nrmse: 1.2826309159470568\n",
      "INFO logger 2022-10-24 22:58:41,126 | helpers.py:136 | EarlyStopping counter: 45 out of 50\n",
      "INFO logger 2022-10-24 22:58:43,043 | train_utils.py:97 | Epoch 72 [Train]: loss 0.000842482468314054, mse: 0.0008435381460003555, rmse: 0.029043728169784875, mae 0.012231046333909035, r2: 0.7847376610853025, nrmse: 1.0307376971927353\n",
      "INFO logger 2022-10-24 22:58:43,043 | train_utils.py:99 | Epoch 72 [Test]: loss 3.652147949511368e-06, mse: 0.00046664741239510477, rmse: 0.021602023340305528, mae 0.009978785179555416, r2: 0.6920698222749886, nrmse: 1.282199164771149\n",
      "INFO logger 2022-10-24 22:58:43,043 | helpers.py:136 | EarlyStopping counter: 46 out of 50\n",
      "INFO logger 2022-10-24 22:58:44,955 | train_utils.py:97 | Epoch 73 [Train]: loss 0.0008406153200836698, mse: 0.0008417903445661068, rmse: 0.029013623430487044, mae 0.01220858097076416, r2: 0.7850860981778084, nrmse: 1.0303189691524994\n",
      "INFO logger 2022-10-24 22:58:44,955 | train_utils.py:99 | Epoch 73 [Test]: loss 3.6534402298912622e-06, mse: 0.0004668122564908117, rmse: 0.02160583848154965, mae 0.00996065977960825, r2: 0.6919682933036336, nrmse: 1.2818093803575576\n",
      "INFO logger 2022-10-24 22:58:44,955 | helpers.py:136 | EarlyStopping counter: 47 out of 50\n",
      "INFO logger 2022-10-24 22:58:47,145 | train_utils.py:97 | Epoch 74 [Train]: loss 0.0008385924778588576, mse: 0.000839976011775434, rmse: 0.028982339653234244, mae 0.012187795713543892, r2: 0.7854290765489171, nrmse: 1.0300204023870316\n",
      "INFO logger 2022-10-24 22:58:47,145 | train_utils.py:99 | Epoch 74 [Test]: loss 3.6545703191377967e-06, mse: 0.00046695611672475934, rmse: 0.02160916742322016, mae 0.00994403101503849, r2: 0.6918699575664212, nrmse: 1.2815254529334106\n",
      "INFO logger 2022-10-24 22:58:47,145 | helpers.py:136 | EarlyStopping counter: 48 out of 50\n",
      "INFO logger 2022-10-24 22:58:48,824 | train_utils.py:97 | Epoch 75 [Train]: loss 0.000836423589053541, mse: 0.0008381427032873034, rmse: 0.02895069434896689, mae 0.012168032117187977, r2: 0.7857517882443517, nrmse: 1.0299144519685477\n",
      "INFO logger 2022-10-24 22:58:48,824 | train_utils.py:99 | Epoch 75 [Test]: loss 3.655583695711416e-06, mse: 0.0004670854250434786, rmse: 0.02161215919438589, mae 0.009928075596690178, r2: 0.691770705966233, nrmse: 1.2813579460549955\n",
      "INFO logger 2022-10-24 22:58:48,824 | helpers.py:136 | EarlyStopping counter: 49 out of 50\n",
      "INFO logger 2022-10-24 22:58:50,803 | train_utils.py:97 | Epoch 76 [Train]: loss 0.000834137367976799, mse: 0.0008364432724192739, rmse: 0.028921329022354312, mae 0.012153858318924904, r2: 0.7860471329829988, nrmse: 1.0299131458009572\n",
      "INFO logger 2022-10-24 22:58:50,803 | train_utils.py:99 | Epoch 76 [Test]: loss 3.6567835452843886e-06, mse: 0.00046723842388018966, rmse: 0.02161569855175145, mae 0.00991846527904272, r2: 0.6916499543898477, nrmse: 1.2812773040482517\n",
      "INFO logger 2022-10-24 22:58:50,803 | helpers.py:136 | EarlyStopping counter: 50 out of 50\n",
      "INFO logger 2022-10-24 22:58:50,803 | train_utils.py:117 | Early Stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuLUlEQVR4nO3de3xV5Z3v8c8ve2fv3EgCISAQFLxUC4JBI1bHHrUO9VK8tGNH1Ha02pd1Wse2M9pqnarH0XN0Zlptp05ntNqq0xFaO1YcWx3r5WhrqwSKCiIVLEqAcgkQciGXnfzOH2vtZCd7QzaQsDfk+3691mut9axnrf1bCaxfnudZa21zd0RERFIV5DoAERHJP0oOIiKSRslBRETSKDmIiEgaJQcREUkTzXUAQ2Hs2LE+ZcqUXIchInJAWbx48RZ3r8607aBIDlOmTKG+vj7XYYiIHFDM7P1dbVO3koiIpFFyEBGRNEoOIiKS5qAYcxCRA1NXVxcNDQ20t7fnOpSDWlFRETU1NRQWFma9j5KDiORMQ0MDo0aNYsqUKZhZrsM5KLk7jY2NNDQ0MHXq1Kz3U7eSiORMe3s7VVVVSgzDyMyoqqra49aZkoOI5JQSw/Dbm5/xiE4Oi9Zs5e5n3qGnR68tFxFJNaKTwxtrt/P9l1azo70r16GISA40NjZSW1tLbW0thxxyCJMmTepd7+zs3O2+9fX1XHfddXv0eVOmTGHGjBnMnDmT0047jfff73sGzcz4zGc+07ueSCSorq5m7ty5AGzcuJG5c+dy3HHHMW3aNM4991wA1qxZQ3FxcW/ctbW1PPLII3sUVyYjekC6qiwGQGNrJ5UlsRxHIyL7W1VVFUuXLgXgtttuo6ysjOuvv753eyKRIBrNfJmsq6ujrq5ujz/zxRdfZOzYsdx6663ccccdPPDAAwCUlpaybNkydu7cSXFxMc899xyTJk3q3e+WW25hzpw5fPnLXwbgzTff7N12xBFH9J7HUBnRLYeq0jgAW1t3/xeCiIwcV1xxBddccw0nnXQSX/va13j99dc5+eSTmTVrFqeccgorV64E4KWXXur9q/62227jyiuv5PTTT+fwww/nu9/97qCfc/LJJ7Nu3bp+Zeeeey5PP/00AI899hiXXHJJ77YNGzZQU1PTuz5z5sx9PtfdGdEthzGlYcuhRclBJNf+91PLeXv9jiE95rSJ5dx63vQ93q+hoYFXX32VSCTCjh07eOWVV4hGo/zqV7/iG9/4Bj/72c/S9nnnnXd48cUXaW5u5uijj+av//qvd/tcwTPPPMOFF17Yr2zevHncfvvtzJ07lzfffJMrr7ySV155BYAvfelLXHzxxXzve9/jz//8z/nc5z7HxIkTAVi9ejW1tbW9x/mXf/kXPvrRj+7xeaca0cmhr1upI8eRiEg++fSnP00kEgGgqamJyy+/nHfffRczo6sr8xjlJz7xCeLxOPF4nHHjxrFx48Z+f+knnXHGGWzdupWysjL+4R/+od+2mTNnsmbNGh577LHeMYWks846i/fee49nnnmGX/7yl8yaNYtly5YBw9OtNKKTQ7LlsFUtB5Gc25u/8IdLaWlp7/I3v/lNzjjjDJ544gnWrFnD6aefnnGfeDzeuxyJREgkEhnrvfjii1RWVnLZZZdx66238u1vf7vf9vPPP5/rr7+el156icbGxn7bxowZw6WXXsqll17K3LlzefnllznhhBP28ix3b0SPOcSjEUbFozRqzEFEdqGpqal3YPhHP/rRkBwzGo1y77338sgjj7B169Z+26688kpuvfVWZsyY0a/8hRdeoK2tDYDm5mZWr17NoYceOiTxZDKikwPAmLKYBqRFZJe+9rWvcdNNNzFr1qxdtgb2xoQJE7jkkku47777+pXX1NRkvEV28eLF1NXVMXPmTE4++WQ+//nPc+KJJwJ9Yw7JKZsB8cGY+4H/AFhdXZ3v7Zf9fPJff0NpLMp/fP6kIY5KRAazYsUKPvzhD+c6jBEh08/azBa7e8b7cbNqOZjZ2Wa20sxWmdmNGbbHzWxBuP01M5sSls82s6Xh9IaZfTIsPzqlfKmZ7TCzr4TbbjOzdSnbzh34eUOpqjTOlhYNSIuIpBp0QNrMIsB9wBygAVhkZgvd/e2UalcB29z9SDObB9wNXAwsA+rcPWFmE4A3zOwpd18J1KYcfx3wRMrx7nH3f9730xtcVWmMNxu274+PEhE5YGTTcpgNrHL399y9E5gPXDCgzgXAw+Hy48CZZmbu3ubuyU66IiBTH9aZwGp33+V3mQ6nMWUxtrV1cjB0r4mIDJVsksMkYG3KekNYlrFOmAyagCoAMzvJzJYDbwHXpCSLpHnAYwPKrjWzN83sITMbnSkoM7vazOrNrH7z5s1ZnEZmVaUxurqdHe1DN9AkInKgG/a7ldz9NXefDpwI3GRmRcltZhYDzgd+mrLL94EjCLqdNgDf2sVx73f3Onevq66u3uv4eh+E07iDiEivbJLDOmByynpNWJaxjplFgQqg39Mb7r4CaAGOTSk+B1ji7htT6m1092537wEeIOjWGjZj9H4lEZE02SSHRcBRZjY1/Et/HrBwQJ2FwOXh8kXAC+7u4T5RADM7DDgGWJOy3yUM6FIKB66TPkkwqD1sqkr73swqIiPLvryyG4KX77366qsZt/3oRz+iurqa2tpajjnmGO65557ebbfddhtmxqpVq3rL7r33XsyM5G35Dz30UO/rvY899liefPJJIHgx4NSpU3vjPOWUU/blR7BLg96tFN5pdC3wLBABHnL35WZ2O1Dv7guBB4FHzWwVsJUggQCcCtxoZl1AD/BFd98CYGalBHdAfWHAR/6jmdUSDF6vybB9SPW+QkPJQWTEGeyV3YN56aWXKCsr2+UFOvmivMbGRo4++mguuugiJk8OOmJmzJjB/Pnz+fu//3sAfvrTnzJ9evAKkYaGBu68806WLFlCRUUFLS0tpI6t/tM//RMXXXTR3pxy1rIac3D3X7j7h9z9CHe/Myy7JUwMuHu7u3/a3Y9099nu/l5Y/qi7T3f3Wnc/3t1/nnLMVnevcvemAZ/1WXef4e4z3f18d98wZGebQd+bWTXmICLBk8innXYaJ5xwAmeddRYbNgSXoO9+97tMmzaNmTNnMm/ePNasWcO//du/cc8991BbW9v79tRMqqqqOPLII3uPBXDhhRf2tgZWr15NRUUFY8eOBWDTpk2MGjWKsrIyAMrKypg6depwnXJGI/rFewBFhRHK9H4lkdz75Y3wp7eG9piHzIBz7sq6urvzN3/zNzz55JNUV1ezYMECbr75Zh566CHuuusu/vjHPxKPx9m+fTuVlZVcc801WbU2PvjgA9rb2/t9B0N5eTmTJ09m2bJlPPnkk1x88cX88Ic/BOC4445j/PjxTJ06lTPPPJNPfepTnHfeeb373nDDDdxxxx0ATJ8+nR//+Md78lPJyohPDhC0HtStJCIdHR0sW7aMOXPmANDd3c2ECcEw6MyZM7nsssu48MIL076HYVcWLFjAyy+/zDvvvMP3vvc9ioqK+m2fN28e8+fP59lnn+X555/vTQ6RSIRnnnmGRYsW8fzzz/PVr36VxYsXc9tttwH7p1tJyQElB5G8sAd/4Q8Xd2f69On89re/Tdv29NNP8/LLL/PUU09x55138tZbg7dykmMO9fX1fPzjH+f888/nkEMO6d0+d+5cbrjhBurq6igvL++3r5kxe/ZsZs+ezZw5c/jc5z7Xmxz2hxH/VlaAsWUxtug7HURGvHg8zubNm3uTQ1dXF8uXL6enp4e1a9dyxhlncPfdd9PU1ERLSwujRo2iubl50OPW1dXx2c9+lu985zv9yktKSrj77ru5+eab+5WvX7+eJUuW9K4vXbqUww47bAjOMHtKDiRbDhqQFhnpCgoKePzxx/n617/OcccdR21tLa+++ird3d185jOfYcaMGcyaNYvrrruOyspKzjvvPJ544olBB6QBvv71r/PDH/4wLZnMmzeP448/vl9ZV1cX119/Pccccwy1tbUsWLCgX2K54YYb+r2iO5vbbvfUiH9lN8Bdv3yHB3/9Hn+44xzMbAgjE5Hd0Su7959heWX3wW5sWfB+peYOvV9JRASUHIDUZx007iAiAkoOQOpT0hp3ENnfDoau7Xy3Nz9jJQeCb4MDtRxE9reioiIaGxuVIIaRu9PY2Jj2jMVg9JwDfa/t1rMOIvtXTU0NDQ0N7Mt3ssjgioqKqKmp2aN9lBxIGXNQchDZrwoLC/f7O4MkO+pWIni/Umksom4lEZGQkkNoTJkehBMRSVJyCFWVxtWtJCISUnIIVZXG1K0kIhJScgjpzawiIn2UHELBmEOn7rcWEUHJodfY0jid3T206P1KIiJKDkl6v5KISJ+skoOZnW1mK81slZndmGF73MwWhNtfM7MpYflsM1saTm+Y2SdT9lljZm+F2+pTyseY2XNm9m44Hz0E5zmoMWV6EE5EJGnQ5GBmEeA+4BxgGnCJmU0bUO0qYJu7HwncA9wdli8D6ty9Fjgb+HczS30q+wx3rx3wPvEbgefd/Sjg+XB92I0N36+kQWkRkexaDrOBVe7+nrt3AvOBCwbUuQB4OFx+HDjTzMzd29w92YlfBGQz2pt6rIeBC7PYZ5+NKdObWUVEkrJJDpOAtSnrDWFZxjphMmgCqgDM7CQzWw68BVyTkiwc+B8zW2xmV6cca7y7bwiX/wSMzxSUmV1tZvVmVj8UL+2q0vuVRER6DfuAtLu/5u7TgROBm8ws+d7YU939eILuqi+Z2f/KsK+zi9aGu9/v7nXuXlddXb3PcRYVRijR+5VERIDsksM6YHLKek1YlrFOOKZQATSmVnD3FUALcGy4vi6cbwKeIOi+AthoZhPCY00ANmV/OvumqkwPwomIQHbJYRFwlJlNNbMYMA9YOKDOQuDycPki4AV393CfKICZHQYcA6wxs1IzGxWWlwIfJxi8Hnisy4En9+7U9twYvV9JRATI4vsc3D1hZtcCzwIR4CF3X25mtwP17r4QeBB41MxWAVsJEgjAqcCNZtYF9ABfdPctZnY48ISZJWP4T3d/JtznLuAnZnYV8D7wl0N1soOpKo2xqbl9f32ciEjeyurLftz9F8AvBpTdkrLcDnw6w36PAo9mKH8POG4Xn9UInJlNXENtTGmMFRt25OKjRUTyip6QTlFVFqNR71cSEVFySFVVGqMz0UNrZ3euQxERySklhxQTKooBeL+xNceRiIjklpJDimkTywFYvl7jDiIysik5pJhaVUpJLMLbSg4iMsIpOaQoKDCmTShn+fqmXIciIpJTSg4DTJ9Yztvrd9DTozuWRGTkUnIYYPrEClo7u1mjQWkRGcGUHAbQoLSIiJJDmg+NH0VhxJQcRGREU3IYIBYt4KhxozQoLSIjmpJDBslBab1GQ0RGKiWHDKZPLKextZONO/SVoSIyMik5ZDB9UgWAupZEZMRScsjgwxPKMdMdSyIycik5ZFAWjzKlqlQtBxEZsZQcdmHaxHK1HERkxFJy2IXpE8tp2LaTprauXIciIrLfKTnswvSJ4aD0BnUticjIo+SwC9PD12jo9d0iMhJllRzM7GwzW2lmq8zsxgzb42a2INz+mplNCctnm9nScHrDzD4Zlk82sxfN7G0zW25mX0451m1mti5lv3OH6Fz3yNiyOOPL4xp3EJERKTpYBTOLAPcBc4AGYJGZLXT3t1OqXQVsc/cjzWwecDdwMbAMqHP3hJlNAN4ws6eABPB37r7EzEYBi83suZRj3uPu/zxkZ7mXpk+s0B1LIjIiZdNymA2scvf33L0TmA9cMKDOBcDD4fLjwJlmZu7e5u6JsLwIcAB33+DuS8LlZmAFMGnfTmXoTZ9YzurNrbR3dec6FBGR/Sqb5DAJWJuy3kD6hby3TpgMmoAqADM7ycyWA28B16QkC8LtU4BZwGspxdea2Ztm9pCZjc4UlJldbWb1Zla/efPmLE5jz806tJLuHufX724ZluOLiOSrYR+QdvfX3H06cCJwk5kVJbeZWRnwM+Ar7p7s3P8+cARQC2wAvrWL497v7nXuXlddXT0ssX/0qGqqR8X5z9c/GJbji4jkq2ySwzpgcsp6TViWsY6ZRYEKoDG1gruvAFqAY8N6hQSJ4cfu/l8p9Ta6e7e79wAPEHRr5URhpIB5J07mxZWbaNjWlqswRET2u2ySwyLgKDObamYxYB6wcECdhcDl4fJFwAvu7uE+UQAzOww4BlhjZgY8CKxw92+nHigcuE76JMGgds7Mm30oBsx/fe2gdUVEDhaDJodwjOBa4FmCgeOfuPtyM7vdzM4Pqz0IVJnZKuBvgeTtrqcS3KG0FHgC+KK7bwH+DPgs8LEMt6z+o5m9ZWZvAmcAXx2SM91LkyqLOf3ocSyoX0tXd08uQxER2W/sYPhCm7q6Oq+vrx+24z+/YiNXPVzP9y87nnNmTBh8BxGRA4CZLXb3ukzb9IR0Fk4/ehwTK4r48WsamBaRkUHJIQuRAmPe7EP59aotrNnSmutwRESGnZJDli4+cTKRAuMx3dYqIiOAkkOWxpcXMefD4/lJ/Vo9MS0iBz0lhz3wV6ccxra2Lr7z/Lu5DkVEZFgpOeyBU44YyyWzJ/P9l1bzyrvD88oOEZF8oOSwh26ZO52jxpXx1QVvsLm5I9fhiIgMCyWHPVQci/C9S4+nub2Lv/3JUnp6DvznREREBlJy2AtHHzKKW86bxivvbuGBV97LdTgiIkNOyWEvXTr7UM6dcQj/9OxKfvHWhlyHIyIypJQc9pKZ8X8/NZMZNRV88cdL+Mdn3qFbXUwicpBQctgHFcWFzL/6I1wyezL/+tJqrvzRIpraunIdlojIPlNy2EfxaIT/+6mZ/J9PzuDV1Vs4/75f87v3GgffUUQkjyk5DJFLTzqU+Vd/hES3M+/+3/H5hxexalNzrsMSEdkrSg5D6ITDxvD8353G184+mt+9t5Wz7n2Fm594i3Xbd+Y6NBGRPaLvcxgmjS0dfOf5d3tf8/3xaeO54pQpzJ46huCL8EREcmt33+eg5DDMGra18R+/+4DHXv+App1dfHhCOX9ZV8MnZkxgXHlRrsMTkRFMySEP7Ozs5sml63jkt+/z9oYdFBh85PAqzjtuInOmjWdsWTzXIYrICKPkkGfe3djMU2+sZ+Eb61nT2AbAjEkVnPahak47uprayZUURjQcJCLDS8khT7k7y9fv4KWVm/h/f9jMkg+2093jFBUWUDu5krrDxlA3ZTS1kyupLInlOlwROcjsc3Iws7OB7wAR4AfufteA7XHgEeAEoBG42N3XmNls4P5kNeA2d39id8c0s6nAfKAKWAx81t07dxffgZocBmra2cWrq7bw+pqt1K/ZxtsbdvQ+dT2psphpE8uZPrGcYw4ZxRHVZRxWVUosqhaGiOydfUoOZhYB/gDMARqARcAl7v52Sp0vAjPd/Rozmwd80t0vNrMSoNPdE2Y2AXgDmAj4ro5pZj8B/svd55vZvwFvuPv3dxfjwZIcBmrtSPDG2u28ta6J5et3sHx9E+9taSX5K4sUGIeOKeGwqhImjy6hZnQxNaNLmFhZxPjyIqpHxdU9JSK7tLvkEM1i/9nAKnd/LzzYfOAC4O2UOhcAt4XLjwPfMzNz97aUOkUESWGXxzSzFcDHgEvDeg+Hx91tcjhYlcajnHLkWE45cmxvWVtngtWbWlm9uYXVm1tYtamFD7a28fsPttO0M/3VHVWlMapHxakqi1FVmpzHGF0aY0xJOC+NMbokRmVJoZKJiADZJYdJwNqU9QbgpF3VCVsJTQTdQlvM7CTgIeAwgi6ihJnt6phVwHZ3T6SUT8oUlJldDVwNcOihh2ZxGgeHkliUGTUVzKipSNvWtLOLhm1tbNzRzsYdHb3zzc0dbG3t4M1t22ls6aS5I5HhyIHyomiQLErDJFISY0yYUMaUxsN5MFWVxSiJZfNPSEQONMP+P9vdXwOmm9mHgYfN7JdDdNz7Cccz6urqDvxR9SFQUVxIRXEF0yemJ45UHYlutrd1sbW1k22tnTS2drK9rZOtrV1sawvWt7V2sm57O2+ta2Jraydd3Zl/xEWFBVSVxvsSRpg0xoStlLHh8tiyGGPL4hQVRobj1EVkiGWTHNYBk1PWa8KyTHUazCwKVBAMTPdy9xVm1gIcu5tjNgKVZhYNWw+ZPkv2UTwaYXx5hPFZPoTn7jR3JNja0snWts5gHiaVbW2dNLZ0srW1g8bWTlZtaqGxtYP2rp6MxyqNRagqi6clktElhYwuiVERzsviUcriUUrjEUrjUeLRAj1ZLrIfZZMcFgFHhXcRrQPm0TcmkLQQuBz4LXAR8IK7e7jP2rAr6TDgGGANsD3TMcN9XgyPMT885pP7doqyr8yM8qJCyosKmUJpVvu0dSZobAkSyJbmDra2drKltYPGlk62tATrG5raWbZ+9y2TVIURozBSQGGkgGiBUVBgRMyIFBjRiBGLFBCLBlNpLEpFcSHlxVHKiwupLI71tmSqSuNUlhRSXBihOBahuDBCNFKAu9Pd4yQyfC9Hc3uCtzcENwW8vX4H67bvZFRRIZXFhVSWBD+beLTv82PRAoy+ZFZgwRjSqKJgKi8qpLw42K+oUIlP8s+gySG8sF8LPEtw2+lD7r7czG4H6t19IfAg8KiZrQK2ElzsAU4FbjSzLqAH+KK7bwHIdMxwn68D883sDuD34bHlAFMSi1IyJsrkMSWD1nV3Wju72d7Wyfa2Lra3ddHS0UVLRzetHQlaOhJ0JHro6u6hK5wnepye8GLe3QOJnh46Ez10JIJ5a2eCDU072dGeoGlnF52JzC2ZJDPI9pGfmtHFHFZVQtPOLj5obKVpZxdNO7vY2+96ikUKKC+Oht2ChVSWxHqXy8N5cupLLFFGxQspjQeJTWSo6SE4GRGSLZktLUHrpWlnFzu7umnv6qats5uu7h4KzHpbJAXhX/Ie3mBXXBjhmEPKmTahnIqSwoyfkejuS06d3f2TUXeP09qRYEd7gh3tXTS3J9gRJpUd7V29y9vb+ubJeoOJRwsoi0d7W0HFsQhF0QixaAHRSHBO0YICIpHgvAoMCswwAyM5D8qCcw+WIwVhqyxlXhgpoDBaQCycx/tNEeKFBRQVBnEUFUYoiUV61/VMTv7Z11tZRQ54e9KS2VvRSAHRSAGlQ/iarO4ep7k9SBjJhBIkkwTNHQlaw6m5I0F7Z3dvwmvv6mFnVzeJ9h66up1ETw/dPY479LjT4/Q+YOkepMBkuYfzRHdPMA/3zabrb3eiBRYkjViQNFITSG95cj0WpSSsVxILxp5KYlFKY8EYVHIsKlmm1tPQU3IQyWORAqOyJJYXr09Jjsl0dTud3X0tpI6ubjoSPb1JqT3RnZKoelJaaAl2dvawsytBW2fQYmvv6mZnZ3fQkgvLdoZlA1tfuxOPFvQljTCxlMajwXKyLJwH5cH2sqIoo8J5WTzoqisrihIp0BiQkoOIZMUsGPiPRqCY4b8luas7SCxtHd20dgYtpLbOYByqNTlPliW3d3TTEpY1tyfYuKOd1o4gMbV2ZJ9wSmOR3hsGkuNByRsbRpcENyFUlvQ9PFoZ3mVXEoscNDcXKDmISF5K3plWXpR5jGdvdHX3BAkkTCYtHQla2oN5czjG0zcFXXhNO7tYt72dFRua2d7WSWtn9y6PH4sUMLo0SBSjS/oeGE0+VJq8hXtMWbitJJa3XWJKDiIyYhRGCqgoKdjlTQXZ6Ez0sH1ncGfdttZOtu/sYntbJ9vagodIt7d2sbUteJB0xZ92sLU1qLsro0sKqSoL3j4wtix4YLSqLN67PHZUnOpwvTi2/x4iVXIQEdkDsWgB40YVMW5U9t/kmOjuYfvO4K0Ejb0PkXawJXyAdGtrJ1tagmSypbmDHbu4Sy35EGlVWV9L5FPH1/CRw6uG6vR6KTmIiAyzaKQgbAnEYfzg9TsS3b23Xm9p6WBLcyebw+Vkglm3vZ03G5o4aerQJwZQchARyTvxaISJlcVMrCzOWQz5ORIiIiI5peQgIiJplBxERCSNkoOIiKRRchARkTRKDiIikkbJQURE0ig5iIhIGiUHERFJo+QgIiJplBxERCRNVsnBzM42s5VmtsrMbsywPW5mC8Ltr5nZlLB8jpktNrO3wvnHwvJRZrY0ZdpiZveG264ws80p2z4/dKcrIiLZGPTFe2YWAe4D5gANwCIzW+jub6dUuwrY5u5Hmtk84G7gYmALcJ67rzezY4FngUnu3gzUpnzGYuC/Uo63wN2v3bdTExGRvZVNy2E2sMrd33P3TmA+cMGAOhcAD4fLjwNnmpm5++/dfX1YvhwoNrN+X79uZh8CxgGv7O1JiIjI0MomOUwC1qasN4RlGeu4ewJoAga+ZPwvgCXu3jGgfB5BS8FT65rZm2b2uJlNziJGEREZQvtlQNrMphN0NX0hw+Z5wGMp608BU9x9JvAcfS2Sgce82szqzax+8+bNQx2yiMiIlk1yWAek/vVeE5ZlrGNmUaACaAzXa4AngL9y99WpO5nZcUDU3Rcny9y9MaV18QPghExBufv97l7n7nXV1dVZnIaIiGQrm+SwCDjKzKaaWYzgL/2FA+osBC4Ply8CXnB3N7NK4GngRnf/TYZjX0L/VgNmNiFl9XxgRRYxiojIEBr0biV3T5jZtQR3GkWAh9x9uZndDtS7+0LgQeBRM1sFbCVIIADXAkcCt5jZLWHZx919U7j8l8C5Az7yOjM7H0iEx7pir89ORET2ivUfBz4w1dXVeX19fa7DEBE5oJjZYnevy7RNT0iLiEgaJQcREUmj5CAiImmUHEREJI2Sg4iIpFFyEBGRNEoOIiKSRslBRETSKDmIiEgaJQcREUmj5CAiImmUHEREJI2Sg4iIpFFyEBGRNEoOIiKSRslBRETSKDl0tuY6AhGRvDOyk8Ov74W7DoVER64jERHJKyM7OVROhp4EbF6Z60hERPLKyE4O448N5pvezm0cIiJ5JqvkYGZnm9lKM1tlZjdm2B43swXh9tfMbEpYPsfMFpvZW+H8Yyn7vBQec2k4jdvdsYbFmCMgEoeNy4ftI0REDkSDJgcziwD3AecA04BLzGzagGpXAdvc/UjgHuDusHwLcJ67zwAuBx4dsN9l7l4bTpsGOdbQi0Sh+kNqOYiIDJBNy2E2sMrd33P3TmA+cMGAOhcAD4fLjwNnmpm5++/dfX1YvhwoNrP4IJ+X8VhZxLl3xk1Xy0FEZIBsksMkYG3KekNYlrGOuyeAJqBqQJ2/AJa4e+qtQT8Mu5S+mZIAsjnW0Bk/DZo3QNvWYfsIEZEDzX4ZkDaz6QTdQ19IKb4s7G76aDh9dg+PebWZ1ZtZ/ebNm/c+uHHTg7m6lkREemWTHNYBk1PWa8KyjHXMLApUAI3heg3wBPBX7r46uYO7rwvnzcB/EnRf7fZYqdz9fnevc/e66urqLE5jF8aHwycblRxERJKySQ6LgKPMbKqZxYB5wMIBdRYSDDgDXAS84O5uZpXA08CN7v6bZGUzi5rZ2HC5EJgLLNvdsfb4zLI1agIUVcImjTuIiCRFB6vg7gkzuxZ4FogAD7n7cjO7Hah394XAg8CjZrYK2EqQQACuBY4EbjGzW8KyjwOtwLNhYogAvwIeCLfv6ljDwyx43kEtBxGRXjacf5TvL3V1dV5fX7/3B/jFDbD0MbhpbZAsRERGADNb7O51mbaN7Cekk8ZNg85m2P5BriMREckLSg4A43XHkohIKiUHgHEfDuYbl+2+nojICKHkABAfBZWHalBaRCSk5JA0brq6lUREQkoOSeOnwZZ39cU/IiIoOfQZPx28G7b8IdeRiIjknJJDUvIdSxp3EBFRcuhVdQREYnqNhogISg59IoUw9mi1HEREUHLob/w03bEkIoKSQ3/jp8OOdbB97eB1RUQOYkoOqaZdCJE4vHhnriMREckpJYdUow+Dk78IbzwG6xbnOhoRkZxRchjo1L+F0mp45htwELzOXERkbyg5DFRUDh/7Jqz9Hbz981xHIyKSE0oOmcz6DIyfAc/dAl3tuY5GRGS/U3LIpCACZ90ZfPnP7/4119GIiOx3Sg67cvhpcPQn4JVvwbY1uY5GRGS/UnLYnbPuhIIo/OfF0N6U62hERPabrJKDmZ1tZivNbJWZ3Zhhe9zMFoTbXzOzKWH5HDNbbGZvhfOPheUlZva0mb1jZsvN7K6UY11hZpvNbGk4fX6IznXPjZkKf/kINK6Cx6+E7kTOQhER2Z8GTQ5mFgHuA84BpgGXmNm0AdWuAra5+5HAPcDdYfkW4Dx3nwFcDjyass8/u/sxwCzgz8zsnJRtC9y9Npx+sDcnNmQOPw0+8S1Y9St49hs5DUVEZH/JpuUwG1jl7u+5eycwH7hgQJ0LgIfD5ceBM83M3P337r4+LF8OFJtZ3N3b3P1FgPCYS4CafT2ZYXPCFXDytfD6v8PrD+Q6GhGRYZdNcpgEpL5sqCEsy1jH3RNAE1A1oM5fAEvcvd9XrZlZJXAe8HxqXTN708weN7PJWcQ4/ObcDh86G375dXjt3/WAnIgc1PbLgLSZTSfoavrCgPIo8BjwXXd/Lyx+Cpji7jOB5+hrkQw85tVmVm9m9Zs3bx6+4JMKIvAXP4Cj5sAvvwY/vQLadwz/54qI5EA2yWEdkPrXe01YlrFOeMGvABrD9RrgCeCv3H31gP3uB95193uTBe7emNK6+AFwQqag3P1+d69z97rq6uosTmMIxEfBvMfgz2+DFQvhgTNgo74cSEQOPtkkh0XAUWY21cxiwDxg4YA6CwkGnAEuAl5wdw+7jJ4GbnT336TuYGZ3ECSRrwwon5Cyej6wIrtT2U8KCuDUr8LlT0FHMzxwJjx7M+xYP/i+IiIHiEGTQziGcC3wLMGF+ifuvtzMbjez88NqDwJVZrYK+FsgebvrtcCRwC0pt6aOC1sTNxPc/bRkwC2r14W3t74BXAdcMTSnOsSmnApfeAWO+UTwFPW9M+HnX4LNK3MdmYjIPjM/CAZW6+rqvL6+PncBbFsDv70PljwKiZ1Qc2Lw3RDTzofKQ3MXl4jIbpjZYnevy7hNyWEItW6BJQ/D8p/Dn94MyiadAEecCVM/GiSNwuKchigikqTkkAuNq4NB6xVPwfrfg/cE3zJXcyLUnACHzIQJx8GYI4JxDBGR/UzJIdfam+CD38EfX4b3fxPc4dTdGWwrLIXqD0HVUTA2nEZPDb6Vrnh0buMWkYPa7pJDdH8HMyIVVcCHzgomgEQnbH4n6Hra8CZsWQnvvwpv/aT/fvEKGH0oVBwK5ROhYhKU18CoQ4KpbBzEy8Fs/5+TiBzUlBxyIRqDCTODaVZKeWdr0B21bQ1sfx+2vR/O18D7v878ZthocfC1piWjoXgMlFRBcWXwTEZ8VJA8YmXBWEdhSd88Gg+Wo0XBVBjOCyL752cgBwf38G0BHnSd9i6H82SdYCF9e3If74GebvDu/vOebujpgp5EMHUngvXusKy7q2+9O6Vev2nA8bwn5XN6+uLJFH+yvF+s3buJuWfAeiIsSynvSWSu6z396/Yev2fAZ/f0P49PfAvqrhzyX62SQz6JlfYljUw6WmDHOmjeAC2boPlP0LIxGAjfuRXatgbJZOf24BmMnq49j6GgMEwgYeJITSjJ8oFlsZKUstIM9ZLHSi4XQ2QI/um5B91zifb+F4buLnovTAPrJ/+jJTqgY0fwlHvHjiDx7twWTG1bg2MWlgTnFisN9m/+E+zYAM3robUxvGiF/9lxiMSCpBuJB3MzsIK+qSDaN0UKU9YjYVmsbx6JBXUihcHvpHc5Gq5HwSLBvlYQLJsNuKh1D7h4dgbn3Tu1h1NH37y7s69e7880eeHNcPHO9HM+EPT72YUTFrbCDYy+9YHbe/e1vp99QaT/MVPXk79jK+xbz1S337IN+N0m47SUeuG2Q44blh+RksOBJF4G1UcH02DcwwtgM3Q2B1932tUWTjuDKdER3Hrb1Z5yoWhP2d6eUr8d2hrD5da+43W2slcXCCsIL6KxsMVS2P8/UvIckhe65EW/uzOchxe0oRavCFpehcXh+YXn6D1QPgFGTYSJs6B0XP8LNJZyYW0Pug5JSUbJJJL8S7i7q+8C29WZchFP9J1jT+r5dvYlv71VUBi2FMOfeSQWJut4kLALi4NzTya5ZBIqSElMyQtTcjnt4gr9L7LJLs9w3q/cBlygUy68qRfQfkk1NZ4M8SXLU5Nu2gVZ3bDZUHI4WJkFXUWFRcAwvl4kmYQSO4ML6cDk0bWzb55MPL1/pab8Fdt78Uy5AKZeSHr/eg7/oo7Gwwtdyl/qqRcSK0iPM3khgqB+vByKysN5BRRVDk2LZjilJsqBXQy9Uv7iTW156KIoeyDP/ydI3ktNQrq7aviZ9XUxiQwj3WAvIiJplBxERCSNkoOIiKRRchARkTRKDiIikkbJQURE0ig5iIhIGiUHERFJc1C8stvMNgPv7+XuY4EtQxjOcMj3GPM9PlCMQyHf44P8jzHf4jvM3TO+QuGgSA77wszqd/U+83yR7zHme3ygGIdCvscH+R9jvseXSt1KIiKSRslBRETSKDnA/bkOIAv5HmO+xweKcSjke3yQ/zHme3y9RvyYg4iIpFPLQURE0ig5iIhImhGdHMzsbDNbaWarzOzGXMcDYGYPmdkmM1uWUjbGzJ4zs3fDec6+VcfMJpvZi2b2tpktN7Mv52GMRWb2upm9Ecb4v8PyqWb2Wvj7XmBmsVzFGMYTMbPfm9l/52l8a8zsLTNbamb1YVk+/Z4rzexxM3vHzFaY2cl5Ft/R4c8uOe0ws6/kU4y7M2KTg5lFgPuAc4BpwCVmNi23UQHwI+DsAWU3As+7+1HA8+F6riSAv3P3acBHgC+FP7d8irED+Ji7HwfUAmeb2UeAu4F73P1IYBtwVe5CBODLwIqU9XyLD+AMd69NuTc/n37P3wGecfdjgOMIfpZ5E5+7rwx/drXACUAb8EQ+xbhb7j4iJ+Bk4NmU9ZuAm3IdVxjLFGBZyvpKYEK4PAFYmesYU2J7EpiTrzECJcAS4CSCJ1OjmX7/OYirhuDC8DHgvwHLp/jCGNYAYweU5cXvGagA/kh4U02+xZch3o8Dv8nnGAdOI7blAEwC1qasN4Rl+Wi8u28Il/8EjM9lMElmNgWYBbxGnsUYdtksBTYBzwGrge3ungir5Pr3fS/wNaAnXK8iv+IDcOB/zGyxmV0dluXL73kqsBn4Ydg19wMzK82j+AaaBzwWLudrjP2M5ORwQPLgz42c339sZmXAz4CvuPuO1G35EKO7d3vQnK8BZgPH5DKeVGY2F9jk7otzHcsgTnX34wm6Xr9kZv8rdWOOf89R4Hjg++4+C2hlQPdMPvw7BAjHjs4HfjpwW77EmMlITg7rgMkp6zVhWT7aaGYTAML5plwGY2aFBInhx+7+X2FxXsWY5O7bgRcJumkqzSwabsrl7/vPgPPNbA0wn6Br6TvkT3wAuPu6cL6JoK98Nvnze24AGtz9tXD9cYJkkS/xpToHWOLuG8P1fIwxzUhODouAo8I7RGIEzb6FOY5pVxYCl4fLlxP08+eEmRnwILDC3b+dsimfYqw2s8pwuZhgTGQFQZK4KKyWsxjd/SZ3r3H3KQT/7l5w98vyJT4AMys1s1HJZYI+82Xkye/Z3f8ErDWzo8OiM4G3yZP4BriEvi4lyM8Y0+V60COXE3Au8AeC/uibcx1PGNNjwAagi+Cvo6sI+qOfB94FfgWMyWF8pxI0g98ElobTuXkW40zg92GMy4BbwvLDgdeBVQRN/Hge/L5PB/473+ILY3kjnJYn/3/k2e+5FqgPf88/B0bnU3xhjKVAI1CRUpZXMe5q0uszREQkzUjuVhIRkV1QchARkTRKDiIikkbJQURE0ig5iIhIGiUHERFJo+QgIiJp/j8NvCRPbwfiDwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 22:58:51,072 | train_utils.py:140 | Best Loss: 3.5939461374057576e-06, Best epoch: 76\n"
     ]
    }
   ],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
