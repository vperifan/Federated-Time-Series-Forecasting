{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da638ca",
   "metadata": {},
   "source": [
    "### In this notebook we perform individual training.\n",
    "In individual learning each base station has access only to it's private dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3d7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent = Path(os.path.abspath(\"\")).resolve().parents[0]\n",
    "if parent not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15abc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.data_utils import read_data, generate_time_lags, time_to_feature, handle_nans, to_Xy, \\\n",
    "    to_torch_dataset, to_timeseries_rep, assign_statistics, \\\n",
    "    to_train_val, scale_features, get_data_by_area, remove_identifiers, get_exogenous_data_by_area, handle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350c9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.utils.train_utils import train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4688fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.models.mlp import MLP\n",
    "from ml.models.rnn import RNN\n",
    "from ml.models.lstm import LSTM\n",
    "from ml.models.gru import GRU\n",
    "from ml.models.cnn import CNN\n",
    "from ml.models.rnn_autoencoder import DualAttentionAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3db1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path='../dataset/full_dataset.csv', # dataset\n",
    "    data_path_test=['../dataset/ElBorn_test.csv'], # test dataset\n",
    "    test_size=0.2, # validation size \n",
    "    targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], # the target columns\n",
    "    num_lags=10, # the number of past observations to feed as input\n",
    "\n",
    "    \n",
    "    filter_bs=None, # whether to use a single bs for training. It will be changed dynamically\n",
    "    identifier='District', # the column name that identifies a bs\n",
    "\n",
    "    nan_constant=0, # the constant to transform nan values\n",
    "    x_scaler='minmax', # x_scaler\n",
    "    y_scaler='minmax', # y_scaler\n",
    "    outlier_detection=None, # whether to perform flooring and capping\n",
    "\n",
    "    \n",
    "    criterion='mse', # optimization criterion, mse or l1\n",
    "    epochs=150, # the number of maximum epochs\n",
    "    lr=0.001, # learning rate\n",
    "    optimizer='adam', # the optimizer, it can be sgd or adam\n",
    "    batch_size=128, # the batch size to use\n",
    "    early_stopping=True, # whether to use early stopping\n",
    "    patience=50, # patience value for the early stopping parameter (if specified)\n",
    "    max_grad_norm=0.0, # whether to clip grad norm\n",
    "    reg1=0.0, # l1 regularization\n",
    "    reg2=0.0, # l2 regularization\n",
    "    \n",
    "    plot_history=True, # plot loss history\n",
    "\n",
    "    cuda=True, # whether to use gpu\n",
    "    \n",
    "    seed=0, # reproducibility\n",
    "\n",
    "    assign_stats=None, # whether to use statistics as exogenous data, [\"mean\", \"median\", \"std\", \"variance\", \"kurtosis\", \"skew\"]\n",
    "    use_time_features=False # whether to use datetime features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a743e",
   "metadata": {},
   "source": [
    "> You can define the base station to perform train on the filter_bs parameter and use it in block 12 or you can define the base station to block 12 explicitly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763c39ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script arguments: Namespace(data_path='../dataset/full_dataset.csv', data_path_test=['../dataset/ElBorn_test.csv'], test_size=0.2, targets=['rnti_count', 'rb_down', 'rb_up', 'down', 'up'], num_lags=10, filter_bs=None, identifier='District', nan_constant=0, x_scaler='minmax', y_scaler='minmax', outlier_detection=None, criterion='mse', epochs=150, lr=0.001, optimizer='adam', batch_size=128, early_stopping=True, patience=50, max_grad_norm=0.0, reg1=0.0, reg2=0.0, plot_history=True, cuda=True, seed=0, assign_stats=None, use_time_features=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Script arguments: {args}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da3431ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if args.cuda and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bb4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection specification\n",
    "if args.outlier_detection is not None:\n",
    "    outlier_columns = ['rb_down', 'rb_up', 'down', 'up']\n",
    "    outlier_kwargs = {\"ElBorn\": (10, 90), \"LesCorts\": (10, 90), \"PobleSec\": (5, 95)}\n",
    "    args.outlier_columns = outlier_columns\n",
    "    args.outlier_kwargs = outlier_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac1d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all():\n",
    "    # ensure reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea3ddd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00543376",
   "metadata": {},
   "source": [
    "### The preprocessing pipeline performed here for the base station specified in filter_bs argument\n",
    "Preprocessing inlcudes:\n",
    "1. NaNs Handling\n",
    "2. Outliers Handling\n",
    "3. Scaling Data\n",
    "4. Generating time lags\n",
    "5. Generating and importing exogenous data as features (time, statistics) (if applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35bc6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessing(filter_bs=None):\n",
    "    \"\"\"Preprocess a given .csv\"\"\"\n",
    "    # read data\n",
    "    df = read_data(args.data_path, filter_data=filter_bs)\n",
    "    # handle nans\n",
    "    df = handle_nans(train_data=df, constant=args.nan_constant,\n",
    "                     identifier=args.identifier)\n",
    "    # split to train/validation\n",
    "    train_data, val_data = to_train_val(df)\n",
    "    \n",
    "    # handle outliers (if specified)\n",
    "    if args.outlier_detection is not None:\n",
    "        train_data = handle_outliers(df=train_data, columns=args.outlier_columns,\n",
    "                                     identifier=args.identifier, kwargs=args.outlier_kwargs)\n",
    "    \n",
    "    # get X and y\n",
    "    X_train, X_val, y_train, y_val = to_Xy(train_data=train_data, val_data=val_data,\n",
    "                                          targets=args.targets)\n",
    "    \n",
    "    # scale X\n",
    "    X_train, X_val, x_scaler = scale_features(train_data=X_train, val_data=X_val,\n",
    "                                             scaler=args.x_scaler, identifier=args.identifier)\n",
    "    # scale y\n",
    "    y_train, y_val, y_scaler = scale_features(train_data=y_train, val_data=y_val,\n",
    "                                             scaler=args.y_scaler, identifier=args.identifier)\n",
    "    \n",
    "    # generate time lags\n",
    "    X_train = generate_time_lags(X_train, args.num_lags)\n",
    "    X_val = generate_time_lags(X_val, args.num_lags)\n",
    "    y_train = generate_time_lags(y_train, args.num_lags, is_y=True)\n",
    "    y_val = generate_time_lags(y_val, args.num_lags, is_y=True)\n",
    "    \n",
    "    # get datetime features as exogenous data\n",
    "    date_time_df_train = time_to_feature(\n",
    "        X_train, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    date_time_df_val = time_to_feature(\n",
    "        X_val, args.use_time_features, identifier=args.identifier\n",
    "    )\n",
    "    \n",
    "    # get statistics as exogenous data\n",
    "    stats_df_train = assign_statistics(X_train, args.assign_stats, args.num_lags,\n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    stats_df_val = assign_statistics(X_val, args.assign_stats, args.num_lags, \n",
    "                                       targets=args.targets, identifier=args.identifier)\n",
    "    \n",
    "    # concat the exogenous features (if any) to a single dataframe\n",
    "    if date_time_df_train is not None or stats_df_train is not None:\n",
    "        exogenous_data_train = pd.concat([date_time_df_train, stats_df_train], axis=1)\n",
    "        # remove duplicate columns (if any)\n",
    "        exogenous_data_train = exogenous_data_train.loc[:, ~exogenous_data_train.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_train) == len(X_train) == len(y_train)\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "    if date_time_df_val is not None or stats_df_val is not None:\n",
    "        exogenous_data_val = pd.concat([date_time_df_val, stats_df_val], axis=1)\n",
    "        exogenous_data_val = exogenous_data_val.loc[:, ~exogenous_data_val.columns.duplicated()].copy()\n",
    "        assert len(exogenous_data_val) == len(X_val) == len(y_val)\n",
    "    else:\n",
    "        exogenous_data_val = None\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fc93eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 16:30:35,446 | data_utils.py:28 | Reading LesCorts's data...\n",
      "INFO logger 2022-10-24 16:30:35,511 | data_utils.py:395 | \tTotal number of samples:  6892\n",
      "INFO logger 2022-10-24 16:30:35,511 | data_utils.py:396 | \tNumber of samples for training: 5514\n",
      "INFO logger 2022-10-24 16:30:35,512 | data_utils.py:397 | \tNumber of samples for validation:  1378\n"
     ]
    }
   ],
   "source": [
    "# here exogenous_data_train and val are None.\n",
    "X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler = make_preprocessing(\n",
    "    filter_bs=\"LesCorts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b99c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb_up_var_lag-10</th>\n",
       "      <th>rb_up_lag-10</th>\n",
       "      <th>rb_down_var_lag-10</th>\n",
       "      <th>rb_down_lag-10</th>\n",
       "      <th>mcs_up_var_lag-10</th>\n",
       "      <th>mcs_up_lag-10</th>\n",
       "      <th>mcs_down_var_lag-10</th>\n",
       "      <th>mcs_down_lag-10</th>\n",
       "      <th>rnti_count_lag-10</th>\n",
       "      <th>up_lag-10</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_down_var_lag-1</th>\n",
       "      <th>rb_down_lag-1</th>\n",
       "      <th>mcs_up_var_lag-1</th>\n",
       "      <th>mcs_up_lag-1</th>\n",
       "      <th>mcs_down_var_lag-1</th>\n",
       "      <th>mcs_down_lag-1</th>\n",
       "      <th>rnti_count_lag-1</th>\n",
       "      <th>up_lag-1</th>\n",
       "      <th>down_lag-1</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>1.083682e-08</td>\n",
       "      <td>0.036054</td>\n",
       "      <td>1.961025e-08</td>\n",
       "      <td>0.109670</td>\n",
       "      <td>0.493544</td>\n",
       "      <td>0.431702</td>\n",
       "      <td>0.459601</td>\n",
       "      <td>0.160269</td>\n",
       "      <td>0.210509</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974816e-08</td>\n",
       "      <td>0.114193</td>\n",
       "      <td>0.572030</td>\n",
       "      <td>0.400005</td>\n",
       "      <td>0.460896</td>\n",
       "      <td>0.164719</td>\n",
       "      <td>0.212757</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>0.131219</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>1.061491e-08</td>\n",
       "      <td>0.030426</td>\n",
       "      <td>1.644211e-08</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.527483</td>\n",
       "      <td>0.476772</td>\n",
       "      <td>0.392094</td>\n",
       "      <td>0.131085</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.791601e-08</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.576015</td>\n",
       "      <td>0.445876</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>0.145817</td>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>9.109702e-09</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>1.761309e-08</td>\n",
       "      <td>0.106045</td>\n",
       "      <td>0.501063</td>\n",
       "      <td>0.471773</td>\n",
       "      <td>0.453949</td>\n",
       "      <td>0.152725</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.766410e-08</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.640010</td>\n",
       "      <td>0.448971</td>\n",
       "      <td>0.387407</td>\n",
       "      <td>0.130386</td>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>5.813802e-09</td>\n",
       "      <td>0.026050</td>\n",
       "      <td>1.668480e-08</td>\n",
       "      <td>0.098968</td>\n",
       "      <td>0.499865</td>\n",
       "      <td>0.450949</td>\n",
       "      <td>0.382516</td>\n",
       "      <td>0.129036</td>\n",
       "      <td>0.183768</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963081e-08</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.534812</td>\n",
       "      <td>0.414812</td>\n",
       "      <td>0.476978</td>\n",
       "      <td>0.171880</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>7.246522e-09</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>1.721707e-08</td>\n",
       "      <td>0.102669</td>\n",
       "      <td>0.535319</td>\n",
       "      <td>0.429488</td>\n",
       "      <td>0.412994</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>0.191964</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.895823e-08</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.547422</td>\n",
       "      <td>0.456544</td>\n",
       "      <td>0.448805</td>\n",
       "      <td>0.157300</td>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rb_up_var_lag-10  rb_up_lag-10  rb_down_var_lag-10  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00      1.083682e-08      0.036054        1.961025e-08   \n",
       "2019-01-12 17:34:00      1.061491e-08      0.030426        1.644211e-08   \n",
       "2019-01-12 17:36:00      9.109702e-09      0.038901        1.761309e-08   \n",
       "2019-01-12 17:38:00      5.813802e-09      0.026050        1.668480e-08   \n",
       "2019-01-12 17:40:00      7.246522e-09      0.028981        1.721707e-08   \n",
       "\n",
       "                     rb_down_lag-10  mcs_up_var_lag-10  mcs_up_lag-10  \\\n",
       "time                                                                    \n",
       "2019-01-12 17:32:00        0.109670           0.493544       0.431702   \n",
       "2019-01-12 17:34:00        0.098938           0.527483       0.476772   \n",
       "2019-01-12 17:36:00        0.106045           0.501063       0.471773   \n",
       "2019-01-12 17:38:00        0.098968           0.499865       0.450949   \n",
       "2019-01-12 17:40:00        0.102669           0.535319       0.429488   \n",
       "\n",
       "                     mcs_down_var_lag-10  mcs_down_lag-10  rnti_count_lag-10  \\\n",
       "time                                                                           \n",
       "2019-01-12 17:32:00             0.459601         0.160269           0.210509   \n",
       "2019-01-12 17:34:00             0.392094         0.131085           0.200300   \n",
       "2019-01-12 17:36:00             0.453949         0.152725           0.221468   \n",
       "2019-01-12 17:38:00             0.382516         0.129036           0.183768   \n",
       "2019-01-12 17:40:00             0.412994         0.139577           0.191964   \n",
       "\n",
       "                     up_lag-10  ...  rb_down_var_lag-1  rb_down_lag-1  \\\n",
       "time                            ...                                     \n",
       "2019-01-12 17:32:00   0.017705  ...       1.974816e-08       0.114193   \n",
       "2019-01-12 17:34:00   0.018282  ...       1.791601e-08       0.105544   \n",
       "2019-01-12 17:36:00   0.019501  ...       1.766410e-08       0.101075   \n",
       "2019-01-12 17:38:00   0.012594  ...       1.963081e-08       0.114777   \n",
       "2019-01-12 17:40:00   0.013056  ...       1.895823e-08       0.107131   \n",
       "\n",
       "                     mcs_up_var_lag-1  mcs_up_lag-1  mcs_down_var_lag-1  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00          0.572030      0.400005            0.460896   \n",
       "2019-01-12 17:34:00          0.576015      0.445876            0.427184   \n",
       "2019-01-12 17:36:00          0.640010      0.448971            0.387407   \n",
       "2019-01-12 17:38:00          0.534812      0.414812            0.476978   \n",
       "2019-01-12 17:40:00          0.547422      0.456544            0.448805   \n",
       "\n",
       "                     mcs_down_lag-1  rnti_count_lag-1  up_lag-1  down_lag-1  \\\n",
       "time                                                                          \n",
       "2019-01-12 17:32:00        0.164719          0.212757  0.018888    0.131219   \n",
       "2019-01-12 17:34:00        0.145817          0.192432  0.014424    0.113276   \n",
       "2019-01-12 17:36:00        0.130386          0.173137  0.011194    0.101414   \n",
       "2019-01-12 17:38:00        0.171880          0.205545  0.023585    0.141225   \n",
       "2019-01-12 17:40:00        0.157300          0.181567  0.022127    0.125069   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec22537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rnti_count</th>\n",
       "      <th>rb_down</th>\n",
       "      <th>rb_up</th>\n",
       "      <th>down</th>\n",
       "      <th>up</th>\n",
       "      <th>District</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:32:00</th>\n",
       "      <td>0.192432</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.033785</td>\n",
       "      <td>0.113276</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:34:00</th>\n",
       "      <td>0.173137</td>\n",
       "      <td>0.101075</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.101414</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:36:00</th>\n",
       "      <td>0.205545</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.060088</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:38:00</th>\n",
       "      <td>0.181567</td>\n",
       "      <td>0.107131</td>\n",
       "      <td>0.042592</td>\n",
       "      <td>0.125069</td>\n",
       "      <td>0.022127</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-12 17:40:00</th>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.101726</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.011213</td>\n",
       "      <td>LesCorts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rnti_count   rb_down     rb_up      down        up  \\\n",
       "time                                                                      \n",
       "2019-01-12 17:32:00    0.192432  0.105544  0.033785  0.113276  0.014424   \n",
       "2019-01-12 17:34:00    0.173137  0.101075  0.025216  0.101414  0.011194   \n",
       "2019-01-12 17:36:00    0.205545  0.114777  0.060088  0.141225  0.023585   \n",
       "2019-01-12 17:38:00    0.181567  0.107131  0.042592  0.125069  0.022127   \n",
       "2019-01-12 17:40:00    0.175900  0.101726  0.023463  0.107920  0.011213   \n",
       "\n",
       "                     District  \n",
       "time                           \n",
       "2019-01-12 17:32:00  LesCorts  \n",
       "2019-01-12 17:34:00  LesCorts  \n",
       "2019-01-12 17:36:00  LesCorts  \n",
       "2019-01-12 17:38:00  LesCorts  \n",
       "2019-01-12 17:40:00  LesCorts  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367c2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MinMaxScaler(), MinMaxScaler())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a9c30",
   "metadata": {},
   "source": [
    "### Postprocessing Stage\n",
    "\n",
    "In this stage we transform data in a way that can be fed into ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c417082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler):\n",
    "    \"\"\"Make data ready to be fed into ml algorithms\"\"\"\n",
    "    # if there are more than one specified areas, get the data per area\n",
    "    if X_train[args.identifier].nunique() != 1:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = get_data_by_area(X_train, X_val,\n",
    "                                                                              y_train, y_val, \n",
    "                                                                              identifier=args.identifier)\n",
    "    else:\n",
    "        area_X_train, area_X_val, area_y_train, area_y_val = None, None, None, None\n",
    "\n",
    "    # Get the exogenous data per area.\n",
    "    if exogenous_data_train is not None:\n",
    "        exogenous_data_train, exogenous_data_val = get_exogenous_data_by_area(exogenous_data_train,\n",
    "                                                                              exogenous_data_val)\n",
    "    # transform to np\n",
    "    if area_X_train is not None:\n",
    "        for area in area_X_train:\n",
    "            tmp_X_train, tmp_y_train, tmp_X_val, tmp_y_val = remove_identifiers(\n",
    "                area_X_train[area], area_y_train[area], area_X_val[area], area_y_val[area])\n",
    "            tmp_X_train, tmp_y_train = tmp_X_train.to_numpy(), tmp_y_train.to_numpy()\n",
    "            tmp_X_val, tmp_y_val = tmp_X_val.to_numpy(), tmp_y_val.to_numpy()\n",
    "            area_X_train[area] = tmp_X_train\n",
    "            area_X_val[area] = tmp_X_val\n",
    "            area_y_train[area] = tmp_y_train\n",
    "            area_y_val[area] = tmp_y_val\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train[area] = exogenous_data_train[area].to_numpy()\n",
    "            exogenous_data_val[area] = exogenous_data_val[area].to_numpy()\n",
    "    \n",
    "    # remove identifiers from features, targets\n",
    "    X_train, y_train, X_val, y_val = remove_identifiers(X_train, y_train, X_val, y_val)\n",
    "    assert len(X_train.columns) == len(X_val.columns)\n",
    "    \n",
    "    num_features = len(X_train.columns) // args.num_lags\n",
    "    \n",
    "    # to timeseries representation\n",
    "    X_train = to_timeseries_rep(X_train.to_numpy(), num_lags=args.num_lags,\n",
    "                                            num_features=num_features)\n",
    "    X_val = to_timeseries_rep(X_val.to_numpy(), num_lags=args.num_lags,\n",
    "                                          num_features=num_features)\n",
    "    \n",
    "    if area_X_train is not None:\n",
    "        area_X_train = to_timeseries_rep(area_X_train, num_lags=args.num_lags,\n",
    "                                                     num_features=num_features)\n",
    "        area_X_val = to_timeseries_rep(area_X_val, num_lags=args.num_lags,\n",
    "                                                   num_features=num_features)\n",
    "    \n",
    "    # transform targets to numpy\n",
    "    y_train, y_val = y_train.to_numpy(), y_val.to_numpy()\n",
    "    \n",
    "    # centralized (all) learning specific\n",
    "    if not args.filter_bs and exogenous_data_train is not None:\n",
    "        exogenous_data_train_combined, exogenous_data_val_combined = [], []\n",
    "        for area in exogenous_data_train:\n",
    "            exogenous_data_train_combined.extend(exogenous_data_train[area])\n",
    "            exogenous_data_val_combined.extend(exogenous_data_val[area])\n",
    "        exogenous_data_train_combined = np.stack(exogenous_data_train_combined)\n",
    "        exogenous_data_val_combined = np.stack(exogenous_data_val_combined)\n",
    "        exogenous_data_train[\"all\"] = exogenous_data_train_combined\n",
    "        exogenous_data_val[\"all\"] = exogenous_data_val_combined\n",
    "    return X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c59dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val, area_X_train, area_X_val, area_y_train, area_y_val, exogenous_data_train, exogenous_data_val = make_postprocessing(X_train, X_val, y_train, y_val, exogenous_data_train, exogenous_data_val, x_scaler, y_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9171667b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1.08368177e-08],\n",
       "         [3.60536501e-02],\n",
       "         [1.96102530e-08],\n",
       "         [1.09670356e-01],\n",
       "         [4.93544310e-01],\n",
       "         [4.31701750e-01],\n",
       "         [4.59600508e-01],\n",
       "         [1.60269260e-01],\n",
       "         [2.10509062e-01],\n",
       "         [1.77047513e-02],\n",
       "         [1.29703119e-01]],\n",
       "\n",
       "        [[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]]],\n",
       "\n",
       "\n",
       "       [[[1.06149072e-08],\n",
       "         [3.04260999e-02],\n",
       "         [1.64421117e-08],\n",
       "         [9.89375487e-02],\n",
       "         [5.27482629e-01],\n",
       "         [4.76772249e-01],\n",
       "         [3.92093986e-01],\n",
       "         [1.31084532e-01],\n",
       "         [2.00299725e-01],\n",
       "         [1.82819255e-02],\n",
       "         [9.46676582e-02]],\n",
       "\n",
       "        [[9.10970233e-09],\n",
       "         [3.89005616e-02],\n",
       "         [1.76130861e-08],\n",
       "         [1.06044851e-01],\n",
       "         [5.01062572e-01],\n",
       "         [4.71772611e-01],\n",
       "         [4.53948647e-01],\n",
       "         [1.52724519e-01],\n",
       "         [2.21467718e-01],\n",
       "         [1.95014086e-02],\n",
       "         [1.11105040e-01]],\n",
       "\n",
       "        [[5.81380233e-09],\n",
       "         [2.60500330e-02],\n",
       "         [1.66847958e-08],\n",
       "         [9.89684388e-02],\n",
       "         [4.99864906e-01],\n",
       "         [4.50949341e-01],\n",
       "         [3.82516414e-01],\n",
       "         [1.29036337e-01],\n",
       "         [1.83768094e-01],\n",
       "         [1.25944829e-02],\n",
       "         [9.68663320e-02]],\n",
       "\n",
       "        [[7.24652205e-09],\n",
       "         [2.89812144e-02],\n",
       "         [1.72170704e-08],\n",
       "         [1.02669150e-01],\n",
       "         [5.35318553e-01],\n",
       "         [4.29487944e-01],\n",
       "         [4.12994266e-01],\n",
       "         [1.39576867e-01],\n",
       "         [1.91963658e-01],\n",
       "         [1.30563863e-02],\n",
       "         [1.04084052e-01]],\n",
       "\n",
       "        [[6.06154105e-09],\n",
       "         [4.28037606e-02],\n",
       "         [2.31000232e-08],\n",
       "         [1.32117063e-01],\n",
       "         [5.78713536e-01],\n",
       "         [4.60009694e-01],\n",
       "         [5.32073081e-01],\n",
       "         [1.96870178e-01],\n",
       "         [2.43010357e-01],\n",
       "         [1.87698063e-02],\n",
       "         [1.75482631e-01]],\n",
       "\n",
       "        [[6.05376371e-09],\n",
       "         [3.68960202e-02],\n",
       "         [2.13197904e-08],\n",
       "         [1.28368676e-01],\n",
       "         [6.27172947e-01],\n",
       "         [4.36633140e-01],\n",
       "         [5.26264966e-01],\n",
       "         [1.94025993e-01],\n",
       "         [2.43150860e-01],\n",
       "         [1.70038119e-02],\n",
       "         [1.67414486e-01]],\n",
       "\n",
       "        [[1.08545919e-08],\n",
       "         [4.05925028e-02],\n",
       "         [1.91197547e-08],\n",
       "         [1.09047189e-01],\n",
       "         [5.68330526e-01],\n",
       "         [3.95149767e-01],\n",
       "         [4.69213665e-01],\n",
       "         [1.62352383e-01],\n",
       "         [2.05591723e-01],\n",
       "         [1.58412419e-02],\n",
       "         [1.19397290e-01]],\n",
       "\n",
       "        [[1.06866844e-08],\n",
       "         [1.01436771e-01],\n",
       "         [1.97316883e-08],\n",
       "         [1.14744321e-01],\n",
       "         [4.81375158e-01],\n",
       "         [3.61676991e-01],\n",
       "         [4.93533880e-01],\n",
       "         [1.76247209e-01],\n",
       "         [2.22966328e-01],\n",
       "         [2.71484163e-02],\n",
       "         [1.40494898e-01]],\n",
       "\n",
       "        [[1.21875967e-08],\n",
       "         [4.45951000e-02],\n",
       "         [1.97481569e-08],\n",
       "         [1.14193007e-01],\n",
       "         [5.72030365e-01],\n",
       "         [4.00004745e-01],\n",
       "         [4.60895538e-01],\n",
       "         [1.64718658e-01],\n",
       "         [2.12756991e-01],\n",
       "         [1.88877694e-02],\n",
       "         [1.31219164e-01]],\n",
       "\n",
       "        [[9.16101239e-09],\n",
       "         [3.37851271e-02],\n",
       "         [1.79160082e-08],\n",
       "         [1.05543904e-01],\n",
       "         [5.76015115e-01],\n",
       "         [4.45875734e-01],\n",
       "         [4.27184075e-01],\n",
       "         [1.45816624e-01],\n",
       "         [1.92431986e-01],\n",
       "         [1.44237261e-02],\n",
       "         [1.13276407e-01]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2949d425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19243199, 0.1055439 , 0.03378513, 0.11327641, 0.01442373],\n",
       "       [0.17313728, 0.10107498, 0.02521631, 0.10141373, 0.01119418]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf7bb251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5504, 1368)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a83357",
   "metadata": {},
   "source": [
    "### Define the input dimensions for the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1486173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dims(X_train, exogenous_data_train):\n",
    "    if args.model_name == \"mlp\":\n",
    "        input_dim = X_train.shape[1] * X_train.shape[2]\n",
    "    else:\n",
    "        input_dim = X_train.shape[2]\n",
    "    \n",
    "    if exogenous_data_train is not None:\n",
    "        if len(exogenous_data_train) == 1:\n",
    "            cid = next(iter(exogenous_data_train.keys()))\n",
    "            exogenous_dim = exogenous_data_train[cid].shape[1]\n",
    "        else:\n",
    "            exogenous_dim = exogenous_data_train[\"all\"].shape[1]\n",
    "    else:\n",
    "        exogenous_dim = 0\n",
    "    \n",
    "    return input_dim, exogenous_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6783ff",
   "metadata": {},
   "source": [
    "### Initialize the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str,\n",
    "              input_dim: int,\n",
    "              out_dim: int,\n",
    "              lags: int = 10,\n",
    "              exogenous_dim: int = 0,\n",
    "              seed=0):\n",
    "    if model == \"mlp\":\n",
    "        model = MLP(input_dim=input_dim, layer_units=[256, 128, 64], num_outputs=out_dim)\n",
    "    elif model == \"rnn\":\n",
    "        model = RNN(input_dim=input_dim, rnn_hidden_size=128, num_rnn_layers=1, rnn_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"lstm\":\n",
    "        model = LSTM(input_dim=input_dim, lstm_hidden_size=128, num_lstm_layers=1, lstm_dropout=0.0,\n",
    "                     layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"gru\":\n",
    "        model = GRU(input_dim=input_dim, gru_hidden_size=128, num_gru_layers=1, gru_dropout=0.0,\n",
    "                    layer_units=[128], num_outputs=out_dim, matrix_rep=True, exogenous_dim=exogenous_dim)\n",
    "    elif model == \"cnn\":\n",
    "        model = CNN(num_features=input_dim, lags=lags, exogenous_dim=exogenous_dim, out_dim=out_dim)\n",
    "    elif model == \"da_encoder_decoder\":\n",
    "        model = DualAttentionAutoEncoder(input_dim=input_dim, architecture=\"lstm\", matrix_rep=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Specified model is not implemented. Plese define your own model or choose one from ['mlp', 'rnn', 'lstm', 'gru', 'cnn', 'da_encoder_decoder']\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44ed1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "args.model_name = \"lstm\"\n",
    "\n",
    "input_dim, exogenous_dim = get_input_dims(X_train, exogenous_data_train)\n",
    "\n",
    "print(input_dim, exogenous_dim)\n",
    "\n",
    "model = get_model(model=args.model_name,\n",
    "                  input_dim=input_dim,\n",
    "                  out_dim=y_train.shape[1],\n",
    "                  lags=args.num_lags,\n",
    "                  exogenous_dim=exogenous_dim,\n",
    "                  seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f285c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(11, 128, batch_first=True)\n",
       "  (MLP_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2473d72",
   "metadata": {},
   "source": [
    "### The fit function used to train the model specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b11fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, X_train, y_train, X_val, y_val, \n",
    "        exogenous_data_train=None, exogenous_data_val=None, \n",
    "        idxs=[8, 3, 1, 10, 9], # the indices of our targets in X\n",
    "        log_per=1):\n",
    "    \n",
    "    # get exogenous data (if any)\n",
    "    if exogenous_data_train is not None and len(exogenous_data_train) > 1:\n",
    "        exogenous_data_train = exogenous_data_train[\"all\"]\n",
    "        exogenous_data_val = exogenous_data_val[\"all\"]\n",
    "    elif exogenous_data_train is not None and len(exogenous_data_train) == 1:\n",
    "        cid = next(iter(exogenous_data_train.keys()))\n",
    "        exogenous_data_train = exogenous_data_train[cid]\n",
    "        exogenous_data_val = exogenous_data_val[cid]\n",
    "    else:\n",
    "        exogenous_data_train = None\n",
    "        exogenous_data_val = None\n",
    "    num_features = len(X_train[0][0])\n",
    "    \n",
    "    # to torch loader\n",
    "    train_loader = to_torch_dataset(X_train, y_train,\n",
    "                                    num_lags=args.num_lags,\n",
    "                                    num_features=num_features,\n",
    "                                    exogenous_data=exogenous_data_train,\n",
    "                                    indices=idxs,\n",
    "                                    batch_size=args.batch_size, \n",
    "                                    shuffle=False)\n",
    "    val_loader = to_torch_dataset(X_val, y_val, \n",
    "                                  num_lags=args.num_lags,\n",
    "                                  num_features=num_features,\n",
    "                                  exogenous_data=exogenous_data_val,\n",
    "                                  indices=idxs,\n",
    "                                  batch_size=args.batch_size,\n",
    "                                  shuffle=False)\n",
    "    \n",
    "    # train the model\n",
    "    model = train(model, \n",
    "                  train_loader, val_loader,\n",
    "                  epochs=args.epochs,\n",
    "                  optimizer=args.optimizer, lr=args.lr,\n",
    "                  criterion=args.criterion,\n",
    "                  early_stopping=args.early_stopping,\n",
    "                  patience=args.patience,\n",
    "                  plot_history=args.plot_history, \n",
    "                  device=device, log_per=log_per)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "681cc512",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 16:30:45,941 | train_utils.py:97 | Epoch 1 [Train]: loss 0.009817008240429987, mse: 0.006684846244752407, rmse: 0.08176090902596672, mae 0.050114553421735764, r2: 0.301586244673529, nrmse: 1.6128628334541237\n",
      "INFO logger 2022-10-24 16:30:45,944 | train_utils.py:99 | Epoch 1 [Test]: loss 6.256589961472398e-05, mse: 0.007847568020224571, rmse: 0.0885865002143361, mae 0.05763064697384834, r2: -203.55665968299832, nrmse: 11.073628867583354\n",
      "INFO logger 2022-10-24 16:30:45,944 | helpers.py:148 | Validation loss decreased (inf --> 0.000063). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:46,461 | train_utils.py:97 | Epoch 2 [Train]: loss 0.005097735968288468, mse: 0.0034416825510561466, rmse: 0.058665855069675296, mae 0.03914261981844902, r2: 0.47421794643750276, nrmse: 1.560140873398426\n",
      "INFO logger 2022-10-24 16:30:46,462 | train_utils.py:99 | Epoch 2 [Test]: loss 3.155781455169165e-05, mse: 0.0039516836404800415, rmse: 0.06286241834737223, mae 0.044917769730091095, r2: -493.236927830232, nrmse: 10.969226500272672\n",
      "INFO logger 2022-10-24 16:30:46,462 | helpers.py:148 | Validation loss decreased (0.000063 --> 0.000032). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:46,933 | train_utils.py:97 | Epoch 3 [Train]: loss 0.002882774695239658, mse: 0.0024190195836126804, rmse: 0.04918352959693601, mae 0.03209643065929413, r2: 0.6228880381554801, nrmse: 1.2593015558573148\n",
      "INFO logger 2022-10-24 16:30:46,935 | train_utils.py:99 | Epoch 3 [Test]: loss 1.901966058077258e-05, mse: 0.0023706769570708275, rmse: 0.04868959803767975, mae 0.03474591672420502, r2: -260.81076397244544, nrmse: 8.268016579939792\n",
      "INFO logger 2022-10-24 16:30:46,936 | helpers.py:148 | Validation loss decreased (0.000032 --> 0.000019). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:47,509 | train_utils.py:97 | Epoch 4 [Train]: loss 0.002493984206054403, mse: 0.002123612677678466, rmse: 0.04608267220635611, mae 0.028472760692238808, r2: 0.6664072736256612, nrmse: 1.1941819656910042\n",
      "INFO logger 2022-10-24 16:30:47,509 | train_utils.py:99 | Epoch 4 [Test]: loss 1.666556934183409e-05, mse: 0.00208742031827569, rmse: 0.04568829519992719, mae 0.03143126145005226, r2: -165.56370212331802, nrmse: 6.598781138265796\n",
      "INFO logger 2022-10-24 16:30:47,509 | helpers.py:148 | Validation loss decreased (0.000019 --> 0.000017). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:47,978 | train_utils.py:97 | Epoch 5 [Train]: loss 0.0022134198827484927, mse: 0.002011147327721119, rmse: 0.04484581728234105, mae 0.026701927185058594, r2: 0.6933988593146196, nrmse: 1.1240115640308126\n",
      "INFO logger 2022-10-24 16:30:47,979 | train_utils.py:99 | Epoch 5 [Test]: loss 1.5305370849431957e-05, mse: 0.0019210338359698653, rmse: 0.04382959999783098, mae 0.029217753559350967, r2: -95.74541261455154, nrmse: 5.323812762092391\n",
      "INFO logger 2022-10-24 16:30:47,979 | helpers.py:148 | Validation loss decreased (0.000017 --> 0.000015). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:48,446 | train_utils.py:97 | Epoch 6 [Train]: loss 0.002086102997012393, mse: 0.0019475903827697039, rmse: 0.044131512355342004, mae 0.02579302154481411, r2: 0.7047533916879749, nrmse: 1.0919205555415705\n",
      "INFO logger 2022-10-24 16:30:48,446 | train_utils.py:99 | Epoch 6 [Test]: loss 1.4485727331181715e-05, mse: 0.001821695826947689, rmse: 0.0426813287861061, mae 0.027945678681135178, r2: -70.61422381406646, nrmse: 4.532984516928823\n",
      "INFO logger 2022-10-24 16:30:48,448 | helpers.py:148 | Validation loss decreased (0.000015 --> 0.000014). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:48,952 | train_utils.py:97 | Epoch 7 [Train]: loss 0.002010044743656181, mse: 0.001904335105791688, rmse: 0.04363868817679661, mae 0.025386527180671692, r2: 0.7131673983675622, nrmse: 1.067163052395978\n",
      "INFO logger 2022-10-24 16:30:48,952 | train_utils.py:99 | Epoch 7 [Test]: loss 1.3906623014556625e-05, mse: 0.0017526295268908143, rmse: 0.04186441838710785, mae 0.026991605758666992, r2: -55.30570041747499, nrmse: 3.8863937663101527\n",
      "INFO logger 2022-10-24 16:30:48,953 | helpers.py:148 | Validation loss decreased (0.000014 --> 0.000014). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:49,512 | train_utils.py:97 | Epoch 8 [Train]: loss 0.001959316431122377, mse: 0.0018611196428537369, rmse: 0.04314069590136136, mae 0.024996168911457062, r2: 0.7199796387607501, nrmse: 1.0550304156425319\n",
      "INFO logger 2022-10-24 16:30:49,513 | train_utils.py:99 | Epoch 8 [Test]: loss 1.34681946579612e-05, mse: 0.0017003401881083846, rmse: 0.04123518143658864, mae 0.026125499978661537, r2: -40.83597887373794, nrmse: 3.2917890743922706\n",
      "INFO logger 2022-10-24 16:30:49,514 | helpers.py:148 | Validation loss decreased (0.000014 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:49,997 | train_utils.py:97 | Epoch 9 [Train]: loss 0.0019211234242437396, mse: 0.0018236491596326232, rmse: 0.04270420540921729, mae 0.024619851261377335, r2: 0.7246989202589899, nrmse: 1.052194173726934\n",
      "INFO logger 2022-10-24 16:30:49,998 | train_utils.py:99 | Epoch 9 [Test]: loss 1.3161462921259805e-05, mse: 0.0016633786726742983, rmse: 0.04078453962807841, mae 0.025405243039131165, r2: -29.40708745135189, nrmse: 2.927648347162758\n",
      "INFO logger 2022-10-24 16:30:49,999 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:50,427 | train_utils.py:97 | Epoch 10 [Train]: loss 0.0018926308304037585, mse: 0.0017932361224666238, rmse: 0.0423466187843448, mae 0.024304036051034927, r2: 0.7281051253931187, nrmse: 1.0524949564393833\n",
      "INFO logger 2022-10-24 16:30:50,428 | train_utils.py:99 | Epoch 10 [Test]: loss 1.2968177298711591e-05, mse: 0.0016396439168602228, rmse: 0.04049251680076484, mae 0.024919116869568825, r2: -22.080447712767626, nrmse: 2.8203674948509305\n",
      "INFO logger 2022-10-24 16:30:50,428 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:50,896 | train_utils.py:97 | Epoch 11 [Train]: loss 0.0018690586605158007, mse: 0.0017685526981949806, rmse: 0.04205416386274944, mae 0.02404385432600975, r2: 0.7308757219198275, nrmse: 1.0523594395072946\n",
      "INFO logger 2022-10-24 16:30:50,897 | train_utils.py:99 | Epoch 11 [Test]: loss 1.2869070075916627e-05, mse: 0.0016270078485831618, rmse: 0.040336185349920756, mae 0.02466597780585289, r2: -18.02202320641705, nrmse: 2.8334821255008857\n",
      "INFO logger 2022-10-24 16:30:50,898 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:51,328 | train_utils.py:97 | Epoch 12 [Train]: loss 0.0018439902880853095, mse: 0.0017486162250861526, rmse: 0.041816458782232536, mae 0.023831959813833237, r2: 0.7333820756553402, nrmse: 1.0506268104640837\n",
      "INFO logger 2022-10-24 16:30:51,329 | train_utils.py:99 | Epoch 12 [Test]: loss 1.282379574878332e-05, mse: 0.001620815135538578, rmse: 0.04025934842416824, mae 0.024556364864110947, r2: -15.910283588485289, nrmse: 2.854575171885754\n",
      "INFO logger 2022-10-24 16:30:51,329 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:51,870 | train_utils.py:97 | Epoch 13 [Train]: loss 0.0018183394785473653, mse: 0.0017314900178462267, rmse: 0.04161117659771503, mae 0.023656001314520836, r2: 0.7356567409504111, nrmse: 1.0479669476187523\n",
      "INFO logger 2022-10-24 16:30:51,871 | train_utils.py:99 | Epoch 13 [Test]: loss 1.2778150603108445e-05, mse: 0.0016146970447152853, rmse: 0.04018329310441449, mae 0.02448296919465065, r2: -14.84994762285117, nrmse: 2.8562282440660773\n",
      "INFO logger 2022-10-24 16:30:51,871 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:52,298 | train_utils.py:97 | Epoch 14 [Train]: loss 0.0017957671492229903, mse: 0.0017158115515485406, rmse: 0.04142235569772126, mae 0.023500630632042885, r2: 0.737667892841527, nrmse: 1.0452064577679552\n",
      "INFO logger 2022-10-24 16:30:52,299 | train_utils.py:99 | Epoch 14 [Test]: loss 1.2712825226021538e-05, mse: 0.0016063643852248788, rmse: 0.04007947586015664, mae 0.02440469339489937, r2: -14.439824893860305, nrmse: 2.859996028261534\n",
      "INFO logger 2022-10-24 16:30:52,299 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:52,764 | train_utils.py:97 | Epoch 15 [Train]: loss 0.0017770896008800644, mse: 0.0017015396151691675, rmse: 0.041249722607178435, mae 0.023363064974546432, r2: 0.739401743970814, nrmse: 1.042776199951948\n",
      "INFO logger 2022-10-24 16:30:52,765 | train_utils.py:99 | Epoch 15 [Test]: loss 1.2638444494189182e-05, mse: 0.0015970472013577819, rmse: 0.039963072971904726, mae 0.024325815960764885, r2: -14.40372632354484, nrmse: 2.876528880883391\n",
      "INFO logger 2022-10-24 16:30:52,765 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:53,232 | train_utils.py:97 | Epoch 16 [Train]: loss 0.0017615010006117044, mse: 0.0016887752572074533, rmse: 0.041094710817907616, mae 0.02324194647371769, r2: 0.740887289750013, nrmse: 1.0407473664812656\n",
      "INFO logger 2022-10-24 16:30:53,232 | train_utils.py:99 | Epoch 16 [Test]: loss 1.2566205957456498e-05, mse: 0.0015880452701821923, rmse: 0.0398502856976232, mae 0.02425360307097435, r2: -14.519125699128546, nrmse: 2.8996380468866176\n",
      "INFO logger 2022-10-24 16:30:53,233 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000013). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:53,777 | train_utils.py:97 | Epoch 17 [Train]: loss 0.001747980407979116, mse: 0.0016773430397734046, rmse: 0.04095537864277908, mae 0.02313517965376377, r2: 0.7421773073481341, nrmse: 1.039052112611569\n",
      "INFO logger 2022-10-24 16:30:53,777 | train_utils.py:99 | Epoch 17 [Test]: loss 1.2499134733487323e-05, mse: 0.0015796968946233392, rmse: 0.0397454009241741, mae 0.024185840040445328, r2: -14.649583662815658, nrmse: 2.9205714890850576\n",
      "INFO logger 2022-10-24 16:30:53,778 | helpers.py:148 | Validation loss decreased (0.000013 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:54,279 | train_utils.py:97 | Epoch 18 [Train]: loss 0.0017358028147967605, mse: 0.001666924566961825, rmse: 0.04082798754484263, mae 0.02303887903690338, r2: 0.7433210165276838, nrmse: 1.0376143059171927\n",
      "INFO logger 2022-10-24 16:30:54,279 | train_utils.py:99 | Epoch 18 [Test]: loss 1.2436177638054373e-05, mse: 0.0015718713402748108, rmse: 0.039646832663843536, mae 0.024120766669511795, r2: -14.745027748499032, nrmse: 2.9360405546819126\n",
      "INFO logger 2022-10-24 16:30:54,280 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:54,762 | train_utils.py:97 | Epoch 19 [Train]: loss 0.0017246299142053155, mse: 0.0016573028406128287, rmse: 0.04070998453221063, mae 0.022950058802962303, r2: 0.7443512580149111, nrmse: 1.0363868057054444\n",
      "INFO logger 2022-10-24 16:30:54,763 | train_utils.py:99 | Epoch 19 [Test]: loss 1.2376281678402928e-05, mse: 0.0015644386876374483, rmse: 0.03955298582455499, mae 0.024054739624261856, r2: -14.80674192968267, nrmse: 2.947318347061988\n",
      "INFO logger 2022-10-24 16:30:54,763 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:55,237 | train_utils.py:97 | Epoch 20 [Train]: loss 0.0017143526825156039, mse: 0.0016483841463923454, rmse: 0.04060029736827485, mae 0.022867755964398384, r2: 0.7452889670785238, nrmse: 1.0353362080632982\n",
      "INFO logger 2022-10-24 16:30:55,238 | train_utils.py:99 | Epoch 20 [Test]: loss 1.231936009618459e-05, mse: 0.0015573855489492416, rmse: 0.03946372446879845, mae 0.023991582915186882, r2: -14.850332576839318, nrmse: 2.9564897863734023\n",
      "INFO logger 2022-10-24 16:30:55,239 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:55,877 | train_utils.py:97 | Epoch 21 [Train]: loss 0.0017049153555828609, mse: 0.0016400909516960382, rmse: 0.04049803639308995, mae 0.022792622447013855, r2: 0.7461504731341287, nrmse: 1.0344269003357494\n",
      "INFO logger 2022-10-24 16:30:55,877 | train_utils.py:99 | Epoch 21 [Test]: loss 1.226570420627542e-05, mse: 0.0015507451025769114, rmse: 0.03937950104530162, mae 0.023932039737701416, r2: -14.884554364707904, nrmse: 2.963979466174132\n",
      "INFO logger 2022-10-24 16:30:55,878 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:56,369 | train_utils.py:97 | Epoch 22 [Train]: loss 0.0016962397345238525, mse: 0.001632362836971879, rmse: 0.04040251028057389, mae 0.02273237332701683, r2: 0.7469502565524674, nrmse: 1.0336186628346462\n",
      "INFO logger 2022-10-24 16:30:56,370 | train_utils.py:99 | Epoch 22 [Test]: loss 1.2215488482735048e-05, mse: 0.0015445363242179155, rmse: 0.039300589362220965, mae 0.023875683546066284, r2: -14.906637900743302, nrmse: 2.96820664109935\n",
      "INFO logger 2022-10-24 16:30:56,370 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:56,852 | train_utils.py:97 | Epoch 23 [Train]: loss 0.001688234890109946, mse: 0.001625165925361216, rmse: 0.04031334673977362, mae 0.022679848596453667, r2: 0.7476990895903259, nrmse: 1.03287435520804\n",
      "INFO logger 2022-10-24 16:30:56,853 | train_utils.py:99 | Epoch 23 [Test]: loss 1.2168807470462999e-05, mse: 0.0015387691091746092, rmse: 0.039227147604364625, mae 0.023822633549571037, r2: -14.907419989752142, nrmse: 2.9666000401596726\n",
      "INFO logger 2022-10-24 16:30:56,854 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:57,362 | train_utils.py:97 | Epoch 24 [Train]: loss 0.0016808258463264115, mse: 0.0016184588894248009, rmse: 0.040230074439712396, mae 0.02263471856713295, r2: 0.7484018977414186, nrmse: 1.0321661940158129\n",
      "INFO logger 2022-10-24 16:30:57,362 | train_utils.py:99 | Epoch 24 [Test]: loss 1.2125883766957858e-05, mse: 0.001533468603156507, rmse: 0.039159527616615945, mae 0.023775016888976097, r2: -14.878269038136963, nrmse: 2.956864513219625\n",
      "INFO logger 2022-10-24 16:30:57,363 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:57,897 | train_utils.py:97 | Epoch 25 [Train]: loss 0.0016739737175580664, mse: 0.0016122523229569197, rmse: 0.040152861952255904, mae 0.022597091272473335, r2: 0.7490574726275477, nrmse: 1.031479471598499\n",
      "INFO logger 2022-10-24 16:30:57,898 | train_utils.py:99 | Epoch 25 [Test]: loss 1.208717920405556e-05, mse: 0.001528688706457615, rmse: 0.03909844890091696, mae 0.023734387010335922, r2: -14.816715132815906, nrmse: 2.93805764550666\n",
      "INFO logger 2022-10-24 16:30:57,898 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:58,408 | train_utils.py:97 | Epoch 26 [Train]: loss 0.0016676724479555898, mse: 0.0016065612435340881, rmse: 0.04008193163426743, mae 0.02256837859749794, r2: 0.7496605754663113, nrmse: 1.0308128659289986\n",
      "INFO logger 2022-10-24 16:30:58,409 | train_utils.py:99 | Epoch 26 [Test]: loss 1.2053367169505825e-05, mse: 0.0015245064860209823, rmse: 0.039044929069227186, mae 0.02370131015777588, r2: -14.727044965513443, nrmse: 2.9110113127114277\n",
      "INFO logger 2022-10-24 16:30:58,409 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:58,864 | train_utils.py:97 | Epoch 27 [Train]: loss 0.0016619334841443349, mse: 0.0016014078864827752, rmse: 0.04001759471136134, mae 0.022549044340848923, r2: 0.7502050447137336, nrmse: 1.0301739657741675\n",
      "INFO logger 2022-10-24 16:30:58,865 | train_utils.py:99 | Epoch 27 [Test]: loss 1.2025262102517381e-05, mse: 0.0015210199635475874, rmse: 0.03900025594207796, mae 0.023676862940192223, r2: -14.615332679169768, nrmse: 2.877906253001302\n",
      "INFO logger 2022-10-24 16:30:58,865 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:30:59,401 | train_utils.py:97 | Epoch 28 [Train]: loss 0.0016567642939112693, mse: 0.0015968646621331573, rmse: 0.03996078905793975, mae 0.022539319470524788, r2: 0.7506863555412286, nrmse: 1.0295739794940297\n",
      "INFO logger 2022-10-24 16:30:59,402 | train_utils.py:99 | Epoch 28 [Test]: loss 1.2003726097693862e-05, mse: 0.0015183283248916268, rmse: 0.038965732700561743, mae 0.02366466633975506, r2: -14.48232816668787, nrmse: 2.841329957862688\n",
      "INFO logger 2022-10-24 16:30:59,402 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:00,055 | train_utils.py:97 | Epoch 29 [Train]: loss 0.0016521477523912286, mse: 0.001592953340150416, rmse: 0.03991181955449308, mae 0.022538183256983757, r2: 0.7511034681526365, nrmse: 1.029022319636665\n",
      "INFO logger 2022-10-24 16:31:00,056 | train_utils.py:99 | Epoch 29 [Test]: loss 1.198952619414586e-05, mse: 0.001516525400802493, rmse: 0.03894259108999417, mae 0.02366577461361885, r2: -14.317609581251663, nrmse: 2.803175212814123\n",
      "INFO logger 2022-10-24 16:31:00,056 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:00,567 | train_utils.py:97 | Epoch 30 [Train]: loss 0.0016480201487823695, mse: 0.0015896735712885857, rmse: 0.039870710694551025, mae 0.02254452556371689, r2: 0.7514601441668968, nrmse: 1.028521712778038\n",
      "INFO logger 2022-10-24 16:31:00,568 | train_utils.py:99 | Epoch 30 [Test]: loss 1.1983086822734198e-05, mse: 0.001515660434961319, rmse: 0.03893148385254946, mae 0.023676637560129166, r2: -14.096902076394548, nrmse: 2.763670712309825\n",
      "INFO logger 2022-10-24 16:31:00,571 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:01,029 | train_utils.py:97 | Epoch 31 [Train]: loss 0.001644262569539552, mse: 0.0015869894996285439, rmse: 0.039837036782729505, mae 0.022555235773324966, r2: 0.7517663994695004, nrmse: 1.028066177248499\n",
      "INFO logger 2022-10-24 16:31:01,031 | train_utils.py:99 | Epoch 31 [Test]: loss 1.198413160344319e-05, mse: 0.0015157050220295787, rmse: 0.03893205648343764, mae 0.02369304560124874, r2: -13.784412197889168, nrmse: 2.720792326132197\n",
      "INFO logger 2022-10-24 16:31:01,031 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:01,531 | train_utils.py:97 | Epoch 32 [Train]: loss 0.001640711215862021, mse: 0.0015847856411710382, rmse: 0.03980936624930166, mae 0.022566093131899834, r2: 0.752038702917058, nrmse: 1.027643296958345\n",
      "INFO logger 2022-10-24 16:31:01,532 | train_utils.py:99 | Epoch 32 [Test]: loss 1.1991303953122924e-05, mse: 0.0015165016520768404, rmse: 0.0389422861691098, mae 0.023710347712039948, r2: -13.341814249849298, nrmse: 2.6704279836866176\n",
      "INFO logger 2022-10-24 16:31:01,533 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:02,108 | train_utils.py:97 | Epoch 33 [Train]: loss 0.0016371978405575562, mse: 0.0015828640898689628, rmse: 0.03978522451701087, mae 0.022573664784431458, r2: 0.7522976248853307, nrmse: 1.0272434913661783\n",
      "INFO logger 2022-10-24 16:31:02,109 | train_utils.py:99 | Epoch 33 [Test]: loss 1.200205115948624e-05, mse: 0.0015177520690485835, rmse: 0.038958337606327394, mae 0.023720677942037582, r2: -12.744120523143074, nrmse: 2.6077752707133945\n",
      "INFO logger 2022-10-24 16:31:02,109 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 16:31:02,624 | train_utils.py:97 | Epoch 34 [Train]: loss 0.001633609391425379, mse: 0.0015810137847438455, rmse: 0.039761964045351754, mae 0.022574532777071, r2: 0.7525616935205248, nrmse: 1.0268714236217211\n",
      "INFO logger 2022-10-24 16:31:02,625 | train_utils.py:99 | Epoch 34 [Test]: loss 1.2013108147191135e-05, mse: 0.001519066747277975, rmse: 0.038975206827905035, mae 0.023715872317552567, r2: -11.997379095670231, nrmse: 2.5296084681868978\n",
      "INFO logger 2022-10-24 16:31:02,625 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2022-10-24 16:31:03,094 | train_utils.py:97 | Epoch 35 [Train]: loss 0.0016299403740282117, mse: 0.0015790362376719713, rmse: 0.03973708894310165, mae 0.02256912551820278, r2: 0.7528406356665425, nrmse: 1.026548267468034\n",
      "INFO logger 2022-10-24 16:31:03,095 | train_utils.py:99 | Epoch 35 [Test]: loss 1.2021590105417234e-05, mse: 0.0015201009809970856, rmse: 0.03898847241168966, mae 0.02369351126253605, r2: -11.146935557382667, nrmse: 2.4363307347336933\n",
      "INFO logger 2022-10-24 16:31:03,095 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2022-10-24 16:31:03,568 | train_utils.py:97 | Epoch 36 [Train]: loss 0.0016263062572000118, mse: 0.001576888607814908, rmse: 0.03971005675914992, mae 0.02255949378013611, r2: 0.753131936522298, nrmse: 1.0262998842204627\n",
      "INFO logger 2022-10-24 16:31:03,569 | train_utils.py:99 | Epoch 36 [Test]: loss 1.2026140812685037e-05, mse: 0.0015206888783723116, rmse: 0.03899601105718778, mae 0.02365647628903389, r2: -10.267222852375605, nrmse: 2.332261151453001\n",
      "INFO logger 2022-10-24 16:31:03,569 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2022-10-24 16:31:04,121 | train_utils.py:97 | Epoch 37 [Train]: loss 0.0016228987671194435, mse: 0.0015746355056762695, rmse: 0.03968167720341807, mae 0.022550104185938835, r2: 0.7534247841133948, nrmse: 1.0261367560835195\n",
      "INFO logger 2022-10-24 16:31:04,122 | train_utils.py:99 | Epoch 37 [Test]: loss 1.2027328538304e-05, mse: 0.0015208887634798884, rmse: 0.038998573864692646, mae 0.023612525314092636, r2: -9.437935986104756, nrmse: 2.2238801586259656\n",
      "INFO logger 2022-10-24 16:31:04,122 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2022-10-24 16:31:04,594 | train_utils.py:97 | Epoch 38 [Train]: loss 0.001619897495560889, mse: 0.0015724286204203963, rmse: 0.03965386009483057, mae 0.022546272724866867, r2: 0.7537081345358356, nrmse: 1.0260400320210348\n",
      "INFO logger 2022-10-24 16:31:04,595 | train_utils.py:99 | Epoch 38 [Test]: loss 1.2027024607513132e-05, mse: 0.001520913909189403, rmse: 0.038998896256040416, mae 0.02357080951333046, r2: -8.724550581129138, nrmse: 2.1176132092598348\n",
      "INFO logger 2022-10-24 16:31:04,595 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2022-10-24 16:31:05,039 | train_utils.py:97 | Epoch 39 [Train]: loss 0.0016173853598872461, mse: 0.001570422202348709, rmse: 0.03962855286720308, mae 0.022550810128450394, r2: 0.7539771673616092, nrmse: 1.0259660892434816\n",
      "INFO logger 2022-10-24 16:31:05,040 | train_utils.py:99 | Epoch 39 [Test]: loss 1.202732672995196e-05, mse: 0.0015210090205073357, rmse: 0.03900011564735848, mae 0.023540975525975227, r2: -8.172885032915032, nrmse: 2.0187136049523398\n",
      "INFO logger 2022-10-24 16:31:05,040 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2022-10-24 16:31:05,493 | train_utils.py:97 | Epoch 40 [Train]: loss 0.0016153083983078621, mse: 0.0015686771366745234, rmse: 0.03960652896524162, mae 0.022564660757780075, r2: 0.7542351457009333, nrmse: 1.0258614726291426\n",
      "INFO logger 2022-10-24 16:31:05,493 | train_utils.py:99 | Epoch 40 [Test]: loss 1.2029699968617976e-05, mse: 0.0015213403385132551, rmse: 0.039004363070216326, mae 0.023529337719082832, r2: -7.81221388377323, nrmse: 1.931393635233572\n",
      "INFO logger 2022-10-24 16:31:05,494 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2022-10-24 16:31:06,049 | train_utils.py:97 | Epoch 41 [Train]: loss 0.0016134910037138514, mse: 0.0015671920264139771, rmse: 0.0395877762246628, mae 0.022585716098546982, r2: 0.7544914178104939, nrmse: 1.0256802645210914\n",
      "INFO logger 2022-10-24 16:31:06,049 | train_utils.py:99 | Epoch 41 [Test]: loss 1.203457777485747e-05, mse: 0.0015219567576423287, rmse: 0.03901226419528004, mae 0.023535557091236115, r2: -7.653744173188175, nrmse: 1.8588067085767717\n",
      "INFO logger 2022-10-24 16:31:06,050 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2022-10-24 16:31:06,528 | train_utils.py:97 | Epoch 42 [Train]: loss 0.001611698055131815, mse: 0.0015658612828701735, rmse: 0.03957096514959135, mae 0.022610412910580635, r2: 0.7547588283222902, nrmse: 1.0253920138953914\n",
      "INFO logger 2022-10-24 16:31:06,529 | train_utils.py:99 | Epoch 42 [Test]: loss 1.2041310354596145e-05, mse: 0.0015227880794554949, rmse: 0.03902291736217956, mae 0.02355559542775154, r2: -7.681681142071525, nrmse: 1.8022508457896556\n",
      "INFO logger 2022-10-24 16:31:06,529 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2022-10-24 16:31:06,996 | train_utils.py:97 | Epoch 43 [Train]: loss 0.001609719812233041, mse: 0.0015645645326003432, rmse: 0.039554576632803734, mae 0.022633295506238937, r2: 0.7550504466610289, nrmse: 1.0249834221462746\n",
      "INFO logger 2022-10-24 16:31:06,997 | train_utils.py:99 | Epoch 43 [Test]: loss 1.2048429560000675e-05, mse: 0.001523659098893404, rmse: 0.03903407612450183, mae 0.02358357422053814, r2: -7.846707357416756, nrmse: 1.76031354425902\n",
      "INFO logger 2022-10-24 16:31:06,997 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2022-10-24 16:31:07,396 | train_utils.py:97 | Epoch 44 [Train]: loss 0.0016074493074314845, mse: 0.0015631724381819367, rmse: 0.03953697558212991, mae 0.022650063037872314, r2: 0.7553753690510963, nrmse: 1.0244583270483538\n",
      "INFO logger 2022-10-24 16:31:07,397 | train_utils.py:99 | Epoch 44 [Test]: loss 1.2054221158437377e-05, mse: 0.0015243702800944448, rmse: 0.03904318480982878, mae 0.023611413314938545, r2: -8.075987053697082, nrmse: 1.729503837958217\n",
      "INFO logger 2022-10-24 16:31:07,398 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2022-10-24 16:31:07,915 | train_utils.py:97 | Epoch 45 [Train]: loss 0.00160491322864356, mse: 0.0015616158489137888, rmse: 0.039517285444648, mae 0.022658100351691246, r2: 0.7557345197909389, nrmse: 1.0238379788439664\n",
      "INFO logger 2022-10-24 16:31:07,916 | train_utils.py:99 | Epoch 45 [Test]: loss 1.2057432770383223e-05, mse: 0.001524777733720839, rmse: 0.0390484024477422, mae 0.023633381351828575, r2: -8.298113530562796, nrmse: 1.7061610266529246\n",
      "INFO logger 2022-10-24 16:31:07,917 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2022-10-24 16:31:08,364 | train_utils.py:97 | Epoch 46 [Train]: loss 0.0016022370095396222, mse: 0.0015598908066749573, rmse: 0.03949545298733713, mae 0.022657252848148346, r2: 0.7561195192418335, nrmse: 1.0231586437227533\n",
      "INFO logger 2022-10-24 16:31:08,365 | train_utils.py:99 | Epoch 46 [Test]: loss 1.2057713915938402e-05, mse: 0.0015248371055349708, rmse: 0.039049162673929015, mae 0.02364555187523365, r2: -8.466043821709428, nrmse: 1.6880304556017107\n",
      "INFO logger 2022-10-24 16:31:08,365 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2022-10-24 16:31:08,778 | train_utils.py:97 | Epoch 47 [Train]: loss 0.0015995741830941602, mse: 0.0015580740291625261, rmse: 0.03947244645524934, mae 0.022649666294455528, r2: 0.7565158960852444, nrmse: 1.0224624722783764\n",
      "INFO logger 2022-10-24 16:31:08,778 | train_utils.py:99 | Epoch 47 [Test]: loss 1.20555371494027e-05, mse: 0.001524605555459857, rmse: 0.03904619770809774, mae 0.023648541420698166, r2: -8.562563165102139, nrmse: 1.6743167826811565\n",
      "INFO logger 2022-10-24 16:31:08,780 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2022-10-24 16:31:09,240 | train_utils.py:97 | Epoch 48 [Train]: loss 0.0015970447683325483, mse: 0.0015562109183520079, rmse: 0.03944883925227722, mae 0.022637849673628807, r2: 0.7569085358229879, nrmse: 1.021785742539645\n",
      "INFO logger 2022-10-24 16:31:09,242 | train_utils.py:99 | Epoch 48 [Test]: loss 1.2051794456375207e-05, mse: 0.0015241845976561308, rmse: 0.03904080682639808, mae 0.02364395186305046, r2: -8.590007462970807, nrmse: 1.6647770111482771\n",
      "INFO logger 2022-10-24 16:31:09,242 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2022-10-24 16:31:09,792 | train_utils.py:97 | Epoch 49 [Train]: loss 0.0015947147169035182, mse: 0.0015543918125331402, rmse: 0.03942577599151525, mae 0.022624216973781586, r2: 0.7572857697165387, nrmse: 1.0211516330255797\n",
      "INFO logger 2022-10-24 16:31:09,793 | train_utils.py:99 | Epoch 49 [Test]: loss 1.2047413734199033e-05, mse: 0.0015236864564940333, rmse: 0.039034426555209355, mae 0.023633942008018494, r2: -8.556286121198138, nrmse: 1.6588420189295572\n",
      "INFO logger 2022-10-24 16:31:09,793 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2022-10-24 16:31:10,280 | train_utils.py:97 | Epoch 50 [Train]: loss 0.0015926071780805338, mse: 0.001552668516524136, rmse: 0.03940391498980953, mae 0.02261003665626049, r2: 0.7576408893088323, nrmse: 1.0205686780007992\n",
      "INFO logger 2022-10-24 16:31:10,281 | train_utils.py:99 | Epoch 50 [Test]: loss 1.2043189019616966e-05, mse: 0.0015232049627229571, rmse: 0.03902825851511898, mae 0.023620475083589554, r2: -8.466944208527787, nrmse: 1.6553586172981871\n",
      "INFO logger 2022-10-24 16:31:10,281 | helpers.py:136 | EarlyStopping counter: 20 out of 50\n",
      "INFO logger 2022-10-24 16:31:10,704 | train_utils.py:97 | Epoch 51 [Train]: loss 0.0015907223183117018, mse: 0.001551095163449645, rmse: 0.03938394550384262, mae 0.022596154361963272, r2: 0.7579717284442344, nrmse: 1.0200333971837954\n",
      "INFO logger 2022-10-24 16:31:10,704 | train_utils.py:99 | Epoch 51 [Test]: loss 1.2039759831008779e-05, mse: 0.0015228126430884004, rmse: 0.03902323209433581, mae 0.02360517345368862, r2: -8.323005003595005, nrmse: 1.6528331040127378\n",
      "INFO logger 2022-10-24 16:31:10,704 | helpers.py:136 | EarlyStopping counter: 21 out of 50\n",
      "INFO logger 2022-10-24 16:31:11,162 | train_utils.py:97 | Epoch 52 [Train]: loss 0.0015890479597485039, mse: 0.0015496781561523676, rmse: 0.03936595173690543, mae 0.022582639008760452, r2: 0.7582799499949334, nrmse: 1.019535701162213\n",
      "INFO logger 2022-10-24 16:31:11,163 | train_utils.py:99 | Epoch 52 [Test]: loss 1.2037641718903317e-05, mse: 0.0015225736424326897, rmse: 0.03902016968738975, mae 0.023588474839925766, r2: -8.122618861797601, nrmse: 1.6498576581388482\n",
      "INFO logger 2022-10-24 16:31:11,164 | helpers.py:136 | EarlyStopping counter: 22 out of 50\n",
      "INFO logger 2022-10-24 16:31:11,703 | train_utils.py:97 | Epoch 53 [Train]: loss 0.0015875564133640966, mse: 0.001548428786918521, rmse: 0.039350079884525276, mae 0.02256922796368599, r2: 0.758570259426335, nrmse: 1.0190642973316806\n",
      "INFO logger 2022-10-24 16:31:11,703 | train_utils.py:99 | Epoch 53 [Test]: loss 1.203718726939865e-05, mse: 0.0015225278912112117, rmse: 0.039019583432056415, mae 0.023570673540234566, r2: -7.86340582870898, nrmse: 1.645533857309603\n",
      "INFO logger 2022-10-24 16:31:11,704 | helpers.py:136 | EarlyStopping counter: 23 out of 50\n",
      "INFO logger 2022-10-24 16:31:12,201 | train_utils.py:97 | Epoch 54 [Train]: loss 0.0015861900152907935, mse: 0.0015472990926355124, rmse: 0.03933572285640004, mae 0.02255462296307087, r2: 0.7588501157963762, nrmse: 1.0186139168693504\n",
      "INFO logger 2022-10-24 16:31:12,202 | train_utils.py:99 | Epoch 54 [Test]: loss 1.203847177376377e-05, mse: 0.0015226867981255054, rmse: 0.03902161962458126, mae 0.02355087362229824, r2: -7.545444361573523, nrmse: 1.639776114101774\n",
      "INFO logger 2022-10-24 16:31:12,202 | helpers.py:136 | EarlyStopping counter: 24 out of 50\n",
      "INFO logger 2022-10-24 16:31:12,662 | train_utils.py:97 | Epoch 55 [Train]: loss 0.0015848477093725727, mse: 0.0015461960574612021, rmse: 0.03932169957493193, mae 0.02253716252744198, r2: 0.7591286698750148, nrmse: 1.0181918517224184\n",
      "INFO logger 2022-10-24 16:31:12,663 | train_utils.py:99 | Epoch 55 [Test]: loss 1.2041101862243458e-05, mse: 0.0015230055432766676, rmse: 0.039025703623082414, mae 0.023527776822447777, r2: -7.1744823357230585, nrmse: 1.6334469623525787\n",
      "INFO logger 2022-10-24 16:31:12,663 | helpers.py:136 | EarlyStopping counter: 25 out of 50\n",
      "INFO logger 2022-10-24 16:31:13,110 | train_utils.py:97 | Epoch 56 [Train]: loss 0.0015833824567020793, mse: 0.0015449741622433066, rmse: 0.03930615934231309, mae 0.02251502312719822, r2: 0.7594145072832494, nrmse: 1.017824693130307\n",
      "INFO logger 2022-10-24 16:31:13,111 | train_utils.py:99 | Epoch 56 [Test]: loss 1.2044022776280416e-05, mse: 0.001523361774161458, rmse: 0.03903026741083717, mae 0.023498907685279846, r2: -6.7665519120237505, nrmse: 1.6282006156842328\n",
      "INFO logger 2022-10-24 16:31:13,111 | helpers.py:136 | EarlyStopping counter: 26 out of 50\n",
      "INFO logger 2022-10-24 16:31:13,651 | train_utils.py:97 | Epoch 57 [Train]: loss 0.0015816294145266977, mse: 0.001543485326692462, rmse: 0.03928721581752087, mae 0.022487619891762733, r2: 0.7597112847595462, nrmse: 1.0175616893639101\n",
      "INFO logger 2022-10-24 16:31:13,652 | train_utils.py:99 | Epoch 57 [Test]: loss 1.204550215717111e-05, mse: 0.0015235525788739324, rmse: 0.03903271165156134, mae 0.02346094883978367, r2: -6.351824528791798, nrmse: 1.6261502674302954\n",
      "INFO logger 2022-10-24 16:31:13,652 | helpers.py:136 | EarlyStopping counter: 27 out of 50\n",
      "INFO logger 2022-10-24 16:31:14,129 | train_utils.py:97 | Epoch 58 [Train]: loss 0.0015794751487953553, mse: 0.0015416352543979883, rmse: 0.03926366328296416, mae 0.022457730025053024, r2: 0.7600115840299999, nrmse: 1.017471883855707\n",
      "INFO logger 2022-10-24 16:31:14,130 | train_utils.py:99 | Epoch 58 [Test]: loss 1.2043516267511764e-05, mse: 0.0015233419835567474, rmse: 0.03903001388107295, mae 0.023411370813846588, r2: -5.974536271008941, nrmse: 1.6291734017732102\n",
      "INFO logger 2022-10-24 16:31:14,131 | helpers.py:136 | EarlyStopping counter: 28 out of 50\n",
      "INFO logger 2022-10-24 16:31:14,643 | train_utils.py:97 | Epoch 59 [Train]: loss 0.0015769594265610829, mse: 0.0015395085792988539, rmse: 0.03923657196161324, mae 0.02243274264037609, r2: 0.7602913229910218, nrmse: 1.01763310347088\n",
      "INFO logger 2022-10-24 16:31:14,644 | train_utils.py:99 | Epoch 59 [Test]: loss 1.203665023139264e-05, mse: 0.0015225594397634268, rmse: 0.039019987695582715, mae 0.023353323340415955, r2: -5.68310456554813, nrmse: 1.6379270688652028\n",
      "INFO logger 2022-10-24 16:31:14,644 | helpers.py:136 | EarlyStopping counter: 29 out of 50\n",
      "INFO logger 2022-10-24 16:31:15,115 | train_utils.py:97 | Epoch 60 [Train]: loss 0.0015743580870865878, mse: 0.0015373952919617295, rmse: 0.03920963264252458, mae 0.02242337539792061, r2: 0.760510044584403, nrmse: 1.01810939964832\n",
      "INFO logger 2022-10-24 16:31:15,116 | train_utils.py:99 | Epoch 60 [Test]: loss 1.2025110243495598e-05, mse: 0.0015212257858365774, rmse: 0.03900289458279446, mae 0.02329331636428833, r2: -5.5104109822193434, nrmse: 1.6506611322904403\n",
      "INFO logger 2022-10-24 16:31:15,117 | helpers.py:136 | EarlyStopping counter: 30 out of 50\n",
      "INFO logger 2022-10-24 16:31:15,607 | train_utils.py:97 | Epoch 61 [Train]: loss 0.0015721833461654125, mse: 0.0015357814263552427, rmse: 0.03918904727542177, mae 0.022442419081926346, r2: 0.7606208591991761, nrmse: 1.0189179259665642\n",
      "INFO logger 2022-10-24 16:31:15,607 | train_utils.py:99 | Epoch 61 [Test]: loss 1.2011140575074204e-05, mse: 0.0015196010936051607, rmse: 0.03898206117697166, mae 0.023240720853209496, r2: -5.457581900098182, nrmse: 1.6626655566942\n",
      "INFO logger 2022-10-24 16:31:15,608 | helpers.py:136 | EarlyStopping counter: 31 out of 50\n",
      "INFO logger 2022-10-24 16:31:16,057 | train_utils.py:97 | Epoch 62 [Train]: loss 0.0015710742187619935, mse: 0.001535149640403688, rmse: 0.03918098569974584, mae 0.022500809282064438, r2: 0.7605905309591757, nrmse: 1.0199895672023074\n",
      "INFO logger 2022-10-24 16:31:16,057 | train_utils.py:99 | Epoch 62 [Test]: loss 1.1998592909597056e-05, mse: 0.0015181335620582104, rmse: 0.038963233465129796, mae 0.023207787424325943, r2: -5.5071276399811016, nrmse: 1.66807268247925\n",
      "INFO logger 2022-10-24 16:31:16,058 | helpers.py:136 | EarlyStopping counter: 32 out of 50\n",
      "INFO logger 2022-10-24 16:31:16,527 | train_utils.py:97 | Epoch 63 [Train]: loss 0.001571563006966117, mse: 0.0015356738585978746, rmse: 0.0391876748302049, mae 0.022599902004003525, r2: 0.7604238196388795, nrmse: 1.0211476545613403\n",
      "INFO logger 2022-10-24 16:31:16,528 | train_utils.py:99 | Epoch 63 [Test]: loss 1.199224172194069e-05, mse: 0.0015173709252849221, rmse: 0.038953445615053386, mae 0.023214388638734818, r2: -5.690231615581956, nrmse: 1.6655549850868325\n",
      "INFO logger 2022-10-24 16:31:16,528 | helpers.py:136 | EarlyStopping counter: 33 out of 50\n",
      "INFO logger 2022-10-24 16:31:17,006 | train_utils.py:97 | Epoch 64 [Train]: loss 0.0015736103096800392, mse: 0.0015369197353720665, rmse: 0.039203567890844664, mae 0.022731533274054527, r2: 0.7601808712047892, nrmse: 1.0221522791544388\n",
      "INFO logger 2022-10-24 16:31:17,006 | train_utils.py:99 | Epoch 64 [Test]: loss 1.1997185947886826e-05, mse: 0.0015179072506725788, rmse: 0.03896032919101915, mae 0.023289406672120094, r2: -6.187468840861032, nrmse: 1.6649091408350467\n",
      "INFO logger 2022-10-24 16:31:17,006 | helpers.py:136 | EarlyStopping counter: 34 out of 50\n",
      "INFO logger 2022-10-24 16:31:17,558 | train_utils.py:97 | Epoch 65 [Train]: loss 0.0015758655269014843, mse: 0.0015378117095679045, rmse: 0.039214942427190994, mae 0.022862892597913742, r2: 0.7599919580187816, nrmse: 1.0227598380089142\n",
      "INFO logger 2022-10-24 16:31:17,558 | train_utils.py:99 | Epoch 65 [Test]: loss 1.201526217104061e-05, mse: 0.0015199730405583978, rmse: 0.0389868316301594, mae 0.02343647927045822, r2: -7.239976094409563, nrmse: 1.6790400712312168\n",
      "INFO logger 2022-10-24 16:31:17,559 | helpers.py:136 | EarlyStopping counter: 35 out of 50\n",
      "INFO logger 2022-10-24 16:31:18,013 | train_utils.py:97 | Epoch 66 [Train]: loss 0.0015756134516037533, mse: 0.0015369607135653496, rmse: 0.03920409052082894, mae 0.02292773313820362, r2: 0.7600860110905473, nrmse: 1.0226198455334112\n",
      "INFO logger 2022-10-24 16:31:18,014 | train_utils.py:99 | Epoch 66 [Test]: loss 1.2034956699071115e-05, mse: 0.0015222546644508839, rmse: 0.03901608212584759, mae 0.023592544719576836, r2: -8.603618030279018, nrmse: 1.6995578527241995\n",
      "INFO logger 2022-10-24 16:31:18,014 | helpers.py:136 | EarlyStopping counter: 36 out of 50\n",
      "INFO logger 2022-10-24 16:31:18,434 | train_utils.py:97 | Epoch 67 [Train]: loss 0.0015710683143223896, mse: 0.0015335835050791502, rmse: 0.039160994689603455, mae 0.022874394431710243, r2: 0.7606820460455681, nrmse: 1.021294902232089\n",
      "INFO logger 2022-10-24 16:31:18,435 | train_utils.py:99 | Epoch 67 [Test]: loss 1.2033604051746232e-05, mse: 0.0015220915665850043, rmse: 0.03901399193347182, mae 0.023647531867027283, r2: -9.400540212734281, nrmse: 1.7060314976230686\n",
      "INFO logger 2022-10-24 16:31:18,435 | helpers.py:136 | EarlyStopping counter: 37 out of 50\n",
      "INFO logger 2022-10-24 16:31:18,888 | train_utils.py:97 | Epoch 68 [Train]: loss 0.001563962995416744, mse: 0.0015287876594811678, rmse: 0.03909971431457227, mae 0.022730473428964615, r2: 0.7616688021345899, nrmse: 1.0188805636043525\n",
      "INFO logger 2022-10-24 16:31:18,888 | train_utils.py:99 | Epoch 68 [Test]: loss 1.2007054529181404e-05, mse: 0.0015189597615972161, rmse: 0.038973834319928236, mae 0.023582199588418007, r2: -9.23891736020713, nrmse: 1.7025075530160232\n",
      "INFO logger 2022-10-24 16:31:18,889 | helpers.py:136 | EarlyStopping counter: 38 out of 50\n",
      "INFO logger 2022-10-24 16:31:19,446 | train_utils.py:97 | Epoch 69 [Train]: loss 0.0015578257653561555, mse: 0.0015247991541400552, rmse: 0.039048676727131935, mae 0.022578546777367592, r2: 0.7626383644490795, nrmse: 1.0162090269544184\n",
      "INFO logger 2022-10-24 16:31:19,447 | train_utils.py:99 | Epoch 69 [Test]: loss 1.197510637371523e-05, mse: 0.001515182782895863, rmse: 0.03892534884745239, mae 0.023468127474188805, r2: -8.590906853298971, nrmse: 1.7039033241644441\n",
      "INFO logger 2022-10-24 16:31:19,448 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:19,861 | train_utils.py:97 | Epoch 70 [Train]: loss 0.0015544954857390156, mse: 0.0015226968098431826, rmse: 0.039021747908610946, mae 0.0224688071757555, r2: 0.763342219821977, nrmse: 1.0139347331699542\n",
      "INFO logger 2022-10-24 16:31:19,862 | train_utils.py:99 | Epoch 70 [Test]: loss 1.1953911934679711e-05, mse: 0.0015126672806218266, rmse: 0.038893023546927113, mae 0.02336963452398777, r2: -7.89846861181104, nrmse: 1.7058366247244912\n",
      "INFO logger 2022-10-24 16:31:19,863 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:20,306 | train_utils.py:97 | Epoch 71 [Train]: loss 0.0015536171080130707, mse: 0.0015218660701066256, rmse: 0.03901110188275417, mae 0.02239546738564968, r2: 0.7638498328947347, nrmse: 1.0120753248495098\n",
      "INFO logger 2022-10-24 16:31:20,306 | train_utils.py:99 | Epoch 71 [Test]: loss 1.194647684208613e-05, mse: 0.001511776470579207, rmse: 0.03888156980600458, mae 0.02330085262656212, r2: -7.2482261723685015, nrmse: 1.6962051874096815\n",
      "INFO logger 2022-10-24 16:31:20,306 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:20,757 | train_utils.py:97 | Epoch 72 [Train]: loss 0.001553752021896629, mse: 0.0015206653624773026, rmse: 0.03899570953934936, mae 0.02232929691672325, r2: 0.7643392430978405, nrmse: 1.0105982383373344\n",
      "INFO logger 2022-10-24 16:31:20,758 | train_utils.py:99 | Epoch 72 [Test]: loss 1.1948986175199099e-05, mse: 0.0015120698371902108, rmse: 0.038885342189444734, mae 0.023249108344316483, r2: -6.643788881970385, nrmse: 1.6762077804819406\n",
      "INFO logger 2022-10-24 16:31:20,758 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:21,325 | train_utils.py:97 | Epoch 73 [Train]: loss 0.0015528884865055493, mse: 0.0015174469444900751, rmse: 0.03895442137280536, mae 0.02225162833929062, r2: 0.7649130924194825, nrmse: 1.0097791158678198\n",
      "INFO logger 2022-10-24 16:31:21,325 | train_utils.py:99 | Epoch 73 [Test]: loss 1.1952657257485862e-05, mse: 0.0015125215286388993, rmse: 0.03889114974693985, mae 0.02319701574742794, r2: -6.169340855339843, nrmse: 1.6610343146339237\n",
      "INFO logger 2022-10-24 16:31:21,326 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:21,771 | train_utils.py:97 | Epoch 74 [Train]: loss 0.001549281258412309, mse: 0.0015121514443308115, rmse: 0.03888639150565158, mae 0.02217528596520424, r2: 0.7654776403188681, nrmse: 1.0100099066592885\n",
      "INFO logger 2022-10-24 16:31:21,772 | train_utils.py:99 | Epoch 74 [Test]: loss 1.194531617792356e-05, mse: 0.0015116940485313535, rmse: 0.03888050988003312, mae 0.023127179592847824, r2: -5.93848819392783, nrmse: 1.6663417252425408\n",
      "INFO logger 2022-10-24 16:31:21,772 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:22,222 | train_utils.py:97 | Epoch 75 [Train]: loss 0.001543356685276596, mse: 0.0015069686342030764, rmse: 0.03881969389630831, mae 0.022146986797451973, r2: 0.7657724435995747, nrmse: 1.011455676613383\n",
      "INFO logger 2022-10-24 16:31:22,223 | train_utils.py:99 | Epoch 75 [Test]: loss 1.1921901784650708e-05, mse: 0.001508968649432063, rmse: 0.03884544567168799, mae 0.023040590807795525, r2: -5.985813628079718, nrmse: 1.6919971704438053\n",
      "INFO logger 2022-10-24 16:31:22,223 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:22,675 | train_utils.py:97 | Epoch 76 [Train]: loss 0.001538230555858062, mse: 0.001504858722910285, rmse: 0.03879250859264305, mae 0.022222205996513367, r2: 0.7655166456068863, nrmse: 1.014116509264384\n",
      "INFO logger 2022-10-24 16:31:22,676 | train_utils.py:99 | Epoch 76 [Test]: loss 1.189222085587163e-05, mse: 0.0015054804971441627, rmse: 0.038800521866904865, mae 0.022961577400565147, r2: -6.210278933277479, nrmse: 1.7192565616194568\n",
      "INFO logger 2022-10-24 16:31:22,676 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:23,247 | train_utils.py:97 | Epoch 77 [Train]: loss 0.0015372553737422073, mse: 0.0015077001880854368, rmse: 0.03882911521121022, mae 0.022425390779972076, r2: 0.7645263856285469, nrmse: 1.0179453870143276\n",
      "INFO logger 2022-10-24 16:31:23,248 | train_utils.py:99 | Epoch 77 [Test]: loss 1.1870197851206748e-05, mse: 0.0015028634807094932, rmse: 0.03876678321333217, mae 0.022927802056074142, r2: -6.471120451445029, nrmse: 1.7249141092076579\n",
      "INFO logger 2022-10-24 16:31:23,248 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:23,720 | train_utils.py:97 | Epoch 78 [Train]: loss 0.0015419391774769814, mse: 0.0015147532103583217, rmse: 0.03891983055408029, mae 0.022743865847587585, r2: 0.7629161494133638, nrmse: 1.0223279252240898\n",
      "INFO logger 2022-10-24 16:31:23,720 | train_utils.py:99 | Epoch 78 [Test]: loss 1.1867536191027939e-05, mse: 0.0015024759341031313, rmse: 0.03876178445457757, mae 0.02300041727721691, r2: -7.021750801925734, nrmse: 1.7150011278221546\n",
      "INFO logger 2022-10-24 16:31:23,720 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:24,161 | train_utils.py:97 | Epoch 79 [Train]: loss 0.0015505273422551783, mse: 0.001521783648058772, rmse: 0.03901004547624588, mae 0.023098327219486237, r2: 0.761265955038696, nrmse: 1.0260766569795075\n",
      "INFO logger 2022-10-24 16:31:24,162 | train_utils.py:99 | Epoch 79 [Test]: loss 1.190004823320921e-05, mse: 0.0015062104212120175, rmse: 0.03880992683852957, mae 0.023275455459952354, r2: -9.005220744901203, nrmse: 1.740684537292451\n",
      "INFO logger 2022-10-24 16:31:24,163 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:24,636 | train_utils.py:97 | Epoch 80 [Train]: loss 0.0015557452076823105, mse: 0.0015233948361128569, rmse: 0.039030690951004914, mae 0.0232989564538002, r2: 0.7604983199382911, nrmse: 1.0281266577091293\n",
      "INFO logger 2022-10-24 16:31:24,637 | train_utils.py:99 | Epoch 80 [Test]: loss 1.1956351933448047e-05, mse: 0.0015128583181649446, rmse: 0.038895479405259226, mae 0.023630468174815178, r2: -12.520476772481796, nrmse: 1.8197063168574306\n",
      "INFO logger 2022-10-24 16:31:24,637 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:25,175 | train_utils.py:97 | Epoch 81 [Train]: loss 0.0015504578429848808, mse: 0.0015173794236034155, rmse: 0.038953554697914484, mae 0.02315325103700161, r2: 0.7616430573157504, nrmse: 1.0260237286415246\n",
      "INFO logger 2022-10-24 16:31:25,176 | train_utils.py:99 | Epoch 81 [Test]: loss 1.1963679439732695e-05, mse: 0.001513796509243548, rmse: 0.03890753794888013, mae 0.023718100041151047, r2: -13.72155804962477, nrmse: 1.8608524478616069\n",
      "INFO logger 2022-10-24 16:31:25,176 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 16:31:25,669 | train_utils.py:97 | Epoch 82 [Train]: loss 0.0015403798663291403, mse: 0.0015099081210792065, rmse: 0.038857536219878974, mae 0.022799594327807426, r2: 0.7640460543638865, nrmse: 1.0193631299089672\n",
      "INFO logger 2022-10-24 16:31:25,669 | train_utils.py:99 | Epoch 82 [Test]: loss 1.1929055391293434e-05, mse: 0.0015097154537215829, rmse: 0.038855056990327305, mae 0.0235355943441391, r2: -11.611601918079739, nrmse: 1.8288464707289953\n",
      "INFO logger 2022-10-24 16:31:25,670 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2022-10-24 16:31:26,136 | train_utils.py:97 | Epoch 83 [Train]: loss 0.0015352999638031009, mse: 0.0015065360348671675, rmse: 0.03881412159082268, mae 0.022543277591466904, r2: 0.7657992258764619, nrmse: 1.0133684319648681\n",
      "INFO logger 2022-10-24 16:31:26,136 | train_utils.py:99 | Epoch 83 [Test]: loss 1.1917684218377878e-05, mse: 0.001508352579548955, rmse: 0.03883751510522983, mae 0.023375798016786575, r2: -9.370279569070934, nrmse: 1.7724185445269984\n",
      "INFO logger 2022-10-24 16:31:26,136 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2022-10-24 16:31:26,580 | train_utils.py:97 | Epoch 84 [Train]: loss 0.0015351921582921848, mse: 0.001504140323959291, rmse: 0.03878324798104577, mae 0.022424791008234024, r2: 0.7666823470482684, nrmse: 1.0109218654752459\n",
      "INFO logger 2022-10-24 16:31:26,581 | train_utils.py:99 | Epoch 84 [Test]: loss 1.1933620459003897e-05, mse: 0.0015102621400728822, rmse: 0.038862091298241815, mae 0.02331390045583248, r2: -8.225436714504243, nrmse: 1.745058194034075\n",
      "INFO logger 2022-10-24 16:31:26,581 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2022-10-24 16:31:27,108 | train_utils.py:97 | Epoch 85 [Train]: loss 0.001533088283606556, mse: 0.0014979890547692776, rmse: 0.038703863563852095, mae 0.022350070998072624, r2: 0.7673574777447356, nrmse: 1.0107847941896835\n",
      "INFO logger 2022-10-24 16:31:27,108 | train_utils.py:99 | Epoch 85 [Test]: loss 1.1934677153561613e-05, mse: 0.0015104427002370358, rmse: 0.03886441431743229, mae 0.02326902747154236, r2: -7.869253716161111, nrmse: 1.7288294789575431\n",
      "INFO logger 2022-10-24 16:31:27,109 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2022-10-24 16:31:27,608 | train_utils.py:97 | Epoch 86 [Train]: loss 0.001527510006807524, mse: 0.0014931464102119207, rmse: 0.03864125269982743, mae 0.022357849404215813, r2: 0.7672899765117711, nrmse: 1.0133536983843823\n",
      "INFO logger 2022-10-24 16:31:27,610 | train_utils.py:99 | Epoch 86 [Test]: loss 1.1915923223887945e-05, mse: 0.0015082594472914934, rmse: 0.03883631608805724, mae 0.023214632645249367, r2: -8.096617591381513, nrmse: 1.7550731136105266\n",
      "INFO logger 2022-10-24 16:31:27,611 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2022-10-24 16:31:28,076 | train_utils.py:97 | Epoch 87 [Train]: loss 0.0015217131405554324, mse: 0.0014905829448252916, rmse: 0.03860806839023796, mae 0.022468572482466698, r2: 0.766730086708321, nrmse: 1.0163169791162365\n",
      "INFO logger 2022-10-24 16:31:28,077 | train_utils.py:99 | Epoch 87 [Test]: loss 1.1877806524321305e-05, mse: 0.0015037194825708866, rmse: 0.03877782204522176, mae 0.023182891309261322, r2: -8.653459403458791, nrmse: 1.7634590854024044\n",
      "INFO logger 2022-10-24 16:31:28,077 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2022-10-24 16:31:28,531 | train_utils.py:97 | Epoch 88 [Train]: loss 0.0015223840491354315, mse: 0.0014959892723709345, rmse: 0.0386780205332555, mae 0.022734936326742172, r2: 0.7649873323014559, nrmse: 1.0218147772471797\n",
      "INFO logger 2022-10-24 16:31:28,531 | train_utils.py:99 | Epoch 88 [Test]: loss 1.1879121877044733e-05, mse: 0.0015038235578686, rmse: 0.038779163965570476, mae 0.023282300680875778, r2: -10.192843578356264, nrmse: 1.8678666489192741\n",
      "INFO logger 2022-10-24 16:31:28,532 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2022-10-24 16:31:29,077 | train_utils.py:97 | Epoch 89 [Train]: loss 0.0015245168206621561, mse: 0.0014985882444307208, rmse: 0.038711603485656866, mae 0.023025361821055412, r2: 0.7632713305787456, nrmse: 1.02579989020518\n",
      "INFO logger 2022-10-24 16:31:29,077 | train_utils.py:99 | Epoch 89 [Test]: loss 1.1879953548784657e-05, mse: 0.001503898878581822, rmse: 0.03878013510267624, mae 0.023546528071165085, r2: -14.068885493782611, nrmse: 1.946282904471307\n",
      "INFO logger 2022-10-24 16:31:29,078 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2022-10-24 16:31:29,559 | train_utils.py:97 | Epoch 90 [Train]: loss 0.0015312425847125353, mse: 0.0015092856483533978, rmse: 0.0388495257159389, mae 0.023224588483572006, r2: 0.7615857486819056, nrmse: 1.0310451984501925\n",
      "INFO logger 2022-10-24 16:31:29,559 | train_utils.py:99 | Epoch 90 [Test]: loss 1.1942620690924263e-05, mse: 0.001511276001110673, rmse: 0.03887513345457058, mae 0.023700175806879997, r2: -14.972619384474893, nrmse: 2.127217633442589\n",
      "INFO logger 2022-10-24 16:31:29,560 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2022-10-24 16:31:30,051 | train_utils.py:97 | Epoch 91 [Train]: loss 0.0015278344029400952, mse: 0.0014958910178393126, rmse: 0.038676750352625446, mae 0.022945502772927284, r2: 0.7628857484656721, nrmse: 1.0245540631269217\n",
      "INFO logger 2022-10-24 16:31:30,052 | train_utils.py:99 | Epoch 91 [Test]: loss 1.1906189013787202e-05, mse: 0.0015072294045239687, rmse: 0.03882305248848896, mae 0.023917313665151596, r2: -21.94370388269737, nrmse: 2.0174700866449733\n",
      "INFO logger 2022-10-24 16:31:30,053 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2022-10-24 16:31:30,524 | train_utils.py:97 | Epoch 92 [Train]: loss 0.0015435595959452045, mse: 0.0015352882910519838, rmse: 0.039182755021207785, mae 0.0235014446079731, r2: 0.7568893089569274, nrmse: 1.0421298581250797\n",
      "INFO logger 2022-10-24 16:31:30,525 | train_utils.py:99 | Epoch 92 [Test]: loss 1.21004092139437e-05, mse: 0.0015303221298381686, rmse: 0.039119331919629824, mae 0.023949436843395233, r2: -19.72737885028088, nrmse: 2.744568929553615\n",
      "INFO logger 2022-10-24 16:31:30,525 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2022-10-24 16:31:31,060 | train_utils.py:97 | Epoch 93 [Train]: loss 0.001543250682135654, mse: 0.0015068668872117996, rmse: 0.03881838336679929, mae 0.021828290075063705, r2: 0.7638377786154507, nrmse: 1.0097716053324315\n",
      "INFO logger 2022-10-24 16:31:31,060 | train_utils.py:99 | Epoch 93 [Test]: loss 1.1823852788194011e-05, mse: 0.0014970966149121523, rmse: 0.03869233276648169, mae 0.022985167801380157, r2: -6.3424807665346945, nrmse: 1.44732531002152\n",
      "INFO logger 2022-10-24 16:31:31,061 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:31,504 | train_utils.py:97 | Epoch 94 [Train]: loss 0.0015776732321936898, mse: 0.001510625472292304, rmse: 0.038866765652576546, mae 0.023008199408650398, r2: 0.7623139371257609, nrmse: 1.0336250990248725\n",
      "INFO logger 2022-10-24 16:31:31,505 | train_utils.py:99 | Epoch 94 [Test]: loss 1.1971130296901108e-05, mse: 0.001514684990979731, rmse: 0.038918954135224795, mae 0.023642094805836678, r2: -12.333242091962893, nrmse: 2.035995250755886\n",
      "INFO logger 2022-10-24 16:31:31,505 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:31,980 | train_utils.py:97 | Epoch 95 [Train]: loss 0.0015184507613221919, mse: 0.0014739513862878084, rmse: 0.03839207452440944, mae 0.021752318367362022, r2: 0.7718937522321597, nrmse: 0.9993982838488323\n",
      "INFO logger 2022-10-24 16:31:31,981 | train_utils.py:99 | Epoch 95 [Test]: loss 1.1808241114846828e-05, mse: 0.001495001488365233, rmse: 0.03866524910517496, mae 0.02311014011502266, r2: -8.569640205866573, nrmse: 1.5463722423656066\n",
      "INFO logger 2022-10-24 16:31:31,981 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:32,473 | train_utils.py:97 | Epoch 96 [Train]: loss 0.0015223491038002892, mse: 0.00147904921323061, rmse: 0.03845840887544114, mae 0.02228638157248497, r2: 0.7702592728696881, nrmse: 1.007950108773042\n",
      "INFO logger 2022-10-24 16:31:32,473 | train_utils.py:99 | Epoch 96 [Test]: loss 1.187366907868115e-05, mse: 0.0015029406640678644, rmse: 0.03876777868369381, mae 0.0234678965061903, r2: -11.687630376314425, nrmse: 1.7699759439435223\n",
      "INFO logger 2022-10-24 16:31:32,474 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:32,998 | train_utils.py:97 | Epoch 97 [Train]: loss 0.001503170680227286, mse: 0.0014688954688608646, rmse: 0.03832617211333353, mae 0.021931972354650497, r2: 0.7724543637681373, nrmse: 1.000371982055207\n",
      "INFO logger 2022-10-24 16:31:32,998 | train_utils.py:99 | Epoch 97 [Test]: loss 1.1821125006154778e-05, mse: 0.001496617798693478, rmse: 0.0386861447897497, mae 0.023239105939865112, r2: -9.923161466144313, nrmse: 1.6089912912809976\n",
      "INFO logger 2022-10-24 16:31:33,000 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:33,468 | train_utils.py:97 | Epoch 98 [Train]: loss 0.0015032960758997983, mse: 0.0014714765129610896, rmse: 0.038359829417778823, mae 0.022099396213889122, r2: 0.7715414304156477, nrmse: 1.0042915758136928\n",
      "INFO logger 2022-10-24 16:31:33,468 | train_utils.py:99 | Epoch 98 [Test]: loss 1.1830195317035323e-05, mse: 0.0014977518003433943, rmse: 0.03870079844581238, mae 0.0232714656740427, r2: -10.066442927875281, nrmse: 1.6923320310071581\n",
      "INFO logger 2022-10-24 16:31:33,469 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 16:31:33,933 | train_utils.py:97 | Epoch 99 [Train]: loss 0.0014983991287027853, mse: 0.0014679930172860622, rmse: 0.03831439699755253, mae 0.021973539143800735, r2: 0.7724645501731184, nrmse: 1.0008510219326296\n",
      "INFO logger 2022-10-24 16:31:33,934 | train_utils.py:99 | Epoch 99 [Test]: loss 1.1809758003086276e-05, mse: 0.0014952915953472257, rmse: 0.038669000444118355, mae 0.02315327152609825, r2: -8.929420672078656, nrmse: 1.6255199534840943\n",
      "INFO logger 2022-10-24 16:31:33,934 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2022-10-24 16:31:34,403 | train_utils.py:97 | Epoch 100 [Train]: loss 0.0015005875934495508, mse: 0.001471140538342297, rmse: 0.03835544991708867, mae 0.022150708362460136, r2: 0.7715264510082094, nrmse: 1.0045885958972252\n",
      "INFO logger 2022-10-24 16:31:34,403 | train_utils.py:99 | Epoch 100 [Test]: loss 1.1824731753658427e-05, mse: 0.0014971046475693583, rmse: 0.03869243656800846, mae 0.023226046934723854, r2: -9.634005196725123, nrmse: 1.713804535364689\n",
      "INFO logger 2022-10-24 16:31:34,404 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2022-10-24 16:31:35,007 | train_utils.py:97 | Epoch 101 [Train]: loss 0.0014994719463703764, mse: 0.0014692823169752955, rmse: 0.03833121856888058, mae 0.02212607115507126, r2: 0.7719942625064563, nrmse: 1.0030066030119862\n",
      "INFO logger 2022-10-24 16:31:35,008 | train_utils.py:99 | Epoch 101 [Test]: loss 1.1821846751365484e-05, mse: 0.00149675237480551, rmse: 0.03868788408281732, mae 0.023209018632769585, r2: -9.44839660298955, nrmse: 1.69556379850419\n",
      "INFO logger 2022-10-24 16:31:35,008 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2022-10-24 16:31:35,590 | train_utils.py:97 | Epoch 102 [Train]: loss 0.0015012482957698985, mse: 0.0014715208671987057, rmse: 0.03836040754734894, mae 0.022273441776633263, r2: 0.7711887887193954, nrmse: 1.006498550680013\n",
      "INFO logger 2022-10-24 16:31:35,590 | train_utils.py:99 | Epoch 102 [Test]: loss 1.183903177608518e-05, mse: 0.0014988312032073736, rmse: 0.038714741419869685, mae 0.023292606696486473, r2: -10.341695597629641, nrmse: 1.7843955790206205\n",
      "INFO logger 2022-10-24 16:31:35,592 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2022-10-24 16:31:36,053 | train_utils.py:97 | Epoch 103 [Train]: loss 0.0015002574689915067, mse: 0.0014689965173602104, rmse: 0.03832749036083905, mae 0.022247938439249992, r2: 0.771667369200435, nrmse: 1.0052984854690719\n",
      "INFO logger 2022-10-24 16:31:36,054 | train_utils.py:99 | Epoch 103 [Test]: loss 1.1838245462069332e-05, mse: 0.0014987315516918898, rmse: 0.03871345440143374, mae 0.023295177146792412, r2: -10.366898573921755, nrmse: 1.7769870220838955\n",
      "INFO logger 2022-10-24 16:31:36,054 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2022-10-24 16:31:36,482 | train_utils.py:97 | Epoch 104 [Train]: loss 0.0015006025063231327, mse: 0.0014700033934786916, rmse: 0.03834062327973675, mae 0.02234930358827114, r2: 0.7710614805002016, nrmse: 1.0083119592506182\n",
      "INFO logger 2022-10-24 16:31:36,483 | train_utils.py:99 | Epoch 104 [Test]: loss 1.1850847527030746e-05, mse: 0.0015002606669440866, rmse: 0.0387331985116655, mae 0.023358259350061417, r2: -11.134633101572836, nrmse: 1.852258616282005\n",
      "INFO logger 2022-10-24 16:31:36,483 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2022-10-24 16:31:36,985 | train_utils.py:97 | Epoch 105 [Train]: loss 0.0014989445602686616, mse: 0.0014670753153041005, rmse: 0.03830241918344193, mae 0.02231266349554062, r2: 0.7715921797924149, nrmse: 1.0070878624301518\n",
      "INFO logger 2022-10-24 16:31:36,985 | train_utils.py:99 | Epoch 105 [Test]: loss 1.1845702637832514e-05, mse: 0.0014996257377788424, rmse: 0.03872500145615029, mae 0.023351546376943588, r2: -11.09113216116744, nrmse: 1.8402137035178718\n",
      "INFO logger 2022-10-24 16:31:36,986 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2022-10-24 16:31:37,433 | train_utils.py:97 | Epoch 106 [Train]: loss 0.001498541249036354, mse: 0.001467804191634059, rmse: 0.0383119327577461, mae 0.02239062264561653, r2: 0.7710814664285742, nrmse: 1.009709930667399\n",
      "INFO logger 2022-10-24 16:31:37,433 | train_utils.py:99 | Epoch 106 [Test]: loss 1.1853230062753714e-05, mse: 0.001500532729551196, rmse: 0.038736710360473256, mae 0.02339445799589157, r2: -11.680707833777607, nrmse: 1.9002441134312225\n",
      "INFO logger 2022-10-24 16:31:37,434 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2022-10-24 16:31:37,848 | train_utils.py:97 | Epoch 107 [Train]: loss 0.0014967994758440318, mse: 0.0014649833319708705, rmse: 0.038275100678781634, mae 0.02235357090830803, r2: 0.7716352715340464, nrmse: 1.0083129606202503\n",
      "INFO logger 2022-10-24 16:31:37,849 | train_utils.py:99 | Epoch 107 [Test]: loss 1.184461532893852e-05, mse: 0.0014994622906669974, rmse: 0.03872289104221168, mae 0.023383889347314835, r2: -11.599686853761979, nrmse: 1.8829137527265534\n",
      "INFO logger 2022-10-24 16:31:37,849 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2022-10-24 16:31:38,258 | train_utils.py:97 | Epoch 108 [Train]: loss 0.0014963401115173204, mse: 0.001465955632738769, rmse: 0.038287800050913985, mae 0.02241448499262333, r2: 0.7711739892205663, nrmse: 1.0105539910011956\n",
      "INFO logger 2022-10-24 16:31:38,259 | train_utils.py:99 | Epoch 108 [Test]: loss 1.1850283150996861e-05, mse: 0.0015001397114247084, rmse: 0.03873163708681455, mae 0.02341284230351448, r2: -12.030889049508826, nrmse: 1.9323877718428433\n",
      "INFO logger 2022-10-24 16:31:38,259 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2022-10-24 16:31:38,751 | train_utils.py:97 | Epoch 109 [Train]: loss 0.001494473204999405, mse: 0.0014626940246671438, rmse: 0.038245183025671925, mae 0.02235490269958973, r2: 0.7719015603372459, nrmse: 1.0084600720991737\n",
      "INFO logger 2022-10-24 16:31:38,751 | train_utils.py:99 | Epoch 109 [Test]: loss 1.1837920809691576e-05, mse: 0.0014986100140959024, rmse: 0.03871188466215385, mae 0.023388128727674484, r2: -11.788396656380424, nrmse: 1.900447907104628\n",
      "INFO logger 2022-10-24 16:31:38,752 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2022-10-24 16:31:39,224 | train_utils.py:97 | Epoch 110 [Train]: loss 0.0014938570439817552, mse: 0.001464129309169948, rmse: 0.03826394267675442, mae 0.022376205772161484, r2: 0.77155634273405, nrmse: 1.0100256256486615\n",
      "INFO logger 2022-10-24 16:31:39,224 | train_utils.py:99 | Epoch 110 [Test]: loss 1.1843575569065778e-05, mse: 0.0014992919750511646, rmse: 0.038720691820409985, mae 0.023385051637887955, r2: -11.800869446970202, nrmse: 1.93548853556191\n",
      "INFO logger 2022-10-24 16:31:39,224 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2022-10-24 16:31:39,737 | train_utils.py:97 | Epoch 111 [Train]: loss 0.0014915803118594002, mse: 0.0014598107663914561, rmse: 0.038207470033901174, mae 0.022278422489762306, r2: 0.7725998067679365, nrmse: 1.0067772066146208\n",
      "INFO logger 2022-10-24 16:31:39,738 | train_utils.py:99 | Epoch 111 [Test]: loss 1.1825100657474304e-05, mse: 0.0014970264164730906, rmse: 0.038691425619548976, mae 0.023329008370637894, r2: -11.154459489745973, nrmse: 1.8713193354888746\n",
      "INFO logger 2022-10-24 16:31:39,738 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2022-10-24 16:31:40,153 | train_utils.py:97 | Epoch 112 [Train]: loss 0.0014913811493242129, mse: 0.001463403576053679, rmse: 0.03825445825068863, mae 0.02228061482310295, r2: 0.7721717409747121, nrmse: 1.0080654232768635\n",
      "INFO logger 2022-10-24 16:31:40,153 | train_utils.py:99 | Epoch 112 [Test]: loss 1.1839144595977659e-05, mse: 0.0014987312024459243, rmse: 0.03871344989077988, mae 0.02329494059085846, r2: -10.79926069860446, nrmse: 1.918887489097513\n",
      "INFO logger 2022-10-24 16:31:40,154 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2022-10-24 16:31:40,707 | train_utils.py:97 | Epoch 113 [Train]: loss 0.0014893343763486523, mse: 0.0014571279752999544, rmse: 0.03817234568768278, mae 0.022162633016705513, r2: 0.7734773611985173, nrmse: 1.0040515646797905\n",
      "INFO logger 2022-10-24 16:31:40,708 | train_utils.py:99 | Epoch 113 [Test]: loss 1.1811023721864926e-05, mse: 0.0014952924102544785, rmse: 0.03866901098107473, mae 0.02322431653738022, r2: -9.836450787148738, nrmse: 1.804993504010064\n",
      "INFO logger 2022-10-24 16:31:40,708 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2022-10-24 16:31:41,221 | train_utils.py:97 | Epoch 114 [Train]: loss 0.0014906959654898708, mse: 0.001465478679165244, rmse: 0.03828157101224092, mae 0.022239556536078453, r2: 0.7723135416808088, nrmse: 1.0073031673345378\n",
      "INFO logger 2022-10-24 16:31:41,222 | train_utils.py:99 | Epoch 114 [Test]: loss 1.1853176067489313e-05, mse: 0.0015004223678261042, rmse: 0.03873528582347243, mae 0.02321978658437729, r2: -10.200319729085962, nrmse: 1.9668137918969166\n",
      "INFO logger 2022-10-24 16:31:41,222 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2022-10-24 16:31:41,726 | train_utils.py:97 | Epoch 115 [Train]: loss 0.0014891352512190785, mse: 0.0014542326098307967, rmse: 0.03813440192045493, mae 0.02206811122596264, r2: 0.7739856024700748, nrmse: 1.0028568414267047\n",
      "INFO logger 2022-10-24 16:31:41,727 | train_utils.py:99 | Epoch 115 [Test]: loss 1.1794558591454437e-05, mse: 0.0014932682970538735, rmse: 0.03864282982719917, mae 0.023121768608689308, r2: -8.635390908269253, nrmse: 1.7346410650822928\n",
      "INFO logger 2022-10-24 16:31:41,728 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:42,190 | train_utils.py:97 | Epoch 116 [Train]: loss 0.001492615076977229, mse: 0.001469469047151506, rmse: 0.03833365423686484, mae 0.022376839071512222, r2: 0.7703578310939863, nrmse: 1.015635862464763\n",
      "INFO logger 2022-10-24 16:31:42,191 | train_utils.py:99 | Epoch 116 [Test]: loss 1.188376906521248e-05, mse: 0.001504190149717033, rmse: 0.038783890337574865, mae 0.023272942751646042, r2: -11.97930843218143, nrmse: 2.175595567252498\n",
      "INFO logger 2022-10-24 16:31:42,191 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:42,727 | train_utils.py:97 | Epoch 117 [Train]: loss 0.0014914606928319108, mse: 0.0014538050163537264, rmse: 0.03812879510755259, mae 0.021968042477965355, r2: 0.7737067166602459, nrmse: 1.003431612436909\n",
      "INFO logger 2022-10-24 16:31:42,728 | train_utils.py:99 | Epoch 117 [Test]: loss 1.177119138553243e-05, mse: 0.0014903545379638672, rmse: 0.038605110257113204, mae 0.023029964417219162, r2: -8.000437698619356, nrmse: 1.6638953084089\n",
      "INFO logger 2022-10-24 16:31:42,728 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:43,207 | train_utils.py:97 | Epoch 118 [Train]: loss 0.0015013332212706594, mse: 0.001475561293773353, rmse: 0.038413035466796336, mae 0.02270479127764702, r2: 0.7660637010625415, nrmse: 1.035169252451012\n",
      "INFO logger 2022-10-24 16:31:43,207 | train_utils.py:99 | Epoch 118 [Test]: loss 1.1874887801581525e-05, mse: 0.0015030953800305724, rmse: 0.03876977405183802, mae 0.023393545299768448, r2: -13.829043912233312, nrmse: 2.306334216341595\n",
      "INFO logger 2022-10-24 16:31:43,208 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:43,668 | train_utils.py:97 | Epoch 119 [Train]: loss 0.001499128778706045, mse: 0.0014535434311255813, rmse: 0.038125364668755386, mae 0.021672191098332405, r2: 0.7747049382023362, nrmse: 0.9984074618197588\n",
      "INFO logger 2022-10-24 16:31:43,669 | train_utils.py:99 | Epoch 119 [Test]: loss 1.1770685727752974e-05, mse: 0.0014900962123647332, rmse: 0.038601764368545816, mae 0.022932836785912514, r2: -6.820380836321715, nrmse: 1.575990283111357\n",
      "INFO logger 2022-10-24 16:31:43,669 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:44,178 | train_utils.py:97 | Epoch 120 [Train]: loss 0.0015064993968672055, mse: 0.0014674878912046552, rmse: 0.03830780457301952, mae 0.0225692056119442, r2: 0.7700829626267851, nrmse: 1.0220713270866295\n",
      "INFO logger 2022-10-24 16:31:44,179 | train_utils.py:99 | Epoch 120 [Test]: loss 1.1847014033456185e-05, mse: 0.0014994768425822258, rmse: 0.03872307893985479, mae 0.02347140572965145, r2: -12.606672112974039, nrmse: 2.146357413072997\n",
      "INFO logger 2022-10-24 16:31:44,179 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:44,718 | train_utils.py:97 | Epoch 121 [Train]: loss 0.0014864327494314936, mse: 0.0014469244051724672, rmse: 0.0380384595530953, mae 0.021626437082886696, r2: 0.7766265040267969, nrmse: 0.9938996281203273\n",
      "INFO logger 2022-10-24 16:31:44,720 | train_utils.py:99 | Epoch 121 [Test]: loss 1.1774034540431726e-05, mse: 0.0014905267162248492, rmse: 0.03860734018583577, mae 0.02299346588551998, r2: -7.6165251274289405, nrmse: 1.6957363670744863\n",
      "INFO logger 2022-10-24 16:31:44,720 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:45,188 | train_utils.py:97 | Epoch 122 [Train]: loss 0.0014880447545925393, mse: 0.0014537551905959845, rmse: 0.038128141714434294, mae 0.022026464343070984, r2: 0.7746478557051963, nrmse: 1.003714439771534\n",
      "INFO logger 2022-10-24 16:31:45,188 | train_utils.py:99 | Epoch 122 [Test]: loss 1.1790170997540516e-05, mse: 0.0014926469884812832, rmse: 0.03863478987235835, mae 0.02314315363764763, r2: -9.180212622902783, nrmse: 1.805232759890885\n",
      "INFO logger 2022-10-24 16:31:45,188 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 16:31:45,666 | train_utils.py:97 | Epoch 123 [Train]: loss 0.0014729341047038974, mse: 0.0014424503315240145, rmse: 0.037979604151755116, mae 0.021517669782042503, r2: 0.7774700834835654, nrmse: 0.99232892446179\n",
      "INFO logger 2022-10-24 16:31:45,666 | train_utils.py:99 | Epoch 123 [Test]: loss 1.1736776978281267e-05, mse: 0.0014861403033137321, rmse: 0.0385504903122351, mae 0.022813744843006134, r2: -6.247442705354852, nrmse: 1.6216580653142734\n",
      "INFO logger 2022-10-24 16:31:45,667 | helpers.py:148 | Validation loss decreased (0.000012 --> 0.000012). Caching model ...\n",
      "INFO logger 2022-10-24 16:31:46,155 | train_utils.py:97 | Epoch 124 [Train]: loss 0.0014785439772272502, mse: 0.0014508316526189446, rmse: 0.03808978409782529, mae 0.021927261725068092, r2: 0.7751770855600244, nrmse: 1.0017308951784953\n",
      "INFO logger 2022-10-24 16:31:46,155 | train_utils.py:99 | Epoch 124 [Test]: loss 1.1773927826386719e-05, mse: 0.0014906401047483087, rmse: 0.03860880864191886, mae 0.023027237504720688, r2: -7.976447226419428, nrmse: 1.7972927657699516\n",
      "INFO logger 2022-10-24 16:31:46,155 | helpers.py:136 | EarlyStopping counter: 1 out of 50\n",
      "INFO logger 2022-10-24 16:31:46,703 | train_utils.py:97 | Epoch 125 [Train]: loss 0.0014732953904967871, mse: 0.0014451247407123446, rmse: 0.03801479633921961, mae 0.021703971549868584, r2: 0.7766860965150985, nrmse: 0.99555848478762\n",
      "INFO logger 2022-10-24 16:31:46,703 | train_utils.py:99 | Epoch 125 [Test]: loss 1.1759278536719755e-05, mse: 0.0014888199511915445, rmse: 0.03858522970245926, mae 0.022907761856913567, r2: -6.640812839125924, nrmse: 1.7297807280955817\n",
      "INFO logger 2022-10-24 16:31:46,704 | helpers.py:136 | EarlyStopping counter: 2 out of 50\n",
      "INFO logger 2022-10-24 16:31:47,158 | train_utils.py:97 | Epoch 126 [Train]: loss 0.0014782548518967307, mse: 0.0014524620492011309, rmse: 0.03811118010769453, mae 0.02203940413892269, r2: 0.7744074708247847, nrmse: 1.0050831511048575\n",
      "INFO logger 2022-10-24 16:31:47,159 | train_utils.py:99 | Epoch 126 [Test]: loss 1.180108657211599e-05, mse: 0.0014938594540581107, rmse: 0.03865047805730365, mae 0.02311651036143303, r2: -8.342914443380186, nrmse: 1.9380092474671142\n",
      "INFO logger 2022-10-24 16:31:47,159 | helpers.py:136 | EarlyStopping counter: 3 out of 50\n",
      "INFO logger 2022-10-24 16:31:47,636 | train_utils.py:97 | Epoch 127 [Train]: loss 0.001475909908612964, mse: 0.0014477784279733896, rmse: 0.038049683677704726, mae 0.021889956668019295, r2: 0.7754764205662632, nrmse: 1.0016090666096458\n",
      "INFO logger 2022-10-24 16:31:47,636 | train_utils.py:99 | Epoch 127 [Test]: loss 1.1793597633181118e-05, mse: 0.0014929117169231176, rmse: 0.038638215757499955, mae 0.023052195087075233, r2: -7.700310025406156, nrmse: 1.9621637065168307\n",
      "INFO logger 2022-10-24 16:31:47,637 | helpers.py:136 | EarlyStopping counter: 4 out of 50\n",
      "INFO logger 2022-10-24 16:31:48,106 | train_utils.py:97 | Epoch 128 [Train]: loss 0.0014780940729991038, mse: 0.0014524816069751978, rmse: 0.038111436695238844, mae 0.02209889143705368, r2: 0.7735889197579293, nrmse: 1.0098281188557883\n",
      "INFO logger 2022-10-24 16:31:48,107 | train_utils.py:99 | Epoch 128 [Test]: loss 1.1819146817947646e-05, mse: 0.0014959964901208878, rmse: 0.03867811383871876, mae 0.023185718804597855, r2: -8.975719893238196, nrmse: 2.126222769769875\n",
      "INFO logger 2022-10-24 16:31:48,107 | helpers.py:136 | EarlyStopping counter: 5 out of 50\n",
      "INFO logger 2022-10-24 16:31:48,672 | train_utils.py:97 | Epoch 129 [Train]: loss 0.001476014269460287, mse: 0.001448745490051806, rmse: 0.038062389442227694, mae 0.021947083994746208, r2: 0.774609948507889, nrmse: 1.0066404722208235\n",
      "INFO logger 2022-10-24 16:31:48,672 | train_utils.py:99 | Epoch 129 [Test]: loss 1.1809102443560184e-05, mse: 0.0014947338495403528, rmse: 0.038661787976506636, mae 0.02309783734381199, r2: -8.273659424305553, nrmse: 2.1415311013002203\n",
      "INFO logger 2022-10-24 16:31:48,673 | helpers.py:136 | EarlyStopping counter: 6 out of 50\n",
      "INFO logger 2022-10-24 16:31:49,147 | train_utils.py:97 | Epoch 130 [Train]: loss 0.0014769956349133535, mse: 0.0014530605403706431, rmse: 0.03811903120975982, mae 0.022112729027867317, r2: 0.7728863803624165, nrmse: 1.0133190357558604\n",
      "INFO logger 2022-10-24 16:31:49,148 | train_utils.py:99 | Epoch 130 [Test]: loss 1.1826353760338143e-05, mse: 0.001496803597547114, rmse: 0.038688546076934886, mae 0.023194845765829086, r2: -9.274396579114626, nrmse: 2.2526531325140566\n",
      "INFO logger 2022-10-24 16:31:49,148 | helpers.py:136 | EarlyStopping counter: 7 out of 50\n",
      "INFO logger 2022-10-24 16:31:49,622 | train_utils.py:97 | Epoch 131 [Train]: loss 0.001475773653869827, mse: 0.001450419775210321, rmse: 0.03808437704899899, mae 0.021978598088026047, r2: 0.7737767458328009, nrmse: 1.0102558614823869\n",
      "INFO logger 2022-10-24 16:31:49,622 | train_utils.py:99 | Epoch 131 [Test]: loss 1.1819231342449403e-05, mse: 0.0014958960236981511, rmse: 0.03867681506662811, mae 0.023107752203941345, r2: -8.6170865058166, nrmse: 2.256847496131023\n",
      "INFO logger 2022-10-24 16:31:49,623 | helpers.py:136 | EarlyStopping counter: 8 out of 50\n",
      "INFO logger 2022-10-24 16:31:50,103 | train_utils.py:97 | Epoch 132 [Train]: loss 0.0014765059035526489, mse: 0.0014558909460902214, rmse: 0.038156139035418944, mae 0.022154122591018677, r2: 0.7718602681266387, nrmse: 1.0162550114254358\n",
      "INFO logger 2022-10-24 16:31:50,104 | train_utils.py:99 | Epoch 132 [Test]: loss 1.183704837556923e-05, mse: 0.0014980238629505038, rmse: 0.03870431323445107, mae 0.023215997964143753, r2: -9.886702094210179, nrmse: 2.3593658387777303\n",
      "INFO logger 2022-10-24 16:31:50,104 | helpers.py:136 | EarlyStopping counter: 9 out of 50\n",
      "INFO logger 2022-10-24 16:31:50,666 | train_utils.py:97 | Epoch 133 [Train]: loss 0.0014764936110284823, mse: 0.0014533753274008632, rmse: 0.03812315998708479, mae 0.02203783020377159, r2: 0.7725508386326159, nrmse: 1.0142160360961048\n",
      "INFO logger 2022-10-24 16:31:50,667 | train_utils.py:99 | Epoch 133 [Test]: loss 1.1832457629259817e-05, mse: 0.0014974409714341164, rmse: 0.0386967824429127, mae 0.02315247617661953, r2: -9.548305084943644, nrmse: 2.3855472018644326\n",
      "INFO logger 2022-10-24 16:31:50,667 | helpers.py:136 | EarlyStopping counter: 10 out of 50\n",
      "INFO logger 2022-10-24 16:31:51,139 | train_utils.py:97 | Epoch 134 [Train]: loss 0.0014766965675313936, mse: 0.0014603656018152833, rmse: 0.03821473016802923, mae 0.022253278642892838, r2: 0.7703360418237264, nrmse: 1.0199808264861674\n",
      "INFO logger 2022-10-24 16:31:51,140 | train_utils.py:99 | Epoch 134 [Test]: loss 1.1854998886344236e-05, mse: 0.0015001218998804688, rmse: 0.03873140715079261, mae 0.02330736443400383, r2: -11.658372333272819, nrmse: 2.5083694521831834\n",
      "INFO logger 2022-10-24 16:31:51,141 | helpers.py:136 | EarlyStopping counter: 11 out of 50\n",
      "INFO logger 2022-10-24 16:31:51,598 | train_utils.py:97 | Epoch 135 [Train]: loss 0.0014784929018872412, mse: 0.0014571984065696597, rmse: 0.03817326821965418, mae 0.02216554805636406, r2: 0.7705681836966418, nrmse: 1.0214622344175122\n",
      "INFO logger 2022-10-24 16:31:51,599 | train_utils.py:99 | Epoch 135 [Test]: loss 1.185391830026488e-05, mse: 0.001499989884905517, rmse: 0.038729702876545764, mae 0.023292800411581993, r2: -11.953159135315115, nrmse: 2.609836178075602\n",
      "INFO logger 2022-10-24 16:31:51,599 | helpers.py:136 | EarlyStopping counter: 12 out of 50\n",
      "INFO logger 2022-10-24 16:31:52,050 | train_utils.py:97 | Epoch 136 [Train]: loss 0.001476943112255296, mse: 0.001465206267312169, rmse: 0.038278012844349285, mae 0.022411709651350975, r2: 0.7683474159511781, nrmse: 1.0259505088111824\n",
      "INFO logger 2022-10-24 16:31:52,051 | train_utils.py:99 | Epoch 136 [Test]: loss 1.1880627723699382e-05, mse: 0.0015031475340947509, rmse: 0.03877044665843754, mae 0.023503504693508148, r2: -15.4411800183998, nrmse: 2.7761080057068233\n",
      "INFO logger 2022-10-24 16:31:52,051 | helpers.py:136 | EarlyStopping counter: 13 out of 50\n",
      "INFO logger 2022-10-24 16:31:52,620 | train_utils.py:97 | Epoch 137 [Train]: loss 0.001480841237417037, mse: 0.0014610739890486002, rmse: 0.038223997554528495, mae 0.02234608307480812, r2: 0.7679973987519418, nrmse: 1.0322255672900111\n",
      "INFO logger 2022-10-24 16:31:52,620 | train_utils.py:99 | Epoch 137 [Test]: loss 1.1881596851468968e-05, mse: 0.001503280596807599, rmse: 0.03877216265321808, mae 0.02351623959839344, r2: -16.136278643603628, nrmse: 2.9486175078168526\n",
      "INFO logger 2022-10-24 16:31:52,621 | helpers.py:136 | EarlyStopping counter: 14 out of 50\n",
      "INFO logger 2022-10-24 16:31:53,080 | train_utils.py:97 | Epoch 138 [Train]: loss 0.0014763261259981106, mse: 0.0014694443671032786, rmse: 0.03833333232453551, mae 0.022556645795702934, r2: 0.767165940776721, nrmse: 1.029040043919905\n",
      "INFO logger 2022-10-24 16:31:53,080 | train_utils.py:99 | Epoch 138 [Test]: loss 1.1907685839956387e-05, mse: 0.0015063564060255885, rmse: 0.038811807559370234, mae 0.02373465709388256, r2: -20.247858122219558, nrmse: 3.077768693973498\n",
      "INFO logger 2022-10-24 16:31:53,081 | helpers.py:136 | EarlyStopping counter: 15 out of 50\n",
      "INFO logger 2022-10-24 16:31:53,539 | train_utils.py:97 | Epoch 139 [Train]: loss 0.0014819486297689532, mse: 0.0014621592126786709, rmse: 0.038238190499534246, mae 0.02244492992758751, r2: 0.7675633902533597, nrmse: 1.0338714188956697\n",
      "INFO logger 2022-10-24 16:31:53,540 | train_utils.py:99 | Epoch 139 [Test]: loss 1.1903131494709775e-05, mse: 0.0015058506978675723, rmse: 0.038805292137382146, mae 0.023686420172452927, r2: -19.44587633935399, nrmse: 3.159148406097306\n",
      "INFO logger 2022-10-24 16:31:53,541 | helpers.py:136 | EarlyStopping counter: 16 out of 50\n",
      "INFO logger 2022-10-24 16:31:54,006 | train_utils.py:97 | Epoch 140 [Train]: loss 0.0014730741955211902, mse: 0.0014676249120384455, rmse: 0.038309592950571084, mae 0.0225098617374897, r2: 0.7690842567740883, nrmse: 1.0208614525927149\n",
      "INFO logger 2022-10-24 16:31:54,007 | train_utils.py:99 | Epoch 140 [Test]: loss 1.1921217823360247e-05, mse: 0.0015080032171681523, rmse: 0.03883301710102052, mae 0.023825420066714287, r2: -21.816782244814718, nrmse: 3.1267821075768354\n",
      "INFO logger 2022-10-24 16:31:54,008 | helpers.py:136 | EarlyStopping counter: 17 out of 50\n",
      "INFO logger 2022-10-24 16:31:54,594 | train_utils.py:97 | Epoch 141 [Train]: loss 0.0014777022249269744, mse: 0.001457069767639041, rmse: 0.03817158324773864, mae 0.02229740098118782, r2: 0.7717727409288948, nrmse: 1.0150685496786636\n",
      "INFO logger 2022-10-24 16:31:54,594 | train_utils.py:99 | Epoch 141 [Test]: loss 1.1900934219334482e-05, mse: 0.0015056681586429477, rmse: 0.03880294007730532, mae 0.023648060858249664, r2: -17.770183576700852, nrmse: 2.9165331042956093\n",
      "INFO logger 2022-10-24 16:31:54,595 | helpers.py:136 | EarlyStopping counter: 18 out of 50\n",
      "INFO logger 2022-10-24 16:31:55,070 | train_utils.py:97 | Epoch 142 [Train]: loss 0.0014654243250756841, mse: 0.0014572353102266788, rmse: 0.038173751587009086, mae 0.02217487432062626, r2: 0.7742276199782641, nrmse: 1.00174709981532\n",
      "INFO logger 2022-10-24 16:31:55,071 | train_utils.py:99 | Epoch 142 [Test]: loss 1.190243695987857e-05, mse: 0.0015058707213029265, rmse: 0.03880555013529542, mae 0.023643873631954193, r2: -17.144787749874876, nrmse: 2.7316995470284957\n",
      "INFO logger 2022-10-24 16:31:55,071 | helpers.py:136 | EarlyStopping counter: 19 out of 50\n",
      "INFO logger 2022-10-24 16:31:55,552 | train_utils.py:97 | Epoch 143 [Train]: loss 0.0014694969389110642, mse: 0.0014487431617453694, rmse: 0.038062358856820334, mae 0.022015586495399475, r2: 0.7765486407190267, nrmse: 0.9964865859499283\n",
      "INFO logger 2022-10-24 16:31:55,552 | train_utils.py:99 | Epoch 143 [Test]: loss 1.1891741621306667e-05, mse: 0.0015046827029436827, rmse: 0.03879023978971621, mae 0.023460645228624344, r2: -13.068972455017544, nrmse: 2.5083270629671306\n",
      "INFO logger 2022-10-24 16:31:55,553 | helpers.py:136 | EarlyStopping counter: 20 out of 50\n",
      "INFO logger 2022-10-24 16:31:56,040 | train_utils.py:97 | Epoch 144 [Train]: loss 0.0014570450025424835, mse: 0.001447205781005323, rmse: 0.03804215794359362, mae 0.021782169118523598, r2: 0.7782477143073104, nrmse: 0.9889335227922427\n",
      "INFO logger 2022-10-24 16:31:56,040 | train_utils.py:99 | Epoch 144 [Test]: loss 1.1875809125033104e-05, mse: 0.0015028206398710608, rmse: 0.03876623066369828, mae 0.023311074823141098, r2: -10.375552254833718, nrmse: 2.1528965256871\n",
      "INFO logger 2022-10-24 16:31:56,041 | helpers.py:136 | EarlyStopping counter: 21 out of 50\n",
      "INFO logger 2022-10-24 16:31:56,644 | train_utils.py:97 | Epoch 145 [Train]: loss 0.0014633772652179983, mse: 0.001443036599084735, rmse: 0.03798732155712923, mae 0.02187172695994377, r2: 0.7792425246497998, nrmse: 0.9879474117165503\n",
      "INFO logger 2022-10-24 16:31:56,645 | train_utils.py:99 | Epoch 145 [Test]: loss 1.1877453640377018e-05, mse: 0.0015030975919216871, rmse: 0.038769802577801285, mae 0.02328340709209442, r2: -9.673140506489421, nrmse: 2.286117361909493\n",
      "INFO logger 2022-10-24 16:31:56,645 | helpers.py:136 | EarlyStopping counter: 22 out of 50\n",
      "INFO logger 2022-10-24 16:31:57,111 | train_utils.py:97 | Epoch 146 [Train]: loss 0.001453645953048646, mse: 0.0014446100685745478, rmse: 0.03800802637042008, mae 0.02161160483956337, r2: 0.7787821410898843, nrmse: 0.9894561405973796\n",
      "INFO logger 2022-10-24 16:31:57,111 | train_utils.py:99 | Epoch 146 [Test]: loss 1.1870857708228273e-05, mse: 0.001502271625213325, rmse: 0.03875914892271662, mae 0.023049501702189445, r2: -5.815431245725277, nrmse: 1.6795270171602026\n",
      "INFO logger 2022-10-24 16:31:57,112 | helpers.py:136 | EarlyStopping counter: 23 out of 50\n",
      "INFO logger 2022-10-24 16:31:57,554 | train_utils.py:97 | Epoch 147 [Train]: loss 0.0014642213961430339, mse: 0.0014494406059384346, rmse: 0.03807151961687942, mae 0.02195407636463642, r2: 0.7787868275078562, nrmse: 0.9874451531546462\n",
      "INFO logger 2022-10-24 16:31:57,554 | train_utils.py:99 | Epoch 147 [Test]: loss 1.189920813667626e-05, mse: 0.0015057135606184602, rmse: 0.038803525105568185, mae 0.023235131055116653, r2: -7.388236870957814, nrmse: 1.9678609248727514\n",
      "INFO logger 2022-10-24 16:31:57,555 | helpers.py:136 | EarlyStopping counter: 24 out of 50\n",
      "INFO logger 2022-10-24 16:31:58,002 | train_utils.py:97 | Epoch 148 [Train]: loss 0.0014596998453482737, mse: 0.001450690091587603, rmse: 0.03808792579791663, mae 0.021654291078448296, r2: 0.7752574008775692, nrmse: 1.0037922404253785\n",
      "INFO logger 2022-10-24 16:31:58,002 | train_utils.py:99 | Epoch 148 [Test]: loss 1.1898786875750183e-05, mse: 0.001505585154518485, rmse: 0.03880187050283124, mae 0.022981788963079453, r2: -4.277801452721617, nrmse: 1.4533515167573297\n",
      "INFO logger 2022-10-24 16:31:58,002 | helpers.py:136 | EarlyStopping counter: 25 out of 50\n",
      "INFO logger 2022-10-24 16:31:58,609 | train_utils.py:97 | Epoch 149 [Train]: loss 0.0014706715427205063, mse: 0.0014659619191661477, rmse: 0.03828788214521858, mae 0.022351209074258804, r2: 0.770208502669963, nrmse: 1.0215505216413294\n",
      "INFO logger 2022-10-24 16:31:58,609 | train_utils.py:99 | Epoch 149 [Test]: loss 1.1986051222319001e-05, mse: 0.0015159858157858253, rmse: 0.03893566251890194, mae 0.02344033494591713, r2: -9.549425811914332, nrmse: 2.3502909164342585\n",
      "INFO logger 2022-10-24 16:31:58,611 | helpers.py:136 | EarlyStopping counter: 26 out of 50\n",
      "INFO logger 2022-10-24 16:31:59,126 | train_utils.py:97 | Epoch 150 [Train]: loss 0.0014703739983561654, mse: 0.00146369612775743, rmse: 0.038258281819201316, mae 0.0222114659845829, r2: 0.7643819551041415, nrmse: 1.0502703636640067\n",
      "INFO logger 2022-10-24 16:31:59,127 | train_utils.py:99 | Epoch 150 [Test]: loss 1.1940089572487911e-05, mse: 0.0015105570200830698, rmse: 0.03886588504180845, mae 0.02343171089887619, r2: -11.61470130861176, nrmse: 2.70729515529516\n",
      "INFO logger 2022-10-24 16:31:59,127 | helpers.py:136 | EarlyStopping counter: 27 out of 50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO logger 2022-10-24 16:31:59,373 | train_utils.py:140 | Best Loss: 1.1736776978281267e-05, Best epoch: 150\n"
     ]
    }
   ],
   "source": [
    "trained_model = fit(model, X_train, y_train, X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a39106e1a9d6d153b7400628e7589ff266b5caee5b0db427f0903be982155882"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
